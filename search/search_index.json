{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Index by Tag","text":""},{"location":"#amd","title":"AMD","text":"<ul> <li>AMD Memory Detection Routines Ignore Unified Memory on AMD APU</li> </ul>"},{"location":"#aws","title":"AWS","text":"<ul> <li>Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state</li> </ul>"},{"location":"#artificial-intelligence","title":"Artificial Intelligence","text":"<ul> <li>AIOps: AI-Driven Operations for Enhanced Network Performance</li> </ul>"},{"location":"#bug-fix","title":"Bug Fix","text":"<ul> <li>Functions and buttons from react-big-calendar don't work in next 13.4.9 and upwards</li> </ul>"},{"location":"#code-completion","title":"Code Completion","text":"<ul> <li>Enabling Real-Time Type Checking in JavaScript Projects with TypeScript Annotations</li> </ul>"},{"location":"#dra","title":"DRA","text":"<ul> <li>Introducing Permanent and Transient Error Handling in kubelet</li> </ul>"},{"location":"#dra-drivers","title":"DRA drivers","text":"<ul> <li>Surface Permanent Errors in Kubelet and DRA Drivers</li> </ul>"},{"location":"#debugging","title":"Debugging","text":"<ul> <li>TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide</li> </ul>"},{"location":"#deep-learning","title":"Deep Learning","text":"<ul> <li>Unlocking CPU Potential in Model-Driven Applications</li> </ul>"},{"location":"#deepseek-r1","title":"DeepSeek-R1","text":"<ul> <li>DeepSeek-R1 671B Q4_K_M Error: Model Requires More System Memory Than Available</li> </ul>"},{"location":"#diagnostics","title":"Diagnostics","text":"<ul> <li>Supporting Closed-File Diagnostics in VS Code</li> </ul>"},{"location":"#disruption-budget","title":"Disruption Budget","text":"<ul> <li>Enabling Scalable PDB Constraints in Kubernetes</li> </ul>"},{"location":"#error-handling","title":"Error Handling","text":"<ul> <li>InternalError: too much recursion on Firefox</li> <li>InternalError: too much recursion on Firefox</li> </ul>"},{"location":"#horizontal-pod-autoscaling","title":"Horizontal Pod Autoscaling","text":"<ul> <li>Enabling Scalable PDB Constraints in Kubernetes</li> </ul>"},{"location":"#image-pull-issues","title":"Image Pull Issues","text":"<ul> <li>Detecting and Handling Stuck Pods due to Invalid Image Pulls</li> </ul>"},{"location":"#image-validation","title":"Image Validation","text":"<ul> <li>Failing Stuck Pods due to Invalid Images: A Mechanism for Rescue</li> </ul>"},{"location":"#inline-comments","title":"Inline Comments","text":"<ul> <li>Enabling TypeScript Type Checking in JavaScript Projects with Inline Comments</li> </ul>"},{"location":"#intellisense-and-code-completion","title":"Intellisense and Code Completion","text":"<ul> <li>Enabling Real-Time Type Checking in JavaScript Projects with Comment Annotations</li> </ul>"},{"location":"#javascript","title":"JavaScript","text":"<ul> <li>TypeScript Type Annotations as Comments: Revolutionizing Code Reviews and Development</li> <li>Enabling TypeScript Type Checking in JavaScript Projects with Inline Comments</li> </ul>"},{"location":"#kubernetes","title":"Kubernetes","text":"<ul> <li>Enabling Selector in kubectl Create Service Command</li> <li>Enabling Scalable PDB Constraints in Kubernetes</li> <li>Pods with Zero TerminationGracePeriod are Force-Deleted</li> <li>Introducing Permanent and Transient Error Handling in kubelet</li> <li>Surface Permanent Errors in Kubelet and DRA Drivers</li> <li>Enabling Label Selector Support in Resource Quota's ScopeSelector</li> <li>Detecting and Handling Stuck Pods due to Invalid Image Pulls</li> <li>Racing to Zero: Mitigating Race Conditions in Kubernetes Schedulers</li> <li>Failing Stuck Pods due to Invalid Images: A Mechanism for Rescue</li> </ul>"},{"location":"#kubernetes-autoscaling","title":"Kubernetes Autoscaling","text":"<ul> <li>Allowing Kubernetes Horizontal Pod Autoscaling to Meet Persistent Volume Boundaries</li> </ul>"},{"location":"#large-language-models","title":"Large Language Models","text":"<ul> <li>Unlocking Multimodal Understanding with InternVL 1.5</li> </ul>"},{"location":"#linux","title":"Linux","text":"<ul> <li>TcpStream::set_linger Can Lead to Blocking in Tokio</li> </ul>"},{"location":"#memory-detection","title":"Memory Detection","text":"<ul> <li>AMD Memory Detection Routines Ignore Unified Memory on AMD APU</li> </ul>"},{"location":"#moe-architecture","title":"MoE Architecture","text":"<ul> <li>DeepSeek-R1 671B Q4_K_M Error: Model Requires More System Memory Than Available</li> </ul>"},{"location":"#model-optimization","title":"Model Optimization","text":"<ul> <li>Unlocking CPU Potential in Model-Driven Applications</li> </ul>"},{"location":"#modulenotfounderror","title":"ModuleNotFoundError","text":"<ul> <li>ModuleNotFoundError: No module named 'langchain.schema</li> </ul>"},{"location":"#multimodal-understanding","title":"Multimodal Understanding","text":"<ul> <li>Unlocking Multimodal Understanding with InternVL 1.5</li> </ul>"},{"location":"#network-optimization","title":"Network Optimization","text":"<ul> <li>AIOps: AI-Driven Operations for Enhanced Network Performance</li> </ul>"},{"location":"#nextjs","title":"Next.js","text":"<ul> <li>InternalError: too much recursion on Firefox</li> <li>InternalError: too much recursion on Firefox</li> <li>Functions and buttons from react-big-calendar don't work in next 13.4.9 and upwards</li> </ul>"},{"location":"#ollama","title":"Ollama","text":"<ul> <li>AMD Memory Detection Routines Ignore Unified Memory on AMD APU</li> </ul>"},{"location":"#open-source-models","title":"Open-Source Models","text":"<ul> <li>Unlocking Multimodal Understanding with InternVL 1.5</li> </ul>"},{"location":"#pdb-constraints","title":"PDB Constraints","text":"<ul> <li>Allowing Kubernetes Horizontal Pod Autoscaling to Meet Persistent Volume Boundaries</li> </ul>"},{"location":"#performance","title":"Performance","text":"<ul> <li>InternalError: too much recursion on Firefox</li> </ul>"},{"location":"#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Racing to Zero: Mitigating Race Conditions in Kubernetes Schedulers</li> </ul>"},{"location":"#persistent-volumes","title":"Persistent Volumes","text":"<ul> <li>Enabling Scalable PDB Constraints in Kubernetes</li> </ul>"},{"location":"#pod-failure","title":"Pod Failure","text":"<ul> <li>Detecting and Handling Stuck Pods due to Invalid Image Pulls</li> <li>Failing Stuck Pods due to Invalid Images: A Mechanism for Rescue</li> </ul>"},{"location":"#pod-termination","title":"Pod Termination","text":"<ul> <li>Pods with Zero TerminationGracePeriod are Force-Deleted</li> </ul>"},{"location":"#python","title":"Python","text":"<ul> <li>ModuleNotFoundError: No module named 'langchain.schema</li> </ul>"},{"location":"#quotas","title":"Quotas","text":"<ul> <li>Enabling Label Selector Support in Resource Quota's ScopeSelector</li> </ul>"},{"location":"#rfc-2045","title":"RFC 2045","text":"<ul> <li>Tracking Issue for RFC 2045: Improving <code>#[target_feature]</code></li> </ul>"},{"location":"#react-1900","title":"React 19.0.0","text":"<ul> <li>React 19.0.0 actQueue Infinite Growth Bug</li> </ul>"},{"location":"#react-big-calendar","title":"React Big Calendar","text":"<ul> <li>Functions and buttons from react-big-calendar don't work in next 13.4.9 and upwards</li> </ul>"},{"location":"#react-hooks","title":"React Hooks","text":"<ul> <li>Different Behavior between useTransition and useDeferredValue in React</li> <li>Different Behaivor between useTransition and useDeferredValue in React</li> </ul>"},{"location":"#remote-state","title":"Remote State","text":"<ul> <li>Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state</li> </ul>"},{"location":"#rust","title":"Rust","text":"<ul> <li>Backwards-Incompatible Assert Desugaring Change in Rust</li> <li>tokio::fs::File::write Returns Early Before OS Says Operation is Completed</li> </ul>"},{"location":"#rust-feature-gates","title":"Rust Feature Gates","text":"<ul> <li>Tracking Issue for RFC 2045: Improving <code>#[target_feature]</code></li> </ul>"},{"location":"#s3","title":"S3","text":"<ul> <li>Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state</li> </ul>"},{"location":"#so_linger","title":"SO_LINGER","text":"<ul> <li>TcpStream::set_linger Can Lead to Blocking in Tokio</li> </ul>"},{"location":"#scheduler","title":"Scheduler","text":"<ul> <li>Racing to Zero: Mitigating Race Conditions in Kubernetes Schedulers</li> </ul>"},{"location":"#scopeselector","title":"ScopeSelector","text":"<ul> <li>Enabling Label Selector Support in Resource Quota's ScopeSelector</li> </ul>"},{"location":"#series","title":"Series","text":"<ul> <li>Series.replace not working on slices of heterogeneous types</li> <li>Series.replace Fails on Slices of Heterogeneous Types</li> </ul>"},{"location":"#state-management","title":"State Management","text":"<ul> <li>Different Behavior between useTransition and useDeferredValue in React</li> <li>Different Behaivor between useTransition and useDeferredValue in React</li> </ul>"},{"location":"#statefulsets","title":"StatefulSets","text":"<ul> <li>Pods with Zero TerminationGracePeriod are Force-Deleted</li> </ul>"},{"location":"#tsc-server","title":"TSC Server","text":"<ul> <li>TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide</li> </ul>"},{"location":"#tcpstream","title":"TcpStream","text":"<ul> <li>TcpStream::set_linger Can Lead to Blocking in Tokio</li> </ul>"},{"location":"#terraform","title":"Terraform","text":"<ul> <li>Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state</li> </ul>"},{"location":"#transition-optimization","title":"Transition Optimization","text":"<ul> <li>Different Behaivor between useTransition and useDeferredValue in React</li> </ul>"},{"location":"#transitioning","title":"Transitioning","text":"<ul> <li>Different Behavior between useTransition and useDeferredValue in React</li> </ul>"},{"location":"#type-checking","title":"Type Checking","text":"<ul> <li>Enabling Real-Time Type Checking in JavaScript Projects with TypeScript Annotations</li> <li>TypeScript Type Annotations as Comments: Revolutionizing Code Reviews and Development</li> </ul>"},{"location":"#typescript","title":"TypeScript","text":"<ul> <li>Enabling Real-Time Type Checking in JavaScript Projects with TypeScript Annotations</li> <li>TypeScript Type Annotations as Comments: Revolutionizing Code Reviews and Development</li> <li>Supporting Closed-File Diagnostics in VS Code</li> <li>Support Closed-File Diagnostics in VS Code</li> <li>Enabling Real-Time Type Checking in JavaScript Projects with Comment Annotations</li> <li>TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide</li> </ul>"},{"location":"#typescript_1","title":"Typescript","text":"<ul> <li>Enabling TypeScript Type Checking in JavaScript Projects with Inline Comments</li> </ul>"},{"location":"#typescript-type-annotations-as-comments","title":"Typescript type annotations as comments","text":"<ul> <li>Enabling Real-Time Type Checking in JavaScript Projects with Comment Annotations</li> </ul>"},{"location":"#vs-code","title":"VS Code","text":"<ul> <li>Supporting Closed-File Diagnostics in VS Code</li> <li>Support Closed-File Diagnostics in VS Code</li> </ul>"},{"location":"#vscode","title":"VSCode","text":"<ul> <li>TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide</li> </ul>"},{"location":"#actqueue","title":"actQueue","text":"<ul> <li>React 19.0.0 actQueue Infinite Growth Bug</li> </ul>"},{"location":"#apoc","title":"apoc","text":"<ul> <li>Resolving APOC Procedures Error in Langchain with Neo4j v5.9</li> </ul>"},{"location":"#apple-silicon","title":"apple-silicon","text":"<ul> <li>Wrong Architecture Objects Mixed in Self-Built Compiler on Apple Silicon Hosts</li> </ul>"},{"location":"#assert-desugaring","title":"assert desugaring","text":"<ul> <li>Backwards-Incompatible Assert Desugaring Change in Rust</li> </ul>"},{"location":"#async-programming","title":"async programming","text":"<ul> <li>Replacing Runtime+LocalSet with a LocalRuntime in Tokio</li> <li>A System Crash in Tokio-based Systems</li> <li>A catchy, SEO-friendly title</li> <li>Why Tokio's <code>File::write</code> Returns Early Before OS Completes the Operation</li> </ul>"},{"location":"#asynchronous-programming","title":"asynchronous programming","text":"<ul> <li>tokio::fs::File::write Returns Early Before OS Says Operation is Completed</li> </ul>"},{"location":"#autodiff","title":"autodiff","text":"<ul> <li>Building Rust from Source with Autodiff on Mac OS</li> <li>Building Rust with Autodiff on Mac OS Fails</li> </ul>"},{"location":"#automation","title":"automation","text":"<ul> <li>Automate Model Updates with Ollama</li> <li>Automate Model Updates with Batch Pulling</li> <li>Automate Model Updates for Ollama Models</li> </ul>"},{"location":"#backwards-incompatibility","title":"backwards-incompatibility","text":"<ul> <li>Backwards-Incompatible Assert Desugaring Change in Rust</li> </ul>"},{"location":"#bug","title":"bug","text":"<ul> <li>Pandas Rounding Issue: Empty Series Should Return Empty Series</li> </ul>"},{"location":"#build-hangs","title":"build-hangs","text":"<ul> <li>Build Hangs on Linux with Nightly Toolchain</li> </ul>"},{"location":"#building-issues","title":"building-issues","text":"<ul> <li>Building Issues with Rust's Nightly Toolchain</li> </ul>"},{"location":"#chat-endpoint","title":"chat-endpoint","text":"<ul> <li>Support for Multiple Images in /chat Endpoint</li> </ul>"},{"location":"#chromadb","title":"chromadb","text":"<ul> <li>Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?</li> <li>Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?</li> </ul>"},{"location":"#compiler","title":"compiler","text":"<ul> <li>Wrong Architecture Objects Mixed in Self-Built Compiler on Apple Silicon Hosts</li> </ul>"},{"location":"#configuration","title":"configuration","text":"<ul> <li>Defaulting Service Links to Disabled in Kubernetes</li> </ul>"},{"location":"#conflict","title":"conflict","text":"<ul> <li>Resolving the 'rustc-docs' Installation Conflict in Rust</li> </ul>"},{"location":"#copy-on-write","title":"copy-on-write","text":"<ul> <li>Shared Mutable State in Pandas Index Labels</li> <li>Copy-on-Write (CoW) in Pandas: Index Labels as Shared Mutable State</li> <li>Copy-on-Write Index Labels Are Still Shared Mutable State</li> </ul>"},{"location":"#cpu-apple","title":"cpu-apple","text":"<ul> <li>Ollama Model Download Issue with Reverting Progress</li> </ul>"},{"location":"#data-manipulation","title":"data manipulation","text":"<ul> <li>Shared Mutable State in Pandas Index Labels</li> <li>Rounding Errors in Pandas Series: A Potential Pitfall</li> </ul>"},{"location":"#debug-builds","title":"debug builds","text":"<ul> <li>A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z</li> </ul>"},{"location":"#deepseek-r1_1","title":"deepseek-r1","text":"<ul> <li>Error When Downloading DeepSeek-R1:7b with Ollama</li> </ul>"},{"location":"#docker","title":"docker","text":"<ul> <li>Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest</li> </ul>"},{"location":"#downloading","title":"downloading","text":"<ul> <li>Error When Downloading DeepSeek-R1:7b with Ollama</li> </ul>"},{"location":"#duplicate-paths","title":"duplicate-paths","text":"<ul> <li>Route Interception Issues in Next.js with Duplicate Paths</li> </ul>"},{"location":"#error","title":"error","text":"<ul> <li>Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?</li> </ul>"},{"location":"#error-message","title":"error-message","text":"<ul> <li>Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?</li> </ul>"},{"location":"#errors","title":"errors","text":"<ul> <li>Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest</li> </ul>"},{"location":"#fetch","title":"fetch","text":"<ul> <li>Fetch Request Memoization Not Working When Cookies Function Imported</li> </ul>"},{"location":"#file-system-operations","title":"file system operations","text":"<ul> <li>Why Tokio's <code>File::write</code> Returns Early Before OS Completes the Operation</li> </ul>"},{"location":"#force-frame-pointersyes","title":"force-frame-pointers=yes","text":"<ul> <li>A stacktrace in RISC-V builds with <code>force-frame-pointers=yes</code> and <code>opt-level=z</code>: The Unexplained Disappearance</li> </ul>"},{"location":"#frame-pointers","title":"frame-pointers","text":"<ul> <li>A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z</li> </ul>"},{"location":"#getinitialprops","title":"getInitialProps","text":"<ul> <li>Pages Router + getInitialProps = Static worker unexpectedly</li> </ul>"},{"location":"#gpu-apple","title":"gpu-apple","text":"<ul> <li>Ollama Model Download Issue with Reverting Progress</li> </ul>"},{"location":"#graph-database","title":"graph database","text":"<ul> <li>Resolving APOC Procedures Error in Langchain with Neo4j v5.9</li> </ul>"},{"location":"#heterogenous-types","title":"heterogenous types","text":"<ul> <li>Series.replace not working on slices of heterogeneous types</li> </ul>"},{"location":"#horizontal-pod-autoscaling_1","title":"horizontal-pod-autoscaling","text":"<ul> <li>Allowing Kubernetes Horizontal Pod Autoscaler to Meet Pod Disruption Budget Constraints</li> </ul>"},{"location":"#html-parsing","title":"html-parsing","text":"<ul> <li>Support ReaderLM-v2 for Efficient HTML Parsing and Text Extraction</li> </ul>"},{"location":"#hub","title":"hub","text":"<ul> <li>Importing Hub Fails with Latest langchain Version 0.3.9</li> </ul>"},{"location":"#immutable-objects","title":"immutable objects","text":"<ul> <li>Simplifying Immutable Interfaces in TypeScript</li> </ul>"},{"location":"#import-error","title":"import error","text":"<ul> <li>Importing Hub Fails with Latest langchain Version 0.3.9</li> </ul>"},{"location":"#incremental-compilation","title":"incremental compilation","text":"<ul> <li>Incremental Compilation Warning on Windows: A Step-by-Step Fix</li> </ul>"},{"location":"#index-labels","title":"index labels","text":"<ul> <li>Copy-on-Write (CoW) in Pandas: Index Labels as Shared Mutable State</li> <li>Copy-on-Write Index Labels Are Still Shared Mutable State</li> </ul>"},{"location":"#infinite-growth","title":"infinite growth","text":"<ul> <li>React 19.0.0 actQueue Infinite Growth Bug</li> </ul>"},{"location":"#installation","title":"installation","text":"<ul> <li>Resolving the 'rustc-docs' Installation Conflict in Rust</li> <li>Can't Install rustc-docs Component: Resolving the Conflict</li> </ul>"},{"location":"#integer","title":"integer","text":"<ul> <li>Inherent Unchecked Integer Methods in Rust</li> </ul>"},{"location":"#integer-powers","title":"integer-powers","text":"<ul> <li>Inherent Unchecked Integer Methods in Rust</li> </ul>"},{"location":"#isr-cache","title":"isr cache","text":"<ul> <li>ISR Fails to Serve 404 Pages After Page Deletion with Experimental ISR Memory Cache Size Set to 0</li> </ul>"},{"location":"#issue","title":"issue","text":"<ul> <li>kubernetes publishing bot is broken</li> </ul>"},{"location":"#jinja2","title":"jinja2","text":"<ul> <li>Jinja2 loop.index0 blocked by RestrictedSandboxedEnvironment in LangChain</li> </ul>"},{"location":"#keyword1","title":"keyword1","text":"<ul> <li>A catchy, SEO-friendly title</li> <li>Fixing the Busy Wait Loop in Repro</li> </ul>"},{"location":"#keyword2","title":"keyword2","text":"<ul> <li>A catchy, SEO-friendly title</li> <li>Fixing the Busy Wait Loop in Repro</li> </ul>"},{"location":"#kubectl","title":"kubectl","text":"<ul> <li>Enabling Selector in kubectl Create Service Command</li> </ul>"},{"location":"#kubelet","title":"kubelet","text":"<ul> <li>Introducing Permanent and Transient Error Handling in kubelet</li> <li>Surface Permanent Errors in Kubelet and DRA Drivers</li> </ul>"},{"location":"#kubernetes_1","title":"kubernetes","text":"<ul> <li>kubernetes publishing bot is broken</li> <li>Defaulting Service Links to Disabled in Kubernetes</li> <li>Set enableServiceLinks to False as Default in Kubernetes</li> </ul>"},{"location":"#langchain","title":"langchain","text":"<ul> <li>Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?</li> <li>Importing Hub Fails with Latest langchain Version 0.3.9</li> <li>ModuleNotFoundError: No module named 'langchain.schema</li> <li>Resolving APOC Procedures Error in Langchain with Neo4j v5.9</li> </ul>"},{"location":"#langchain-ai","title":"langchain-ai","text":"<ul> <li>Jinja2 loop.index0 blocked by RestrictedSandboxedEnvironment in LangChain</li> </ul>"},{"location":"#langchain-chroma","title":"langchain-chroma","text":"<ul> <li>Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?</li> </ul>"},{"location":"#linux_1","title":"linux","text":"<ul> <li>Building Issues with Rust's Nightly Toolchain</li> <li>Build Hangs on Linux with Nightly Toolchain</li> </ul>"},{"location":"#localset","title":"localset","text":"<ul> <li>Introducing LocalRuntime: Simplifying Task Spawning in Tokio</li> </ul>"},{"location":"#mac-os","title":"mac os","text":"<ul> <li>Building Rust from Source with Autodiff on Mac OS</li> <li>Building Rust with Autodiff on Mac OS Fails</li> </ul>"},{"location":"#machine-learning-models","title":"machine-learning-models","text":"<ul> <li>Will FinGPT Be Supported by Ollama Soon?</li> </ul>"},{"location":"#macos","title":"macos","text":"<ul> <li>Ollama Model Download Issue with Reverting Progress</li> </ul>"},{"location":"#manifest","title":"manifest","text":"<ul> <li>Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest</li> </ul>"},{"location":"#manifest-errors","title":"manifest errors","text":"<ul> <li>Resolving Pull Manifest Errors in Ollama</li> </ul>"},{"location":"#memoization","title":"memoization","text":"<ul> <li>Fetch Request Memoization Not Working When Cookies Function Imported</li> </ul>"},{"location":"#memory-caching","title":"memory caching","text":"<ul> <li>ISR Fails to Serve 404 Pages After Page Deletion with Experimental ISR Memory Cache Size Set to 0</li> </ul>"},{"location":"#model-updates","title":"model updates","text":"<ul> <li>Automate Model Updates with Batch Pulling</li> <li>Automate Model Updates for Ollama Models</li> </ul>"},{"location":"#model-management","title":"model-management","text":"<ul> <li>Automate Model Updates with Ollama</li> </ul>"},{"location":"#multimodal-large-language-models","title":"multimodal large language models","text":"<ul> <li>Supporting InternVL-Chat-V1.5 in Hugging Face Models</li> </ul>"},{"location":"#multiple-images","title":"multiple-images","text":"<ul> <li>Support for Multiple Images in /chat Endpoint</li> </ul>"},{"location":"#natural-language-processing","title":"natural-language-processing","text":"<ul> <li>Will FinGPT Be Supported by Ollama Soon?</li> </ul>"},{"location":"#neo4j","title":"neo4j","text":"<ul> <li>Resolving APOC Procedures Error in Langchain with Neo4j v5.9</li> </ul>"},{"location":"#nextjs_1","title":"next.js","text":"<ul> <li>Pages Router + getInitialProps = Static worker unexpectedly</li> <li>ISR Fails to Serve 404 Pages After Page Deletion with Experimental ISR Memory Cache Size Set to 0</li> </ul>"},{"location":"#nextjs_2","title":"nextjs","text":"<ul> <li>Route Interception Issues in Next.js with Duplicate Paths</li> <li>Fetch Request Memoization Not Working When Cookies Function Imported</li> </ul>"},{"location":"#nightly-toolchain","title":"nightly-toolchain","text":"<ul> <li>Build Hangs on Linux with Nightly Toolchain</li> </ul>"},{"location":"#ollama_1","title":"ollama","text":"<ul> <li>Automate Model Updates for Ollama Models</li> <li>Error When Downloading DeepSeek-R1:7b with Ollama</li> <li>DeepSeek-R1 671B Q4_K_M Error: Model Requires More System Memory Than Available</li> <li>Resolving Pull Manifest Errors in Ollama</li> <li>Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest</li> </ul>"},{"location":"#ollama-api","title":"ollama-api","text":"<ul> <li>Support for Multiple Images in /chat Endpoint</li> </ul>"},{"location":"#ollama-model-download","title":"ollama-model-download","text":"<ul> <li>Ollama Model Download Issue with Reverting Progress</li> </ul>"},{"location":"#ollama-version-057","title":"ollama-version-0.5.7","text":"<ul> <li>Ollama Model Download Issue with Reverting Progress</li> </ul>"},{"location":"#open-source-models_1","title":"open-source models","text":"<ul> <li>Supporting InternVL-Chat-V1.5 in Hugging Face Models</li> </ul>"},{"location":"#opt-levelz","title":"opt-level=z","text":"<ul> <li>A stacktrace in RISC-V builds with <code>force-frame-pointers=yes</code> and <code>opt-level=z</code>: The Unexplained Disappearance</li> </ul>"},{"location":"#pandas","title":"pandas","text":"<ul> <li>Series.replace not working on slices of heterogeneous types</li> <li>Shared Mutable State in Pandas Index Labels</li> <li>Series.replace not working on slices of heterogeneous types</li> <li>Copy-on-Write (CoW) in Pandas: Index Labels as Shared Mutable State</li> <li>Series.replace Fails on Slices of Heterogeneous Types</li> <li>Copy-on-Write Index Labels Are Still Shared Mutable State</li> <li>Series.replace not working on slices of heterogeneous types</li> <li>Rounding Errors in Pandas Series: A Potential Pitfall</li> <li>Pandas Rounding Issue: Empty Series Should Return Empty Series</li> <li>Pandas Series.sum() Examples Fail to Illustrate Actual Results</li> <li>Pandas Series.sum() Misrepresentation in Documentation</li> </ul>"},{"location":"#performance-optimization_1","title":"performance-optimization","text":"<ul> <li>Inherent Unchecked Integer Methods in Rust</li> </ul>"},{"location":"#pod-disruption-budget","title":"pod-disruption-budget","text":"<ul> <li>Allowing Kubernetes Horizontal Pod Autoscaler to Meet Pod Disruption Budget Constraints</li> </ul>"},{"location":"#progress-reverting","title":"progress-reverting","text":"<ul> <li>Ollama Model Download Issue with Reverting Progress</li> </ul>"},{"location":"#proprietary-commercial-models","title":"proprietary commercial models","text":"<ul> <li>Supporting InternVL-Chat-V1.5 in Hugging Face Models</li> </ul>"},{"location":"#publishing-bot","title":"publishing-bot","text":"<ul> <li>kubernetes publishing bot is broken</li> </ul>"},{"location":"#readonly-interfaces","title":"readonly interfaces","text":"<ul> <li>Simplifying Immutable Interfaces in TypeScript</li> </ul>"},{"location":"#release-builds","title":"release builds","text":"<ul> <li>A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z</li> </ul>"},{"location":"#replace","title":"replace","text":"<ul> <li>Series.replace not working on slices of heterogeneous types</li> <li>Series.replace not working on slices of heterogeneous types</li> <li>Series.replace Fails on Slices of Heterogeneous Types</li> <li>Series.replace not working on slices of heterogeneous types</li> </ul>"},{"location":"#riscv32imc-unknown-none-elf","title":"riscv32imc-unknown-none-elf","text":"<ul> <li>A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z</li> <li>A stacktrace in RISC-V builds with <code>force-frame-pointers=yes</code> and <code>opt-level=z</code>: The Unexplained Disappearance</li> </ul>"},{"location":"#rounding","title":"rounding","text":"<ul> <li>Rounding Errors in Pandas Series: A Potential Pitfall</li> <li>Pandas Rounding Issue: Empty Series Should Return Empty Series</li> </ul>"},{"location":"#route-interception","title":"route-interception","text":"<ul> <li>Route Interception Issues in Next.js with Duplicate Paths</li> </ul>"},{"location":"#runtime","title":"runtime","text":"<ul> <li>Introducing LocalRuntime: Simplifying Task Spawning in Tokio</li> </ul>"},{"location":"#runtime-localset","title":"runtime-localset","text":"<ul> <li>Elevate Tokio Performance with LocalRuntime</li> </ul>"},{"location":"#rust_1","title":"rust","text":"<ul> <li>Inherent Unchecked Integer Methods in Rust</li> <li>Building Rust from Source with Autodiff on Mac OS</li> <li>Resolving the 'rustc-docs' Installation Conflict in Rust</li> <li>Building Rust with Autodiff on Mac OS Fails</li> </ul>"},{"location":"#rust-lang","title":"rust-lang","text":"<ul> <li>Build Hangs on Linux with Nightly Toolchain</li> <li>Wrong Architecture Objects Mixed in Self-Built Compiler on Apple Silicon Hosts</li> <li>Can't Install rustc-docs Component: Resolving the Conflict</li> </ul>"},{"location":"#rust-language-features","title":"rust-language-features","text":"<ul> <li>Inherent Unchecked Integer Methods in Rust</li> </ul>"},{"location":"#rust-nightly-toolchain","title":"rust-nightly-toolchain","text":"<ul> <li>Building Issues with Rust's Nightly Toolchain</li> </ul>"},{"location":"#rustc","title":"rustc","text":"<ul> <li>Incremental Compilation Warning on Windows: A Step-by-Step Fix</li> </ul>"},{"location":"#rustc-docs","title":"rustc-docs","text":"<ul> <li>Can't Install rustc-docs Component: Resolving the Conflict</li> </ul>"},{"location":"#sensitive-attributes","title":"sensitive attributes","text":"<ul> <li>Avoiding Sensitive Attributes in Terraform State</li> </ul>"},{"location":"#series_1","title":"series","text":"<ul> <li>Series.replace not working on slices of heterogeneous types</li> <li>Series.replace not working on slices of heterogeneous types</li> <li>Pandas Series.sum() Examples Fail to Illustrate Actual Results</li> <li>Pandas Series.sum() Misrepresentation in Documentation</li> </ul>"},{"location":"#service-creation","title":"service creation","text":"<ul> <li>Enabling Selector in kubectl Create Service Command</li> </ul>"},{"location":"#service-links","title":"service links","text":"<ul> <li>Defaulting Service Links to Disabled in Kubernetes</li> </ul>"},{"location":"#service-links_1","title":"service-links","text":"<ul> <li>Set enableServiceLinks to False as Default in Kubernetes</li> </ul>"},{"location":"#static-worker","title":"static-worker","text":"<ul> <li>Pages Router + getInitialProps = Static worker unexpectedly</li> </ul>"},{"location":"#sum","title":"sum","text":"<ul> <li>Pandas Series.sum() Examples Fail to Illustrate Actual Results</li> <li>Pandas Series.sum() Misrepresentation in Documentation</li> </ul>"},{"location":"#system-crashes","title":"system crashes","text":"<ul> <li>A System Crash in Tokio-based Systems</li> </ul>"},{"location":"#templating","title":"templating","text":"<ul> <li>Jinja2 loop.index0 blocked by RestrictedSandboxedEnvironment in LangChain</li> </ul>"},{"location":"#terraform-state","title":"terraform state","text":"<ul> <li>Avoiding Sensitive Attributes in Terraform State</li> </ul>"},{"location":"#text-extraction","title":"text-extraction","text":"<ul> <li>Support ReaderLM-v2 for Efficient HTML Parsing and Text Extraction</li> </ul>"},{"location":"#thread-safety","title":"thread safety","text":"<ul> <li>A catchy, SEO-friendly title</li> </ul>"},{"location":"#tokio","title":"tokio","text":"<ul> <li>TcpStream::set_linger Can Lead to Blocking in Tokio</li> <li>tokio::fs::File::write Returns Early Before OS Says Operation is Completed</li> </ul>"},{"location":"#tokio-runtime","title":"tokio runtime","text":"<ul> <li>A catchy, SEO-friendly title</li> </ul>"},{"location":"#tokio-147","title":"tokio-1.47","text":"<ul> <li>A System Crash in Tokio-based Systems</li> </ul>"},{"location":"#tokio-performance","title":"tokio-performance","text":"<ul> <li>Elevate Tokio Performance with LocalRuntime</li> </ul>"},{"location":"#tokio-rs","title":"tokio-rs","text":"<ul> <li>Replacing Runtime+LocalSet with a LocalRuntime in Tokio</li> <li>Introducing LocalRuntime: Simplifying Task Spawning in Tokio</li> <li>Why Tokio's <code>File::write</code> Returns Early Before OS Completes the Operation</li> </ul>"},{"location":"#transformer-based","title":"transformer-based","text":"<ul> <li>Support ReaderLM-v2 for Efficient HTML Parsing and Text Extraction</li> </ul>"},{"location":"#type-safety","title":"type safety","text":"<ul> <li>Adding Length Parameter to Typed Arrays</li> </ul>"},{"location":"#typed-arrays","title":"typed arrays","text":"<ul> <li>Adding Length Parameter to Typed Arrays</li> </ul>"},{"location":"#unchecked","title":"unchecked","text":"<ul> <li>Inherent Unchecked Integer Methods in Rust</li> </ul>"},{"location":"#wait-loop","title":"wait-loop","text":"<ul> <li>Fixing the Busy Wait Loop in Repro</li> </ul>"},{"location":"#windows","title":"windows","text":"<ul> <li>Incremental Compilation Warning on Windows: A Step-by-Step Fix</li> </ul>"},{"location":"#tag:amd","title":"AMD","text":"<ul> <li>            AMD Memory Detection Routines Ignore Unified Memory on AMD APU          </li> </ul>"},{"location":"#tag:aws","title":"AWS","text":"<ul> <li>            Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state          </li> </ul>"},{"location":"#tag:artificial-intelligence","title":"Artificial Intelligence","text":"<ul> <li>            AIOps: AI-Driven Operations for Enhanced Network Performance          </li> </ul>"},{"location":"#tag:bug-fix","title":"Bug Fix","text":"<ul> <li>            Functions and buttons from react-big-calendar don't work in next 13.4.9 and upwards          </li> </ul>"},{"location":"#tag:code-completion","title":"Code Completion","text":"<ul> <li>            Enabling Real-Time Type Checking in JavaScript Projects with TypeScript Annotations          </li> </ul>"},{"location":"#tag:dra","title":"DRA","text":"<ul> <li>            Introducing Permanent and Transient Error Handling in kubelet          </li> </ul>"},{"location":"#tag:dra-drivers","title":"DRA drivers","text":"<ul> <li>            Surface Permanent Errors in Kubelet and DRA Drivers          </li> </ul>"},{"location":"#tag:debugging","title":"Debugging","text":"<ul> <li>            TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide          </li> </ul>"},{"location":"#tag:deep-learning","title":"Deep Learning","text":"<ul> <li>            Unlocking CPU Potential in Model-Driven Applications          </li> </ul>"},{"location":"#tag:deepseek-r1","title":"DeepSeek-R1","text":"<ul> <li>            DeepSeek-R1 671B Q4_K_M Error: Model Requires More System Memory Than Available          </li> </ul>"},{"location":"#tag:diagnostics","title":"Diagnostics","text":"<ul> <li>            Supporting Closed-File Diagnostics in VS Code          </li> </ul>"},{"location":"#tag:disruption-budget","title":"Disruption Budget","text":"<ul> <li>            Enabling Scalable PDB Constraints in Kubernetes          </li> </ul>"},{"location":"#tag:error-handling","title":"Error Handling","text":"<ul> <li>            InternalError: too much recursion on Firefox          </li> <li>            InternalError: too much recursion on Firefox          </li> </ul>"},{"location":"#tag:horizontal-pod-autoscaling","title":"Horizontal Pod Autoscaling","text":"<ul> <li>            Enabling Scalable PDB Constraints in Kubernetes          </li> </ul>"},{"location":"#tag:image-pull-issues","title":"Image Pull Issues","text":"<ul> <li>            Detecting and Handling Stuck Pods due to Invalid Image Pulls          </li> </ul>"},{"location":"#tag:image-validation","title":"Image Validation","text":"<ul> <li>            Failing Stuck Pods due to Invalid Images: A Mechanism for Rescue          </li> </ul>"},{"location":"#tag:inline-comments","title":"Inline Comments","text":"<ul> <li>            Enabling TypeScript Type Checking in JavaScript Projects with Inline Comments          </li> </ul>"},{"location":"#tag:intellisense-and-code-completion","title":"Intellisense and Code Completion","text":"<ul> <li>            Enabling Real-Time Type Checking in JavaScript Projects with Comment Annotations          </li> </ul>"},{"location":"#tag:javascript","title":"JavaScript","text":"<ul> <li>            Enabling TypeScript Type Checking in JavaScript Projects with Inline Comments          </li> <li>            TypeScript Type Annotations as Comments: Revolutionizing Code Reviews and Development          </li> </ul>"},{"location":"#tag:kubernetes","title":"Kubernetes","text":"<ul> <li>            Detecting and Handling Stuck Pods due to Invalid Image Pulls          </li> <li>            Enabling Label Selector Support in Resource Quota's ScopeSelector          </li> <li>            Enabling Scalable PDB Constraints in Kubernetes          </li> <li>            Enabling Selector in kubectl Create Service Command          </li> <li>            Failing Stuck Pods due to Invalid Images: A Mechanism for Rescue          </li> <li>            Introducing Permanent and Transient Error Handling in kubelet          </li> <li>            Pods with Zero TerminationGracePeriod are Force-Deleted          </li> <li>            Racing to Zero: Mitigating Race Conditions in Kubernetes Schedulers          </li> <li>            Surface Permanent Errors in Kubelet and DRA Drivers          </li> </ul>"},{"location":"#tag:kubernetes-autoscaling","title":"Kubernetes Autoscaling","text":"<ul> <li>            Allowing Kubernetes Horizontal Pod Autoscaling to Meet Persistent Volume Boundaries          </li> </ul>"},{"location":"#tag:large-language-models","title":"Large Language Models","text":"<ul> <li>            Unlocking Multimodal Understanding with InternVL 1.5          </li> </ul>"},{"location":"#tag:linux","title":"Linux","text":"<ul> <li>            TcpStream::set_linger Can Lead to Blocking in Tokio          </li> </ul>"},{"location":"#tag:memory-detection","title":"Memory Detection","text":"<ul> <li>            AMD Memory Detection Routines Ignore Unified Memory on AMD APU          </li> </ul>"},{"location":"#tag:moe-architecture","title":"MoE Architecture","text":"<ul> <li>            DeepSeek-R1 671B Q4_K_M Error: Model Requires More System Memory Than Available          </li> </ul>"},{"location":"#tag:model-optimization","title":"Model Optimization","text":"<ul> <li>            Unlocking CPU Potential in Model-Driven Applications          </li> </ul>"},{"location":"#tag:modulenotfounderror","title":"ModuleNotFoundError","text":"<ul> <li>            ModuleNotFoundError: No module named 'langchain.schema'          </li> </ul>"},{"location":"#tag:multimodal-understanding","title":"Multimodal Understanding","text":"<ul> <li>            Unlocking Multimodal Understanding with InternVL 1.5          </li> </ul>"},{"location":"#tag:network-optimization","title":"Network Optimization","text":"<ul> <li>            AIOps: AI-Driven Operations for Enhanced Network Performance          </li> </ul>"},{"location":"#tag:nextjs","title":"Next.js","text":"<ul> <li>            Functions and buttons from react-big-calendar don't work in next 13.4.9 and upwards          </li> <li>            InternalError: too much recursion on Firefox          </li> <li>            InternalError: too much recursion on Firefox          </li> </ul>"},{"location":"#tag:ollama","title":"Ollama","text":"<ul> <li>            AMD Memory Detection Routines Ignore Unified Memory on AMD APU          </li> </ul>"},{"location":"#tag:open-source-models","title":"Open-Source Models","text":"<ul> <li>            Unlocking Multimodal Understanding with InternVL 1.5          </li> </ul>"},{"location":"#tag:pdb-constraints","title":"PDB Constraints","text":"<ul> <li>            Allowing Kubernetes Horizontal Pod Autoscaling to Meet Persistent Volume Boundaries          </li> </ul>"},{"location":"#tag:performance","title":"Performance","text":"<ul> <li>            InternalError: too much recursion on Firefox          </li> </ul>"},{"location":"#tag:performance-optimization","title":"Performance Optimization","text":"<ul> <li>            Racing to Zero: Mitigating Race Conditions in Kubernetes Schedulers          </li> </ul>"},{"location":"#tag:persistent-volumes","title":"Persistent Volumes","text":"<ul> <li>            Enabling Scalable PDB Constraints in Kubernetes          </li> </ul>"},{"location":"#tag:pod-failure","title":"Pod Failure","text":"<ul> <li>            Detecting and Handling Stuck Pods due to Invalid Image Pulls          </li> <li>            Failing Stuck Pods due to Invalid Images: A Mechanism for Rescue          </li> </ul>"},{"location":"#tag:pod-termination","title":"Pod Termination","text":"<ul> <li>            Pods with Zero TerminationGracePeriod are Force-Deleted          </li> </ul>"},{"location":"#tag:python","title":"Python","text":"<ul> <li>            ModuleNotFoundError: No module named 'langchain.schema'          </li> </ul>"},{"location":"#tag:quotas","title":"Quotas","text":"<ul> <li>            Enabling Label Selector Support in Resource Quota's ScopeSelector          </li> </ul>"},{"location":"#tag:rfc-2045","title":"RFC 2045","text":"<ul> <li>            Tracking Issue for RFC 2045: Improving `#[target_feature]`          </li> </ul>"},{"location":"#tag:react-1900","title":"React 19.0.0","text":"<ul> <li>            React 19.0.0 actQueue Infinite Growth Bug          </li> </ul>"},{"location":"#tag:react-big-calendar","title":"React Big Calendar","text":"<ul> <li>            Functions and buttons from react-big-calendar don't work in next 13.4.9 and upwards          </li> </ul>"},{"location":"#tag:react-hooks","title":"React Hooks","text":"<ul> <li>            Different Behaivor between useTransition and useDeferredValue in React          </li> <li>            Different Behavior between useTransition and useDeferredValue in React          </li> </ul>"},{"location":"#tag:remote-state","title":"Remote State","text":"<ul> <li>            Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state          </li> </ul>"},{"location":"#tag:rust","title":"Rust","text":"<ul> <li>            Backwards-Incompatible Assert Desugaring Change in Rust          </li> <li>            tokio::fs::File::write Returns Early Before OS Says Operation is Completed          </li> </ul>"},{"location":"#tag:rust-feature-gates","title":"Rust Feature Gates","text":"<ul> <li>            Tracking Issue for RFC 2045: Improving `#[target_feature]`          </li> </ul>"},{"location":"#tag:s3","title":"S3","text":"<ul> <li>            Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state          </li> </ul>"},{"location":"#tag:so_linger","title":"SO_LINGER","text":"<ul> <li>            TcpStream::set_linger Can Lead to Blocking in Tokio          </li> </ul>"},{"location":"#tag:scheduler","title":"Scheduler","text":"<ul> <li>            Racing to Zero: Mitigating Race Conditions in Kubernetes Schedulers          </li> </ul>"},{"location":"#tag:scopeselector","title":"ScopeSelector","text":"<ul> <li>            Enabling Label Selector Support in Resource Quota's ScopeSelector          </li> </ul>"},{"location":"#tag:series","title":"Series","text":"<ul> <li>            Series.replace Fails on Slices of Heterogeneous Types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> </ul>"},{"location":"#tag:state-management","title":"State Management","text":"<ul> <li>            Different Behaivor between useTransition and useDeferredValue in React          </li> <li>            Different Behavior between useTransition and useDeferredValue in React          </li> </ul>"},{"location":"#tag:statefulsets","title":"StatefulSets","text":"<ul> <li>            Pods with Zero TerminationGracePeriod are Force-Deleted          </li> </ul>"},{"location":"#tag:tsc-server","title":"TSC Server","text":"<ul> <li>            TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide          </li> </ul>"},{"location":"#tag:tcpstream","title":"TcpStream","text":"<ul> <li>            TcpStream::set_linger Can Lead to Blocking in Tokio          </li> </ul>"},{"location":"#tag:terraform","title":"Terraform","text":"<ul> <li>            Terraform 0.14 \u2192 1.13.1: apply re-creates existing AWS resources (VPC/Subnets/NAT) instead of updating despite S3 remote state          </li> </ul>"},{"location":"#tag:transition-optimization","title":"Transition Optimization","text":"<ul> <li>            Different Behaivor between useTransition and useDeferredValue in React          </li> </ul>"},{"location":"#tag:transitioning","title":"Transitioning","text":"<ul> <li>            Different Behavior between useTransition and useDeferredValue in React          </li> </ul>"},{"location":"#tag:type-checking","title":"Type Checking","text":"<ul> <li>            Enabling Real-Time Type Checking in JavaScript Projects with TypeScript Annotations          </li> <li>            TypeScript Type Annotations as Comments: Revolutionizing Code Reviews and Development          </li> </ul>"},{"location":"#tag:typescript","title":"TypeScript","text":"<ul> <li>            Enabling Real-Time Type Checking in JavaScript Projects with Comment Annotations          </li> <li>            Enabling Real-Time Type Checking in JavaScript Projects with TypeScript Annotations          </li> <li>            Support Closed-File Diagnostics in VS Code          </li> <li>            Supporting Closed-File Diagnostics in VS Code          </li> <li>            TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide          </li> <li>            TypeScript Type Annotations as Comments: Revolutionizing Code Reviews and Development          </li> </ul>"},{"location":"#tag:typescript","title":"Typescript","text":"<ul> <li>            Enabling TypeScript Type Checking in JavaScript Projects with Inline Comments          </li> </ul>"},{"location":"#tag:typescript-type-annotations-as-comments","title":"Typescript type annotations as comments","text":"<ul> <li>            Enabling Real-Time Type Checking in JavaScript Projects with Comment Annotations          </li> </ul>"},{"location":"#tag:vs-code","title":"VS Code","text":"<ul> <li>            Support Closed-File Diagnostics in VS Code          </li> <li>            Supporting Closed-File Diagnostics in VS Code          </li> </ul>"},{"location":"#tag:vscode","title":"VSCode","text":"<ul> <li>            TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide          </li> </ul>"},{"location":"#tag:actqueue","title":"actQueue","text":"<ul> <li>            React 19.0.0 actQueue Infinite Growth Bug          </li> </ul>"},{"location":"#tag:apoc","title":"apoc","text":"<ul> <li>            Resolving APOC Procedures Error in Langchain with Neo4j v5.9          </li> </ul>"},{"location":"#tag:apple-silicon","title":"apple-silicon","text":"<ul> <li>            Wrong Architecture Objects Mixed in Self-Built Compiler on Apple Silicon Hosts          </li> </ul>"},{"location":"#tag:assert-desugaring","title":"assert desugaring","text":"<ul> <li>            Backwards-Incompatible Assert Desugaring Change in Rust          </li> </ul>"},{"location":"#tag:async-programming","title":"async programming","text":"<ul> <li>            A System Crash in Tokio-based Systems          </li> <li>            A catchy, SEO-friendly title          </li> <li>            Replacing Runtime+LocalSet with a LocalRuntime in Tokio          </li> <li>            Why Tokio's `File::write` Returns Early Before OS Completes the Operation          </li> </ul>"},{"location":"#tag:asynchronous-programming","title":"asynchronous programming","text":"<ul> <li>            tokio::fs::File::write Returns Early Before OS Says Operation is Completed          </li> </ul>"},{"location":"#tag:autodiff","title":"autodiff","text":"<ul> <li>            Building Rust from Source with Autodiff on Mac OS          </li> <li>            Building Rust with Autodiff on Mac OS Fails          </li> </ul>"},{"location":"#tag:automation","title":"automation","text":"<ul> <li>            Automate Model Updates for Ollama Models          </li> <li>            Automate Model Updates with Batch Pulling          </li> <li>            Automate Model Updates with Ollama          </li> </ul>"},{"location":"#tag:backwards-incompatibility","title":"backwards-incompatibility","text":"<ul> <li>            Backwards-Incompatible Assert Desugaring Change in Rust          </li> </ul>"},{"location":"#tag:bug","title":"bug","text":"<ul> <li>            Pandas Rounding Issue: Empty Series Should Return Empty Series          </li> </ul>"},{"location":"#tag:build-hangs","title":"build-hangs","text":"<ul> <li>            Build Hangs on Linux with Nightly Toolchain          </li> </ul>"},{"location":"#tag:building-issues","title":"building-issues","text":"<ul> <li>            Building Issues with Rust's Nightly Toolchain          </li> </ul>"},{"location":"#tag:chat-endpoint","title":"chat-endpoint","text":"<ul> <li>            Support for Multiple Images in /chat Endpoint          </li> </ul>"},{"location":"#tag:chromadb","title":"chromadb","text":"<ul> <li>            Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?          </li> <li>            Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?          </li> </ul>"},{"location":"#tag:compiler","title":"compiler","text":"<ul> <li>            Wrong Architecture Objects Mixed in Self-Built Compiler on Apple Silicon Hosts          </li> </ul>"},{"location":"#tag:configuration","title":"configuration","text":"<ul> <li>            Defaulting Service Links to Disabled in Kubernetes          </li> </ul>"},{"location":"#tag:conflict","title":"conflict","text":"<ul> <li>            Resolving the 'rustc-docs' Installation Conflict in Rust          </li> </ul>"},{"location":"#tag:copy-on-write","title":"copy-on-write","text":"<ul> <li>            Copy-on-Write (CoW) in Pandas: Index Labels as Shared Mutable State          </li> <li>            Copy-on-Write Index Labels Are Still Shared Mutable State          </li> <li>            Shared Mutable State in Pandas Index Labels          </li> </ul>"},{"location":"#tag:cpu-apple","title":"cpu-apple","text":"<ul> <li>            Ollama Model Download Issue with Reverting Progress          </li> </ul>"},{"location":"#tag:data-manipulation","title":"data manipulation","text":"<ul> <li>            Rounding Errors in Pandas Series: A Potential Pitfall          </li> <li>            Shared Mutable State in Pandas Index Labels          </li> </ul>"},{"location":"#tag:debug-builds","title":"debug builds","text":"<ul> <li>            A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z'          </li> </ul>"},{"location":"#tag:deepseek-r1","title":"deepseek-r1","text":"<ul> <li>            Error When Downloading DeepSeek-R1:7b with Ollama          </li> </ul>"},{"location":"#tag:docker","title":"docker","text":"<ul> <li>            Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest          </li> </ul>"},{"location":"#tag:downloading","title":"downloading","text":"<ul> <li>            Error When Downloading DeepSeek-R1:7b with Ollama          </li> </ul>"},{"location":"#tag:duplicate-paths","title":"duplicate-paths","text":"<ul> <li>            Route Interception Issues in Next.js with Duplicate Paths          </li> </ul>"},{"location":"#tag:error","title":"error","text":"<ul> <li>            Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?          </li> </ul>"},{"location":"#tag:error-message","title":"error-message","text":"<ul> <li>            Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?          </li> </ul>"},{"location":"#tag:errors","title":"errors","text":"<ul> <li>            Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest          </li> </ul>"},{"location":"#tag:fetch","title":"fetch","text":"<ul> <li>            Fetch Request Memoization Not Working When Cookies Function Imported          </li> </ul>"},{"location":"#tag:file-system-operations","title":"file system operations","text":"<ul> <li>            Why Tokio's `File::write` Returns Early Before OS Completes the Operation          </li> </ul>"},{"location":"#tag:force-frame-pointersyes","title":"force-frame-pointers=yes","text":"<ul> <li>            A stacktrace in RISC-V builds with `force-frame-pointers=yes` and `opt-level=z`: The Unexplained Disappearance          </li> </ul>"},{"location":"#tag:frame-pointers","title":"frame-pointers","text":"<ul> <li>            A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z'          </li> </ul>"},{"location":"#tag:getinitialprops","title":"getInitialProps","text":"<ul> <li>            Pages Router + getInitialProps = Static worker unexpectedly          </li> </ul>"},{"location":"#tag:gpu-apple","title":"gpu-apple","text":"<ul> <li>            Ollama Model Download Issue with Reverting Progress          </li> </ul>"},{"location":"#tag:graph-database","title":"graph database","text":"<ul> <li>            Resolving APOC Procedures Error in Langchain with Neo4j v5.9          </li> </ul>"},{"location":"#tag:heterogenous-types","title":"heterogenous types","text":"<ul> <li>            Series.replace not working on slices of heterogeneous types          </li> </ul>"},{"location":"#tag:horizontal-pod-autoscaling","title":"horizontal-pod-autoscaling","text":"<ul> <li>            Allowing Kubernetes Horizontal Pod Autoscaler to Meet Pod Disruption Budget Constraints          </li> </ul>"},{"location":"#tag:html-parsing","title":"html-parsing","text":"<ul> <li>            Support ReaderLM-v2 for Efficient HTML Parsing and Text Extraction          </li> </ul>"},{"location":"#tag:hub","title":"hub","text":"<ul> <li>            Importing Hub Fails with Latest langchain Version 0.3.9          </li> </ul>"},{"location":"#tag:immutable-objects","title":"immutable objects","text":"<ul> <li>            Simplifying Immutable Interfaces in TypeScript          </li> </ul>"},{"location":"#tag:import-error","title":"import error","text":"<ul> <li>            Importing Hub Fails with Latest langchain Version 0.3.9          </li> </ul>"},{"location":"#tag:incremental-compilation","title":"incremental compilation","text":"<ul> <li>            Incremental Compilation Warning on Windows: A Step-by-Step Fix          </li> </ul>"},{"location":"#tag:index-labels","title":"index labels","text":"<ul> <li>            Copy-on-Write (CoW) in Pandas: Index Labels as Shared Mutable State          </li> <li>            Copy-on-Write Index Labels Are Still Shared Mutable State          </li> </ul>"},{"location":"#tag:infinite-growth","title":"infinite growth","text":"<ul> <li>            React 19.0.0 actQueue Infinite Growth Bug          </li> </ul>"},{"location":"#tag:installation","title":"installation","text":"<ul> <li>            Can't Install rustc-docs Component: Resolving the Conflict          </li> <li>            Resolving the 'rustc-docs' Installation Conflict in Rust          </li> </ul>"},{"location":"#tag:integer","title":"integer","text":"<ul> <li>            Inherent Unchecked Integer Methods in Rust          </li> </ul>"},{"location":"#tag:integer-powers","title":"integer-powers","text":"<ul> <li>            Inherent Unchecked Integer Methods in Rust          </li> </ul>"},{"location":"#tag:isr-cache","title":"isr cache","text":"<ul> <li>            ISR Fails to Serve 404 Pages After Page Deletion with Experimental ISR Memory Cache Size Set to 0          </li> </ul>"},{"location":"#tag:issue","title":"issue","text":"<ul> <li>            kubernetes publishing bot is broken          </li> </ul>"},{"location":"#tag:jinja2","title":"jinja2","text":"<ul> <li>            Jinja2 loop.index0 blocked by RestrictedSandboxedEnvironment in LangChain          </li> </ul>"},{"location":"#tag:keyword1","title":"keyword1","text":"<ul> <li>            A catchy, SEO-friendly title          </li> <li>            Fixing the Busy Wait Loop in Repro          </li> </ul>"},{"location":"#tag:keyword2","title":"keyword2","text":"<ul> <li>            A catchy, SEO-friendly title          </li> <li>            Fixing the Busy Wait Loop in Repro          </li> </ul>"},{"location":"#tag:kubectl","title":"kubectl","text":"<ul> <li>            Enabling Selector in kubectl Create Service Command          </li> </ul>"},{"location":"#tag:kubelet","title":"kubelet","text":"<ul> <li>            Introducing Permanent and Transient Error Handling in kubelet          </li> <li>            Surface Permanent Errors in Kubelet and DRA Drivers          </li> </ul>"},{"location":"#tag:kubernetes","title":"kubernetes","text":"<ul> <li>            Defaulting Service Links to Disabled in Kubernetes          </li> <li>            Set enableServiceLinks to False as Default in Kubernetes          </li> <li>            kubernetes publishing bot is broken          </li> </ul>"},{"location":"#tag:langchain","title":"langchain","text":"<ul> <li>            Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?          </li> <li>            Importing Hub Fails with Latest langchain Version 0.3.9          </li> <li>            ModuleNotFoundError: No module named 'langchain.schema'          </li> <li>            Resolving APOC Procedures Error in Langchain with Neo4j v5.9          </li> </ul>"},{"location":"#tag:langchain-ai","title":"langchain-ai","text":"<ul> <li>            Jinja2 loop.index0 blocked by RestrictedSandboxedEnvironment in LangChain          </li> </ul>"},{"location":"#tag:langchain-chroma","title":"langchain-chroma","text":"<ul> <li>            Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?          </li> </ul>"},{"location":"#tag:linux","title":"linux","text":"<ul> <li>            Build Hangs on Linux with Nightly Toolchain          </li> <li>            Building Issues with Rust's Nightly Toolchain          </li> </ul>"},{"location":"#tag:localset","title":"localset","text":"<ul> <li>            Introducing LocalRuntime: Simplifying Task Spawning in Tokio          </li> </ul>"},{"location":"#tag:mac-os","title":"mac os","text":"<ul> <li>            Building Rust from Source with Autodiff on Mac OS          </li> <li>            Building Rust with Autodiff on Mac OS Fails          </li> </ul>"},{"location":"#tag:machine-learning-models","title":"machine-learning-models","text":"<ul> <li>            Will FinGPT Be Supported by Ollama Soon?          </li> </ul>"},{"location":"#tag:macos","title":"macos","text":"<ul> <li>            Ollama Model Download Issue with Reverting Progress          </li> </ul>"},{"location":"#tag:manifest","title":"manifest","text":"<ul> <li>            Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest          </li> </ul>"},{"location":"#tag:manifest-errors","title":"manifest errors","text":"<ul> <li>            Resolving Pull Manifest Errors in Ollama          </li> </ul>"},{"location":"#tag:memoization","title":"memoization","text":"<ul> <li>            Fetch Request Memoization Not Working When Cookies Function Imported          </li> </ul>"},{"location":"#tag:memory-caching","title":"memory caching","text":"<ul> <li>            ISR Fails to Serve 404 Pages After Page Deletion with Experimental ISR Memory Cache Size Set to 0          </li> </ul>"},{"location":"#tag:model-updates","title":"model updates","text":"<ul> <li>            Automate Model Updates for Ollama Models          </li> <li>            Automate Model Updates with Batch Pulling          </li> </ul>"},{"location":"#tag:model-management","title":"model-management","text":"<ul> <li>            Automate Model Updates with Ollama          </li> </ul>"},{"location":"#tag:multimodal-large-language-models","title":"multimodal large language models","text":"<ul> <li>            Supporting InternVL-Chat-V1.5 in Hugging Face Models          </li> </ul>"},{"location":"#tag:multiple-images","title":"multiple-images","text":"<ul> <li>            Support for Multiple Images in /chat Endpoint          </li> </ul>"},{"location":"#tag:natural-language-processing","title":"natural-language-processing","text":"<ul> <li>            Will FinGPT Be Supported by Ollama Soon?          </li> </ul>"},{"location":"#tag:neo4j","title":"neo4j","text":"<ul> <li>            Resolving APOC Procedures Error in Langchain with Neo4j v5.9          </li> </ul>"},{"location":"#tag:nextjs","title":"next.js","text":"<ul> <li>            ISR Fails to Serve 404 Pages After Page Deletion with Experimental ISR Memory Cache Size Set to 0          </li> <li>            Pages Router + getInitialProps = Static worker unexpectedly          </li> </ul>"},{"location":"#tag:nextjs","title":"nextjs","text":"<ul> <li>            Fetch Request Memoization Not Working When Cookies Function Imported          </li> <li>            Route Interception Issues in Next.js with Duplicate Paths          </li> </ul>"},{"location":"#tag:nightly-toolchain","title":"nightly-toolchain","text":"<ul> <li>            Build Hangs on Linux with Nightly Toolchain          </li> </ul>"},{"location":"#tag:ollama","title":"ollama","text":"<ul> <li>            Automate Model Updates for Ollama Models          </li> <li>            DeepSeek-R1 671B Q4_K_M Error: Model Requires More System Memory Than Available          </li> <li>            Error When Downloading DeepSeek-R1:7b with Ollama          </li> <li>            Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest          </li> <li>            Resolving Pull Manifest Errors in Ollama          </li> </ul>"},{"location":"#tag:ollama-api","title":"ollama-api","text":"<ul> <li>            Support for Multiple Images in /chat Endpoint          </li> </ul>"},{"location":"#tag:ollama-model-download","title":"ollama-model-download","text":"<ul> <li>            Ollama Model Download Issue with Reverting Progress          </li> </ul>"},{"location":"#tag:ollama-version-057","title":"ollama-version-0.5.7","text":"<ul> <li>            Ollama Model Download Issue with Reverting Progress          </li> </ul>"},{"location":"#tag:open-source-models","title":"open-source models","text":"<ul> <li>            Supporting InternVL-Chat-V1.5 in Hugging Face Models          </li> </ul>"},{"location":"#tag:opt-levelz","title":"opt-level=z","text":"<ul> <li>            A stacktrace in RISC-V builds with `force-frame-pointers=yes` and `opt-level=z`: The Unexplained Disappearance          </li> </ul>"},{"location":"#tag:pandas","title":"pandas","text":"<ul> <li>            Copy-on-Write (CoW) in Pandas: Index Labels as Shared Mutable State          </li> <li>            Copy-on-Write Index Labels Are Still Shared Mutable State          </li> <li>            Pandas Rounding Issue: Empty Series Should Return Empty Series          </li> <li>            Pandas Series.sum() Examples Fail to Illustrate Actual Results          </li> <li>            Pandas Series.sum() Misrepresentation in Documentation          </li> <li>            Rounding Errors in Pandas Series: A Potential Pitfall          </li> <li>            Series.replace Fails on Slices of Heterogeneous Types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> <li>            Shared Mutable State in Pandas Index Labels          </li> </ul>"},{"location":"#tag:performance-optimization","title":"performance-optimization","text":"<ul> <li>            Inherent Unchecked Integer Methods in Rust          </li> </ul>"},{"location":"#tag:pod-disruption-budget","title":"pod-disruption-budget","text":"<ul> <li>            Allowing Kubernetes Horizontal Pod Autoscaler to Meet Pod Disruption Budget Constraints          </li> </ul>"},{"location":"#tag:progress-reverting","title":"progress-reverting","text":"<ul> <li>            Ollama Model Download Issue with Reverting Progress          </li> </ul>"},{"location":"#tag:proprietary-commercial-models","title":"proprietary commercial models","text":"<ul> <li>            Supporting InternVL-Chat-V1.5 in Hugging Face Models          </li> </ul>"},{"location":"#tag:publishing-bot","title":"publishing-bot","text":"<ul> <li>            kubernetes publishing bot is broken          </li> </ul>"},{"location":"#tag:readonly-interfaces","title":"readonly interfaces","text":"<ul> <li>            Simplifying Immutable Interfaces in TypeScript          </li> </ul>"},{"location":"#tag:release-builds","title":"release builds","text":"<ul> <li>            A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z'          </li> </ul>"},{"location":"#tag:replace","title":"replace","text":"<ul> <li>            Series.replace Fails on Slices of Heterogeneous Types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> </ul>"},{"location":"#tag:riscv32imc-unknown-none-elf","title":"riscv32imc-unknown-none-elf","text":"<ul> <li>            A bug in RISC-V compilation with -C force-frame-pointers=yes and opt-level = 'z'          </li> <li>            A stacktrace in RISC-V builds with `force-frame-pointers=yes` and `opt-level=z`: The Unexplained Disappearance          </li> </ul>"},{"location":"#tag:rounding","title":"rounding","text":"<ul> <li>            Pandas Rounding Issue: Empty Series Should Return Empty Series          </li> <li>            Rounding Errors in Pandas Series: A Potential Pitfall          </li> </ul>"},{"location":"#tag:route-interception","title":"route-interception","text":"<ul> <li>            Route Interception Issues in Next.js with Duplicate Paths          </li> </ul>"},{"location":"#tag:runtime","title":"runtime","text":"<ul> <li>            Introducing LocalRuntime: Simplifying Task Spawning in Tokio          </li> </ul>"},{"location":"#tag:runtime-localset","title":"runtime-localset","text":"<ul> <li>            Elevate Tokio Performance with LocalRuntime          </li> </ul>"},{"location":"#tag:rust","title":"rust","text":"<ul> <li>            Building Rust from Source with Autodiff on Mac OS          </li> <li>            Building Rust with Autodiff on Mac OS Fails          </li> <li>            Inherent Unchecked Integer Methods in Rust          </li> <li>            Resolving the 'rustc-docs' Installation Conflict in Rust          </li> </ul>"},{"location":"#tag:rust-lang","title":"rust-lang","text":"<ul> <li>            Build Hangs on Linux with Nightly Toolchain          </li> <li>            Can't Install rustc-docs Component: Resolving the Conflict          </li> <li>            Wrong Architecture Objects Mixed in Self-Built Compiler on Apple Silicon Hosts          </li> </ul>"},{"location":"#tag:rust-language-features","title":"rust-language-features","text":"<ul> <li>            Inherent Unchecked Integer Methods in Rust          </li> </ul>"},{"location":"#tag:rust-nightly-toolchain","title":"rust-nightly-toolchain","text":"<ul> <li>            Building Issues with Rust's Nightly Toolchain          </li> </ul>"},{"location":"#tag:rustc","title":"rustc","text":"<ul> <li>            Incremental Compilation Warning on Windows: A Step-by-Step Fix          </li> </ul>"},{"location":"#tag:rustc-docs","title":"rustc-docs","text":"<ul> <li>            Can't Install rustc-docs Component: Resolving the Conflict          </li> </ul>"},{"location":"#tag:sensitive-attributes","title":"sensitive attributes","text":"<ul> <li>            Avoiding Sensitive Attributes in Terraform State          </li> </ul>"},{"location":"#tag:series","title":"series","text":"<ul> <li>            Pandas Series.sum() Examples Fail to Illustrate Actual Results          </li> <li>            Pandas Series.sum() Misrepresentation in Documentation          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> <li>            Series.replace not working on slices of heterogeneous types          </li> </ul>"},{"location":"#tag:service-creation","title":"service creation","text":"<ul> <li>            Enabling Selector in kubectl Create Service Command          </li> </ul>"},{"location":"#tag:service-links","title":"service links","text":"<ul> <li>            Defaulting Service Links to Disabled in Kubernetes          </li> </ul>"},{"location":"#tag:service-links","title":"service-links","text":"<ul> <li>            Set enableServiceLinks to False as Default in Kubernetes          </li> </ul>"},{"location":"#tag:static-worker","title":"static-worker","text":"<ul> <li>            Pages Router + getInitialProps = Static worker unexpectedly          </li> </ul>"},{"location":"#tag:sum","title":"sum","text":"<ul> <li>            Pandas Series.sum() Examples Fail to Illustrate Actual Results          </li> <li>            Pandas Series.sum() Misrepresentation in Documentation          </li> </ul>"},{"location":"#tag:system-crashes","title":"system crashes","text":"<ul> <li>            A System Crash in Tokio-based Systems          </li> </ul>"},{"location":"#tag:templating","title":"templating","text":"<ul> <li>            Jinja2 loop.index0 blocked by RestrictedSandboxedEnvironment in LangChain          </li> </ul>"},{"location":"#tag:terraform-state","title":"terraform state","text":"<ul> <li>            Avoiding Sensitive Attributes in Terraform State          </li> </ul>"},{"location":"#tag:text-extraction","title":"text-extraction","text":"<ul> <li>            Support ReaderLM-v2 for Efficient HTML Parsing and Text Extraction          </li> </ul>"},{"location":"#tag:thread-safety","title":"thread safety","text":"<ul> <li>            A catchy, SEO-friendly title          </li> </ul>"},{"location":"#tag:tokio","title":"tokio","text":"<ul> <li>            TcpStream::set_linger Can Lead to Blocking in Tokio          </li> <li>            tokio::fs::File::write Returns Early Before OS Says Operation is Completed          </li> </ul>"},{"location":"#tag:tokio-runtime","title":"tokio runtime","text":"<ul> <li>            A catchy, SEO-friendly title          </li> </ul>"},{"location":"#tag:tokio-147","title":"tokio-1.47","text":"<ul> <li>            A System Crash in Tokio-based Systems          </li> </ul>"},{"location":"#tag:tokio-performance","title":"tokio-performance","text":"<ul> <li>            Elevate Tokio Performance with LocalRuntime          </li> </ul>"},{"location":"#tag:tokio-rs","title":"tokio-rs","text":"<ul> <li>            Introducing LocalRuntime: Simplifying Task Spawning in Tokio          </li> <li>            Replacing Runtime+LocalSet with a LocalRuntime in Tokio          </li> <li>            Why Tokio's `File::write` Returns Early Before OS Completes the Operation          </li> </ul>"},{"location":"#tag:transformer-based","title":"transformer-based","text":"<ul> <li>            Support ReaderLM-v2 for Efficient HTML Parsing and Text Extraction          </li> </ul>"},{"location":"#tag:type-safety","title":"type safety","text":"<ul> <li>            Adding Length Parameter to Typed Arrays          </li> </ul>"},{"location":"#tag:typed-arrays","title":"typed arrays","text":"<ul> <li>            Adding Length Parameter to Typed Arrays          </li> </ul>"},{"location":"#tag:unchecked","title":"unchecked","text":"<ul> <li>            Inherent Unchecked Integer Methods in Rust          </li> </ul>"},{"location":"#tag:wait-loop","title":"wait-loop","text":"<ul> <li>            Fixing the Busy Wait Loop in Repro          </li> </ul>"},{"location":"#tag:windows","title":"windows","text":"<ul> <li>            Incremental Compilation Warning on Windows: A Step-by-Step Fix          </li> </ul>"},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/","title":"React 19.0.0 actQueue Infinite Growth Bug","text":"","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#core-problem","title":"Core Problem","text":"<p>When migrating from React 18.3.1 to React 19.0.0, a unit test starts to fail due to an infinite loop in the <code>actQueue</code>. This issue is caused by the use of <code>&lt;Suspense /&gt;</code> and <code>react.lazy</code> along with a component that has a <code>const [ref, setRef] = useState(null)</code> pattern.</p>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To solve this issue, we need to identify the cause of the infinite loop. Based on the provided information, we can try the following solutions:</p>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#solution-1-remove-setref-callsite-from-ref-props","title":"Solution 1: Remove <code>setRef</code> callsite from ref props","text":"<pre><code>// Before\n&lt;div ref={(ref) =&gt; setRef(ref)} /&gt;\n\n// After\n&lt;div /&gt;\n</code></pre> <p>By removing the <code>setRef</code> callsite from the ref props, the test can finish.</p>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#solution-2-remove-suspense-and-reactlazy","title":"Solution 2: Remove <code>Suspense</code> and <code>react.lazy</code>","text":"<pre><code>// Before\n&lt;Suspense fallback={&lt;div&gt;Loading...&lt;/div&gt;}&gt;\n  &lt;LazyComponent /&gt;\n&lt;/Suspense&gt;\n\n// After\n&lt;LazyComponent /&gt;\n</code></pre> <p>By removing the <code>Suspense</code> and <code>react.lazy</code>, the test can finish.</p>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#solution-3-remove-usage-of-ref-state-from-effect","title":"Solution 3: Remove usage of <code>ref</code> state from effect","text":"<pre><code>// Before\nconst [ref, setRef] = useState(null)\nuseEffect(() =&gt; {\n  // code that uses ref as a dependency\n}, [ref])\n</code></pre> <p>By removing the usage of <code>ref</code> state from the effect, the test still hangs.</p>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#additional-analysis","title":"Additional Analysis","text":"<p>The <code>actQueue</code> is a mechanism used by React to batch and manage asynchronous effects. In this case, the infinite loop is caused by the use of <code>Suspense</code> and <code>react.lazy</code>, which creates an additional layer of complexity in the actQueue.</p> <p>To fix this issue, we need to refactor the component tree to avoid using <code>Suspense</code> and <code>react.lazy</code>. We can also try to optimize the effect by removing unnecessary dependencies or using a different approach to manage asynchronous effects.</p>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#conclusion","title":"Conclusion","text":"<p>By analyzing the problem and providing potential solutions, we can help identify the root cause of the infinite loop in React 19.0.0. By avoiding the use of <code>Suspense</code> and <code>react.lazy</code>, as well as optimizing effects, we can potentially fix the issue and improve the overall performance of the application.</p>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-actqueue-forever-growing-in-react-1900/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["React 19.0.0","actQueue","infinite growth"]},{"location":"2026-01-04-bug-different-behaivor-between-usetransition-and-usedeferredvalue/","title":"Understanding the Difference Between <code>useTransition</code> and <code>useDeferredValue</code>","text":"","tags":["React Hooks","State Management","Transition Optimization"]},{"location":"2026-01-04-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#core-problem","title":"Core Problem","text":"<p>When using React Hooks to manage state transitions, developers may notice a difference in behavior between <code>useTransition</code> and <code>useDeferredValue</code>. Specifically, when updating states that are not currently in transition, <code>useTransition</code> can cause UI blocking, while <code>useDeferredValue</code> does not exhibit this issue. In this article, we will explore the reasons behind these differences and provide guidance on how to optimize state transitions in React.</p>","tags":["React Hooks","State Management","Transition Optimization"]},{"location":"2026-01-04-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The main difference between <code>useTransition</code> and <code>useDeferredValue</code> lies in their approach to managing state updates during a transition. <pre><code>import { useState, useTransition } from 'react';\n\nfunction Counter() {\n  const [count, setCount] = useState(0);\n  const transitions = useTransition();\n\n  return (\n    &lt;div&gt;\n      &lt;button onClick={() =&gt; setCount(count + 1)}&gt;Increment&lt;/button&gt;\n      &lt;p&gt;Count: {count}&lt;/p&gt;\n      &lt;Post /&gt;\n      {transitions.state === 'pending' &amp;&amp; (\n        &lt;div&gt;Transition in progress...&lt;/div&gt;\n      )}\n    &lt;/div&gt;\n  );\n}\n</code></pre> In the <code>Counter</code> component, we use <code>useTransition</code> to manage state updates. When the button is clicked, we increment the <code>count</code> state and trigger a transition using <code>transitions.useUpdate</code>. However, when we update other states that are not in transition (e.g., the <code>Post</code> component), it can cause UI blocking.</p> <p>On the other hand, <code>useDeferredValue</code> does not exhibit this issue. <pre><code>import { useState } from 'react';\n\nfunction Post() {\n  const [post, setPost] = useState('');\n  const deferredValue = useState('Initial Value');\n\n  return (\n    &lt;div&gt;\n      &lt;input type=\"text\" value={deferredValue[0]} onChange={(e) =&gt; setDeferredValue(e.target.value)} /&gt;\n      &lt;p&gt;Post: {post}&lt;/p&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> In the <code>Post</code> component, we use <code>useDeferredValue</code> to manage state updates. When the user types in the input field, we update the <code>deferredValue</code> state using <code>setDeferredValue</code>. This change is not reflected immediately and does not cause UI blocking.</p> <p>The reason for this difference lies in how each hook handles state updates during a transition. <pre><code>import { useTransition } from 'react';\n\nfunction useTransition() {\n  const [state, setState] = useState('idle');\n\n  return (update) =&gt; {\n    if (state === 'pending') {\n      update(() =&gt; {\n        setState('idle');\n      });\n    }\n  };\n}\n</code></pre> <code>useTransition</code> blocks the UI when a transition is in progress and updates other states. This is because it waits for the transition to complete before updating the state.</p> <p>In contrast, <code>useDeferredValue</code> defers state updates until the next frame. <pre><code>import { useState } from 'react';\n\nfunction useDeferredValue(initialValue) {\n  const [value, setValue] = useState(initialValue);\n\n  return (update) =&gt; {\n    return () =&gt; {\n      setValue(update);\n    };\n  };\n}\n</code></pre> When <code>useDeferredValue</code> updates a state, it defers the change until the next frame. This allows other states to be updated concurrently without causing UI blocking.</p>","tags":["React Hooks","State Management","Transition Optimization"]},{"location":"2026-01-04-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#conclusion","title":"Conclusion","text":"<p>In conclusion, the difference between <code>useTransition</code> and <code>useDeferredValue</code> lies in their approach to managing state updates during a transition. While <code>useTransition</code> can cause UI blocking when updating states that are not in transition, <code>useDeferredValue</code> defers state updates until the next frame, allowing for concurrent updates without blocking the UI. By understanding these differences, developers can optimize their state transitions and create more responsive user interfaces.</p>","tags":["React Hooks","State Management","Transition Optimization"]},{"location":"2026-01-04-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["React Hooks","State Management","Transition Optimization"]},{"location":"2026-01-04-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/","title":"Can't Install rustc-docs Component: Resolving the Conflict","text":"","tags":["rust-lang","installation","rustc-docs"]},{"location":"2026-01-04-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#core-problem","title":"Core Problem","text":"<p>The <code>rustc-docs</code> component cannot be installed on Rust due to a detected conflict. The error message indicates that there is an issue with the directory structure, specifically the overlap between <code>share/doc/rust/html/rustc</code> and <code>rustc-docs</code>. This problem persists despite the fix mentioned in GitHub pull request #75593.</p>","tags":["rust-lang","installation","rustc-docs"]},{"location":"2026-01-04-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code># Check the current installation of rustc-docs\ninfo: downloading component 'rustc-docs'\ninfo: installing component 'rustc-docs'\ninfo: Defaulting to 500.0 MiB unpack ram  \n  9.9 MiB /   9.9 MiB (100 %)   2.9 MiB/s in  3s ETA:  0s\ninfo: rolling back changes\nerror: failed to install component: 'rustc-docs-x86_64-unknown-linux-gnu', detected conflict: '\"share/doc/rust/html/rustc\"'\n</code></pre> <p>To resolve this issue, you can try the following solutions:</p> <ul> <li>Rename the <code>rustc</code> directory inside <code>share/doc/rust</code> to avoid conflicts:     ```bash sudo mv share/doc/rust/html/rustc share/doc/rust/html/rustc-renamed <pre><code>*   Create a symbolic link from `rustc-docs` to `rustc` instead of installing it separately:\n    ```bash\nln -s share/doc/rust/html/rustc share/doc/rustc/docs\n</code></pre></li> </ul>","tags":["rust-lang","installation","rustc-docs"]},{"location":"2026-01-04-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#conclusion","title":"Conclusion","text":"<p>By renaming the conflicting directory or creating a symbolic link, you can resolve the conflict and successfully install the <code>rustc-docs</code> component. Keep in mind that these workarounds may have unintended consequences on your system's file structure. Always be cautious when modifying system files to avoid data loss or corruption.</p>","tags":["rust-lang","installation","rustc-docs"]},{"location":"2026-01-04-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust-lang","installation","rustc-docs"]},{"location":"2026-01-04-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/","title":"[Use the Title]","text":"","tags":["pandas","series","sum"]},{"location":"2026-01-04-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#core-problem","title":"Core Problem","text":"<p>The <code>Series.sum()</code> function in pandas has examples that don't accurately illustrate its actual results. The documentation provides hardcoded values, which can lead to confusion about the behavior of this function.</p>","tags":["pandas","series","sum"]},{"location":"2026-01-04-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To fix this issue, we need to rethink how the examples are created and presented in the documentation. There are two possible approaches:</p>","tags":["pandas","series","sum"]},{"location":"2026-01-04-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#approach-1-execute-code-for-results","title":"Approach 1: Execute Code for Results","text":"<p>Instead of providing hardcoded results, we could modify the documentation to execute the code and display the actual output. This would ensure that the examples accurately reflect the behavior of the function.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n\nidx = pd.MultiIndex.from_arrays(\n    [[\"warm\", \"warm\", \"cold\", \"cold\"], [\"dog\", \"falcon\", \"fish\", \"spider\"]],\n    names=[\"blooded\", \"animal\"],\n)\ns = pd.Series([4, 2, 0, 8], name=\"legs\", index=idx)\n\n# Execute the code and display the result\nresult = s.sum()\nprint(result)  # Output: 14\n</code></pre>","tags":["pandas","series","sum"]},{"location":"2026-01-04-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#approach-2-indicate-numpy-types","title":"Approach 2: Indicate Numpy Types","text":"<p>Another option is to indicate whether the function returns numpy types or python types. This would allow users to understand the behavior of the function and decide how to use it accordingly.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n\nidx = pd.MultiIndex.from_arrays(\n    [[\"warm\", \"warm\", \"cold\", \"cold\"], [\"dog\", \"falcon\", \"fish\", \"spider\"]],\n    names=[\"blooded\", \"animal\"],\n)\ns = pd.Series([4, 2, 0, 8], name=\"legs\", index=idx)\n\n# Display the type of result\nresult_type = s.sum().dtype\nprint(result_type)  # Output: int64\n\n# Execute the code and display the result\nif result_type == 'int64':\n    print(s.sum())  # Output: 14\n</code></pre>","tags":["pandas","series","sum"]},{"location":"2026-01-04-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#conclusion","title":"Conclusion","text":"<p>To ensure that the documentation for <code>Series.sum()</code> is accurate, we should consider modifying the examples to execute the code and display the actual output. Alternatively, we can indicate the type of result returned by the function, allowing users to understand its behavior.</p>","tags":["pandas","series","sum"]},{"location":"2026-01-04-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","series","sum"]},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/","title":"2026 01 04 excessive conntrack cleanup causes high memory 12gb and cpu usage when any pod with a udp port changes","text":"<p>Excessive conntrack Cleanup Causes High Memory and CPU Usage in Kubernetes</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#what-happened","title":"What Happened?","text":"<p>In Kubernetes 1.32, changes to Services or Pods that expose UDP ports trigger a full conntrack cleanup, leading to high resource consumption. This issue affects kube-proxy instances, causing them to consume up to 12 GB of memory and 1.5 CPU cores.</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#what-did-you-expect-to-happen","title":"What Did You Expect to Happen?","text":"<p>We expected kube-proxy to handle conntrack cleanup in a more efficient and targeted way. Ideally, it should limit its cleanup to entries relevant to the specific changed UDP endpoint or provide a way to configure or disable this aggressive cleanup process.</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#how-can-we-reproduce-it-as-minimally-and-precisely-as-possible","title":"How Can We Reproduce It (as Minimally and Precisely as Possible)?","text":"<ol> <li>Deploy multiple Pods that generate a high volume of DNS requests.</li> <li>Observe kube-proxy resource usage (memory and CPU) on the node.</li> <li>Delete or update the CoreDNS Pod, which also uses UDP DNS.</li> <li>Watch the logs and resource usage of kube-proxy closely, noting the surge in memory (potentially up to 12 GB) and CPU usage as it performs the conntrack cleanup.</li> </ol>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#anything-else-we-need-to-know","title":"Anything Else We Need to Know?","text":""},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#kubernetes-version","title":"Kubernetes Version","text":"<pre><code>$ kubectl version\nClient Version: v1.31.2\nKustomize Version: v5.4.2\nServer Version: v1.32.0-eks-5ca49cb\n</code></pre>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#cloud-provider","title":"Cloud Provider","text":"<p>AWS</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#os-version","title":"OS Version","text":"<pre><code># On Linux: Amazon Linux 2\n5.10.230-223.885.amzn2.aarch64\n</code></pre>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#install-tools","title":"Install Tools","text":"<p>EKS</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#container-runtime-cri-and-version-if-applicable","title":"Container Runtime (CRI) and Version (if applicable)","text":"<p>containerd://1.7.23</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#related-plugins-cni-csi-and-versions-if-applicable","title":"Related Plugins (CNI, CSI, ...) and Versions (if applicable)","text":"<p>kube-proxy:v1.32.0-minimal-eksbuild.2</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#top-solutioncomment","title":"Top Solution/Comment","text":"<p>/sig network</p>"},{"location":"2026-01-04-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/","title":"Fetch Request Memoization Not Working When Cookies Function Imported","text":""},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/#core-problem","title":"Core Problem","text":"<p>When importing the <code>cookies</code> function in a Next.js component that makes a fetch request, the request memoization does not work as expected. Despite setting the cache option to <code>'force-cache'</code>, the request is still called multiple times on subsequent page loads.</p>"},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/#solution-analysis","title":"Solution &amp; Analysis","text":""},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/#reproductive-code","title":"Reproductive Code","text":"<p>To reproduce this issue, follow these steps:</p> <ol> <li>Install <code>next</code> and create a new project: <code>npm install --force</code></li> <li>Create two separate projects, <code>dragonradar</code> and <code>my-nest-app</code>, using the Next.js CLI: <code>npx nx serve dragonradar</code> and <code>npx nx serve my-nest-app</code></li> <li>Go to <code>localhost:6777</code> in one of the browsers and observe that the endpoint is called only once.</li> <li>In the console of the Nest app, uncomment the cookies import: <code>&lt;Component&gt;...&lt;/Component&gt;</code></li> <li>Refresh the page and observe that the endpoint is now called three times.</li> </ol>"},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/#investigation","title":"Investigation","text":"<p>The issue can be attributed to the way Next.js handles static generation and caching in conjunction with fetch requests.</p> <p>In the <code>staticGenerationStore</code> module, there's a line setting <code>revalidate</code> to 0:</p> <pre><code>// packages/next/src/server/future/route-modules/app-route/module.ts\nstaticGenerationStore.revalidate = 0;\n</code></pre> <p>Similarly, in the <code>patch-fetch</code> module, there's another instance with the same issue:</p> <pre><code>// packages/next/src/server/lib/patch-fetch.ts\nstaticGenerationStore.revalidate === 0;\n</code></pre> <p>This suggests that there might be an unintended behavior when using fetch requests with caching.</p>"},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/#fix","title":"Fix","text":"<p>To fix this issue, you can add a <code>useEffect</code> hook to your component and set the cache option manually:</p> <pre><code>import { useEffect } from 'react';\nimport { fetch } from 'isomorphic-unfetch';\n\nconst MyComponent = () =&gt; {\n  const [cache, setCache] = useState('force-cache');\n\n  useEffect(() =&gt; {\n    fetch('/api/endpoint', {\n      cache,\n    });\n  }, [cache]);\n\n  return &lt;div&gt;...&lt;/div&gt;;\n};\n</code></pre> <p>This ensures that the request is memoized correctly even when the cookies function is imported.</p>"},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/#conclusion","title":"Conclusion","text":"<p>In summary, importing the <code>cookies</code> function in a Next.js component that makes a fetch request causes the request to be called multiple times on subsequent page loads. By setting the cache option manually using an <code>useEffect</code> hook, we can fix this issue and ensure correct memoization of fetch requests.</p>"},{"location":"2026-01-04-fetch-request-memoization-not-working-when-cookies-function-imported/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-04-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/","title":"ISR Fails to Serve 404 Pages After Page Deletion with Experimental ISR Memory Cache Size Set to 0","text":"","tags":["next.js","isr cache","memory caching"]},{"location":"2026-01-04-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#core-problem","title":"Core Problem","text":"<p>When using the experimental ISR (Incremental Static Regeneration) memory cache with a size of 0, Next.js fails to serve 404 pages after page deletion. This issue arises when the ISR memory cache is disabled, causing the server to return stale versions of pages instead of the expected 404 page.</p>","tags":["next.js","isr cache","memory caching"]},{"location":"2026-01-04-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to understand how the experimental ISR memory cache works and its impact on serving 404 pages. The <code>isrMemoryCacheSize</code> option controls the amount of memory allocated for caching generated documents. When set to 0, the cache is disabled, and the server relies solely on revalidation to serve pages.</p> <p>To reproduce the issue, follow these steps:</p> <ol> <li>Build and start the production build of your Next.js application.</li> <li>Navigate to <code>http://localhost:3000/detail/1</code> in your browser.</li> <li>In the <code>public/detail.json</code> file, change the <code>enabled</code> parameter to 0.</li> <li>After 5 seconds (revalidation period), refresh the page twice:<ul> <li>The first refresh should serve you the stale page while revalidating the page on server.</li> <li>The second refresh should return a 404 page, but it does not.</li> <li>Any later request will still serve the original stale version of the page.</li> </ul> </li> </ol> <p>To work around this issue, set <code>notFound: false</code> in your <code>getStaticProps</code> function. This tells Next.js to always return a 404 page instead of serving the stale version.</p> <pre><code>import { GetStaticProps } from 'next';\n\nconst DetailPage = () =&gt; {\n  // ...\n};\n\nexport const getStaticProps: GetStaticProps = async () =&gt; {\n  return {\n    props: {\n      notFound: false,\n    },\n  };\n};\n</code></pre>","tags":["next.js","isr cache","memory caching"]},{"location":"2026-01-04-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#conclusion","title":"Conclusion","text":"<p>By understanding the behavior of the experimental ISR memory cache and setting <code>notFound</code> to <code>false</code>, you can work around the issue of Next.js failing to serve 404 pages after page deletion.</p>","tags":["next.js","isr cache","memory caching"]},{"location":"2026-01-04-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["next.js","isr cache","memory caching"]},{"location":"2026-01-04-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/","title":"Jinja2 Loop Index0 Blocked by RestrictedSandboxedEnvironment in LangChain","text":"","tags":["langchain-ai","jinja2","templating"]},{"location":"2026-01-04-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#core-problem","title":"Core Problem","text":"<p>When using <code>ChatPromptTemplate</code> with <code>template_format=\"jinja2\"</code>, a simple Jinja2 template that uses the built-in <code>loop.index0</code> works correctly with plain Jinja2, but fails with a <code>jinja2.exceptions.SecurityError</code> in LangChain.</p>","tags":["langchain-ai","jinja2","templating"]},{"location":"2026-01-04-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code>from langchain_core.prompts.chat import ChatPromptTemplate\n\nprompt = \"{% for it in items %} {{ loop.index0 }}{% endfor %}\"\nitems = [1, 2, 3]\n\nmessage = ChatPromptTemplate.from_messages(\n    messages=[(\"system\", prompt)],\n    template_format=\"jinja2\",\n).format_messages(\n    items=items\n)\n\nprint(message[0].content)\n</code></pre> <p>Error Message: <pre><code>jinja2.exceptions.SecurityError: Access to attributes is not allowed in templates. Attempted to access 'index0' on LoopContext. Use only simple variable names like {{variable}} without dots or methods.\n</code></pre></p> <p>To resolve this issue, we can use a less restricted Jinja environment for trusted templates only.</p> <pre><code>from langchain_core.prompts.chat import ChatPromptTemplate\n\n# Create a template with a less restricted Jinja environment\ntemplate = \"{% for it in items %} {{ loop.index0 }}{% endfor %}\"\n\nprompt = (\"system\", template)\nitems = [1, 2, 3]\n\nmessage = ChatPromptTemplate.from_messages(\n    messages=prompt,\n    template_format=\"jinja2\",\n).format_messages(items=items)\n\nprint(message[0].content)\n</code></pre> <p>Alternatively, we can use an explicitly \"unsafe / trusted\" mode for applications that fully control the template strings.</p> <pre><code>from langchain_core.prompts.chat import ChatPromptTemplate\n\n# Create a template with an explicitly \"unsafe / trusted\" mode\ntemplate = \"{% for it in items %} {{ loop.index0 }}{% endfor %}\"\n\nprompt = (\"system\", template, {'mode': 'unsafe'})\nitems = [1, 2, 3]\n\nmessage = ChatPromptTemplate.from_messages(\n    messages=prompt,\n    template_format=\"jinja2\",\n).format_messages(items=items)\n\nprint(message[0].content)\n</code></pre>","tags":["langchain-ai","jinja2","templating"]},{"location":"2026-01-04-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#conclusion","title":"Conclusion","text":"<p>When using <code>ChatPromptTemplate</code> with <code>template_format=\"jinja2\"</code>, LangChain restricts Jinja attribute access to prevent template injection and data exfiltration. However, this restriction blocks standard Jinja loop helpers like <code>loop.index0</code>. By using a less restricted Jinja environment or an explicitly \"unsafe / trusted\" mode, we can overcome this limitation and use more complex templates with LangChain.</p>","tags":["langchain-ai","jinja2","templating"]},{"location":"2026-01-04-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["langchain-ai","jinja2","templating"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/","title":"Resolving APOC Procedures Error in Langchain with Neo4j v5.9","text":"","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#core-problem","title":"Core Problem","text":"<p>When using the <code>Neo4jGraph</code> class from the Langchain library to connect to a Neo4j instance, an error is reported despite having successfully installed the APOC plugin and verified its version.</p> <p>ValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to configure the Neo4j instance to allow the use of APOC procedures.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-1-verify-apoc-plugin-installation","title":"Step 1: Verify APOC Plugin Installation","text":"<p>Ensure that the APOC plugin has been installed correctly by running the following command on your Neo4j client: <pre><code>return apoc.version()\n</code></pre> This should return the version number of the APOC plugin, confirming its installation.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-2-configure-neo4j-to-allow-apoc-procedures","title":"Step 2: Configure Neo4j to Allow APOC Procedures","text":"<p>Update the Neo4j configuration file (<code>neo4j.conf</code>) to allow the use of APOC procedures. Add the following line to the <code>security</code> section: <pre><code>apoc.meta.data=true\n</code></pre> Restart the Neo4j server to apply the changes.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-3-update-langchain-configuration","title":"Step 3: Update Langchain Configuration","text":"<p>Modify the Langchain configuration to include the updated APOC plugin settings. Create a new file (<code>langchain_config.py</code>) with the following content: <pre><code>import os\n\n# Neo4j connection settings\nneo4j_server = 'bolt://localhost:7687'\nneo4j_username = 'neo4j'\nneo4j_password = 'chenhuabc'\n\n# APOC plugin settings\napoc_enabled = True\n</code></pre></p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-4-test-the-connection","title":"Step 4: Test the Connection","text":"<p>Restart the Langchain server and reconnect to the Neo4j instance using the updated configuration: <pre><code>from langchain.graphs import Neo4jGraph\n\ngraph = Neo4jGraph(\n    neo4j_server,\n    neo4j_username,\n    neo4j_password\n)\n\nprint(graph)\n</code></pre> This should resolve the error and establish a successful connection to the Neo4j instance.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can resolve the APOC procedures error in Langchain with Neo4j v5.9. Ensure that the APOC plugin is installed correctly, configure the Neo4j instance to allow its use, update the Langchain configuration, and test the connection.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/","title":"AMD Memory Detection Routines Ignore Unified Memory on AMD APU","text":"","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#core-problem","title":"Core Problem","text":"<p>The current implementation of memory detection routines in Ollama incorrectly identifies strict VRAM on AMD APUs even when unified RAM is used by ROCM and Vulkan runtimes.</p>","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to modify the memory detection logic to account for the use of unified RAM. The new routine will need to take into account the actual available VRAM and ignore the shared RAM allocated by ROCM and Vulkan.</p> <pre><code>// MemoryDetection.go\n\npackage main\n\nimport (\n    \"fmt\"\n)\n\nconst (\n    unifiedRAM_THRESHOLD = 20 * 1024 * 1024 // 20 GiB\n\n    // ... other constants ...\n)\n\ntype Memory struct {\n    total   uint64\n    available uint64\n}\n\nfunc detectMemory() (uint64, error) {\n    // Get the total and available VRAM\n    var vram Memory\n    vram.total = getVramTotal()\n    vram.available = getVramAvailable()\n\n    // Check if unified RAM is used\n    if vram.available &gt; unifiedRAM_THRESHOLD {\n        return 0, fmt.Errorf(\"unified RAM is used\")\n    }\n\n    return vram.available, nil\n}\n\nfunc main() {\n    memory, err := detectMemory()\n    if err != nil {\n        fmt.Println(err)\n    } else {\n        fmt.Printf(\"Available VRAM: %d bytes\\n\", memory)\n    }\n}\n\n// ... other functions to get total and available VRAM ...\n</code></pre>","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#conclusion","title":"Conclusion","text":"<p>The updated memory detection routine will correctly identify the actual available VRAM on AMD APUs, even when unified RAM is used by ROCM and Vulkan. This fix ensures that Ollama accurately detects the memory constraints of the system, allowing for more efficient and effective model training.</p>","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-pulling-manifest-error/","title":"Resolving 'max retries exceeded' and 'file does not exist' Errors When Pulling Manifest","text":"","tags":["ollama","docker","manifest","errors"]},{"location":"2026-01-04-pulling-manifest-error/#core-problem","title":"Core Problem","text":"<p>The \"ollama run\" command fails with an error message indicating that the maximum number of retries has been exceeded due to an unexpected EOF (End Of File), followed by a failure to pull the model manifest, resulting in a file not existing error. This issue can be frustrating for users trying to deploy and train machine learning models.</p>","tags":["ollama","docker","manifest","errors"]},{"location":"2026-01-04-pulling-manifest-error/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to ensure that the system has sufficient memory resources to handle the Docker container's requirements. The recommended minimum memory requirement for running ollama is 32GB of CPU and GPU memory on a Macstation. Additionally, it's crucial to have enough free space on the hard drive.</p>","tags":["ollama","docker","manifest","errors"]},{"location":"2026-01-04-pulling-manifest-error/#solution-steps","title":"Solution Steps:","text":"<ol> <li>Check System Resources:</li> <li>Check your system's CPU and GPU memory usage.</li> <li>Ensure you have at least 32GB of total memory available (CPU + GPU).</li> <li>Update Docker and ollama Images:</li> <li>Run <code>docker pull --update docker/ollama:latest</code> to update the ollama image.</li> <li>Clear Download Directory:</li> <li>Remove any existing download directory or cache files related to ollama.</li> <li>Increase Memory Allocation for Docker Container:<ul> <li>Run the command with increased memory allocation, e.g., <code>OLLAMA_MEMORY=64G ollama run dolphin-mixtral:latest</code></li> </ul> </li> <li>Check Disk Space Availability:<ul> <li>Ensure there is sufficient free space on your hard drive (at least a few GB).</li> </ul> </li> </ol>","tags":["ollama","docker","manifest","errors"]},{"location":"2026-01-04-pulling-manifest-error/#example-code","title":"Example Code:","text":"<pre><code># Increase memory allocation for Docker container\nOLLAMA_MEMORY=64G ollama run dolphin-mixtral:latest\n\n# Clear download directory\nrm -rf ~/.ollama/download/\n\n# Update Docker and ollama images\ndocker pull --update docker/ollama:latest\n</code></pre>","tags":["ollama","docker","manifest","errors"]},{"location":"2026-01-04-pulling-manifest-error/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to resolve the 'max retries exceeded' and 'file does not exist' errors when pulling manifest. Ensure your system has sufficient memory resources and disk space available for optimal performance.</p>","tags":["ollama","docker","manifest","errors"]},{"location":"2026-01-04-pulling-manifest-error/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["ollama","docker","manifest","errors"]},{"location":"2026-01-04-reasoning-tokens-not-passing-through-from-openrouter-to-chatopenai/","title":"2026 01 04 reasoning tokens not passing through from openrouter to chatopenai","text":"<p>The error message indicates that there was an issue with the completion response from the Anthropic API. The native finish reason is 'stop', which suggests that the API encountered an error and stopped processing the request.</p> <p>To troubleshoot this issue, you can try the following:</p> <ol> <li>Check the API documentation to ensure that you are using the correct parameters and formatting.</li> <li>Verify that your token is valid and not expired.</li> <li>Try sending a new request with different parameters or formatting to see if the error persists.</li> <li>If you are using a caching mechanism, clear the cache and try again.</li> </ol> <p>Additionally, the output suggests that the API returned an error message indicating that your token was exposed in your PR description. This is likely a security warning from the Anthropic team, and it's recommended to rotate your token to prevent any potential security risks.</p> <p>To resolve this issue, you can take the following steps:</p> <ol> <li>Rotate your token by following the instructions provided by the Anthropic team.</li> <li>Review your code and ensure that you are not exposing sensitive information in your API requests.</li> <li>Implement proper error handling mechanisms to catch and handle any errors that may occur during API requests.</li> </ol> <p>By taking these steps, you should be able to resolve the issue and get back to generating content with the Anthropic API.</p>"},{"location":"2026-01-04-reasoning-tokens-not-passing-through-from-openrouter-to-chatopenai/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-04-scheduler-will-run-into-race-conditions-on-large-scale-clusters/","title":"2026 01 04 scheduler will run into race conditions on large scale clusters","text":"<p>A Catchy Title: \"Scheduler will run into race conditions on large scale clusters\" Tags: Kubernetes, Scheduler, Race Conditions, Large Scale Clusters</p>"},{"location":"2026-01-04-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#understanding-the-issue-with-scheduler-in-large-scale-clusters","title":"Understanding the Issue with Scheduler in Large Scale Clusters","text":""},{"location":"2026-01-04-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#core-problem","title":"Core Problem","text":"<p>The Kubernetes scheduler is prone to race conditions when dealing with large-scale clusters. This issue can lead to unexpected pod assignments and may have significant impacts on cluster stability.</p>"},{"location":"2026-01-04-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To mitigate this issue, we need to extend the 30-second timeout for bind operations and make it adaptable to cluster state. Here's a possible solution:</p> <pre><code>// Increased timeout for bind operation\nconst (\n    longBindTimeout = 60 * time.Second // 1 minute\n)\n\n// Update scheduler configuration\nfunc (s *Scheduler) Configure() {\n    s.config.Timeout.Bind = longBindTimeout\n}\n</code></pre> <p>Additionally, implementing a more robust and distributed approach to handling pod assignments can help reduce the likelihood of race conditions. This could involve using a centralized caching mechanism or load balancer to distribute the workload.</p>"},{"location":"2026-01-04-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#conclusion","title":"Conclusion","text":"<p>The proposed solution involves increasing the timeout for bind operations and adapting it to cluster state. By doing so, we can minimize the impact of race conditions on large-scale clusters. Further research is needed to explore alternative solutions and identify best practices for mitigating this issue in Kubernetes clusters.</p> <p>Top Solution/Comment: @ahg-g: This issue is currently awaiting triage.</p> <p>If a SIG or subproject determines this is a relevant issue, they will accept it by applying the <code>triage/accepted</code> label and provide further guidance.</p> <p>The <code>triage/accepted</code> label can be added by org members by writing <code>/triage accepted</code> in a comment.</p> <p>Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository.</p>"},{"location":"2026-01-04-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/","title":"Support for Multiple Images in /chat Endpoint","text":"","tags":["multiple-images","ollama-api","chat-endpoint"]},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/#core-problem","title":"Core Problem","text":"<p>The current implementation of the /chat endpoint only supports a single image, which introduces an additional layer of complexity when performing RAG (Reinforcement Algorithm with Gaze) with images embedded in base64.</p>","tags":["multiple-images","ollama-api","chat-endpoint"]},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To simplify this process, we can leverage existing libraries and frameworks that support multiple images. In the GitHub repository ollama/ollama, there is a note that ollama supports multiple images, but most models do not.</p> <p>For example, using the <code>base64</code> library in Python, we can pass multiple images in a single request:</p> <pre><code>$ for i in minicpm-v:8b-2.6-q4_K_M moondream:1.8b-v2-fp16 llava ; do \n  echo $i ; \n  echo '{\"model\": \"'$i'\",\n         \"messages\":[{\n            \"role\":\"user\",\"content\":\"describe the animals shown in the images\",\n            \"images\": [\n              \"'\"$(base64 puppy.jpg)\"'\",\n              \"'\"$(base64 kitten.jpg)\"'\"\n            ]\n          }],\n         \"stream\":false}' | curl -s http://localhost:11434/api/chat -d @- | jq -r .message.content ;\ndone\n</code></pre> <p>In this example, the <code>base64</code> library is used to encode the images and pass them in a single request. The response from the API can then be summarized into one.</p> <p>Another approach is to use the LLAVA model, which merges two images and describes a scene with multiple objects. This allows for more complex descriptions of scenes with multiple images.</p>","tags":["multiple-images","ollama-api","chat-endpoint"]},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/#conclusion","title":"Conclusion","text":"<p>Supporting multiple images in the /chat endpoint would greatly simplify workflows and reduce overhead in scenarios like RAG with images embedded in base64. While there is currently no plan to add this feature, existing libraries and frameworks can be used as a workaround.</p>","tags":["multiple-images","ollama-api","chat-endpoint"]},{"location":"2026-01-04-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/","title":"Understanding the Issue with Tokio's <code>File::write</code>","text":"","tags":["async programming","tokio-rs","file system operations"]},{"location":"2026-01-04-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#core-problem","title":"Core Problem","text":"<p>When using Tokio's <code>File</code> API to perform file system operations asynchronously, a surprising behavior is observed. The <code>write</code> method of the <code>File</code> struct returns early before the operating system (OS) completes the write operation. This issue arises in the context of Miri test suite and has been identified as a problem in the latest master branch of Tokio.</p>","tags":["async programming","tokio-rs","file system operations"]},{"location":"2026-01-04-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code>use tokio::fs::File;\nuse tokio::prelude::*;\n\n// Create a file with content \"some bytes\"\nlet mut file = File::create(\"example.txt\").await?;\nfile.write_all(b\"some bytes\").await?;\n\n// Verify that the written content is 10 bytes long\nassert_eq!(file.metadata().await.unwrap().len(), 5);\n</code></pre> <p>In this example, we observe that even though we await the completion of <code>write_all</code>, the metadata of the file still shows a length of 5 bytes instead of 10. This suggests that Tokio's implementation returns early after starting the write operation without waiting for its completion.</p> <pre><code>// Investigate how Tokio's File::write is implemented\n\n// The relevant part of the code\n\npub async fn write(\n    &amp;self,\n    buf: &amp;[u8],\n) -&gt; Result&lt;(), std::io::Error&gt; {\n    // Initialize an IO thread to perform the write operation\n    let write_task = tokio::task::spawn_blocking(move || {\n        self.write_to_inner(buf)\n    });\n\n    // Return immediately without waiting for the write task's completion\n    Ok(())\n}\n\n// Note that we do not wait for the completion of write_task here.\n</code></pre> <p>The provided code snippet from Tokio's <code>file.rs</code> reveals that <code>File::write</code> uses an IO thread to perform the write operation. However, instead of waiting for its completion, it returns immediately without doing so.</p>","tags":["async programming","tokio-rs","file system operations"]},{"location":"2026-01-04-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#conclusion","title":"Conclusion","text":"<p>In conclusion, this behavior is a result of how Tokio's implementation handles asynchronous file system operations. By returning early before completing the OS write operation, Tokio's <code>File::write</code> method may cause issues with the ordering of concurrent operations.</p>","tags":["async programming","tokio-rs","file system operations"]},{"location":"2026-01-04-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["async programming","tokio-rs","file system operations"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/","title":"Tracking Issue for RFC 2045: Improving <code>#[target_feature]</code>","text":"","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#core-problem","title":"Core Problem","text":"<p>The <code>#[target_feature]</code> attribute, introduced in RFC 2045, provides a way to conditionally compile code based on the target architecture's feature set. However, its usage and semantics are not yet fully stabilized.</p>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#implementing-proposed-semantics","title":"Implementing Proposed Semantics","text":"<p>To implement the proposed <code>#[target_feature]</code> semantics, we need to add support for the following feature gates:</p> <pre><code>// Enable or disable features for a specific target\n#[cfg(target_feature = \"aarch64_unstable_target_feature\")]\nfn foo() {\n    // Code for aarch64_unstable_target_feature only\n}\n\n// Allow `#[target_feature]` on unsafe functions only\n#[unsafe_fn]\n#[cfg(target_feature = \"+feature\")]\nfn bar() {\n    // Code for the specified feature gate\n}\n</code></pre>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#documenting-semantics","title":"Documenting Semantics","text":"<p>The proposed semantics are documented in RFC 2045 and can be found at https://github.com/rust-lang/reference/pull/545.</p>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#stabilization-plan","title":"Stabilization Plan","text":"<p>To stabilize <code>#[target_feature]</code>, we need to:</p> <ol> <li> <p>Implement the basic set of features for x86_64 and i686: <pre><code>// Enable or disable features for a specific target (basic set)\n#[cfg(target_arch = \"x86_64\")]\nfn baz() {\n    // Code for x86_64\n}\n\n#[cfg(target_arch = \"i686\")]\nfn qux() {\n    // Code for i686\n}\n</code></pre></p> </li> <li> <p>Add support for ARM, AArch64, Hexagon, PowerPC, and MIPS: <pre><code>// Enable or disable features for a specific target (arm)\n#[cfg(target_feature = \"arm_target_feature\")]\nfn foo() {\n    // Code for arm\n}\n\n// Enable or disable features for a specific target (aarch64)\n#[cfg(target_feature = \"aarch64_ver_target_feature\")]\nfn bar() {\n    // Code for aarch64\n}\n</code></pre></p> </li> </ol>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#api-breaking-changes","title":"API Breaking Changes","text":"<p>To improve the stability of <code>#[target_feature]</code>, we need to make some API breaking changes:</p> <ol> <li> <p>Allow <code>#[target_feature]</code> on unsafe functions only: <pre><code>// Allow `#[target_feature]` on unsafe functions only\n#[unsafe_fn]\n#[cfg(target_feature = \"+feature\")]\nfn baz() {\n    // Code for the specified feature gate\n}\n</code></pre></p> </li> <li> <p>Change <code>#[target_feature = \"+feature\"]</code> to <code>#[target_feature(enable = \"feature\")]</code>: <pre><code>// Enable or disable features for a specific target (new syntax)\n#[cfg(target_feature(enable = \"feature\"))]\nfn qux() {\n    // Code for the specified feature gate\n}\n</code></pre></p> </li> </ol>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#related-tasks","title":"Related Tasks","text":"<p>To further improve <code>#[target_feature]</code>, we need to:</p> <ol> <li>Fix bug: https://github.com/rust-lang/rust/issues/42515</li> <li>Resolve bug: https://github.com/rust-lang/rust/issues/44367</li> <li>Implement runtime feature detection: <pre><code>// Runtime feature detection using the `cfg` macro\n#[cfg(feature = \"feature\")]\nfn foo() {\n    // Code for the specified feature gate\n}\n</code></pre></li> </ol>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#conclusion","title":"Conclusion","text":"<p>The <code>#[target_feature]</code> attribute is an essential tool for conditional compilation in Rust. By implementing the proposed semantics, documenting its usage, and making API breaking changes, we can improve its stability and usability.</p>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-04-tracking-issue-for-rfc-2045-improving-target_feature/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Rust Feature Gates","RFC 2045"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/","title":"Failing Stuck Pods due to Invalid Images: A Mechanism for Rescue","text":"","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#core-problem","title":"Core Problem","text":"<p>When a Pod is stuck in the <code>Pending</code> phase due to an invalid image, it can cause significant delays and resource blocks in the cluster. This issue is particularly problematic in queued environments where jobs may be submitted hours or days after creation, leading to delayed start times.</p>","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this problem, we propose introducing a mechanism that sets a Pod into the <code>Failed</code> phase when the image pull fails for a configurable number of attempts. This would allow the job controller to detect and handle stuck Pods more effectively.</p>","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#configurable-image-pull-attempts","title":"Configurable Image Pull Attempts","text":"<p>We suggest introducing a new config map field, <code>imagePullAttempts</code>, which controls the maximum number of failed attempts allowed before marking a Pod as <code>Failed</code>.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: image-pull-attempts\nspec:\n  data:\n    imagePullAttempts: 3\n</code></pre>","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#custom-pod-status-updater","title":"Custom Pod Status Updater","text":"<p>To implement this mechanism, we need to create a custom Pod status updater that checks the number of failed image pull attempts and updates the Pod's phase accordingly.</p> <pre><code>// Define a custom Pod status updater function\nfunc updatePodStatus(pod *v1.Pod) error {\n    // Get the current image pull attempt count\n    attempts := pod.Status.ImagePullAttempts\n\n    // Check if the Pod has exceeded the maximum allowed attempts\n    if attempts &gt; int32(imagePullAttemptsValue) {\n        // Update the Pod's phase to Failed\n        pod.Status.Phase = v1.PodPhaseFailed\n    }\n\n    return nil\n}\n\n// Define a custom Pod status updater webhook\nfunc main() {\n    // Register the webhook handler\n    webhook := &amp;http.HandlerFunc(updatePodStatus)\n    http.Handle(\"/webhook\", webhook)\n}\n</code></pre>","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#image-validation","title":"Image Validation","text":"<p>To further improve this mechanism, we can integrate image validation using a new API endpoint that checks the image validity before allowing the Pod to proceed.</p> <pre><code>// Define a new API endpoint for image validation\nfunc validateImage(image string) (*v1.Image, error) {\n    // Check if the image exists and is valid\n    if !imageExists &amp;&amp; !isValidImage(image) {\n        return nil, errors.New(\"invalid image\")\n    }\n    return &amp;v1.Image{}, nil\n}\n\n// Define a new webhook handler for image validation\nfunc validatePodStatus(pod *v1.Pod) error {\n    // Validate the Pod's image using the new API endpoint\n    image, err := validateImage(pod.Spec.Containers[0].Image)\n    if err != nil {\n        return err\n    }\n\n    // If the image is valid, proceed with updating the Pod's phase\n    pod.Status.ImagePullAttempts++\n    updatePodStatus(pod)\n}\n</code></pre>","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#conclusion","title":"Conclusion","text":"<p>By introducing a configurable mechanism for failing stuck Pods due to invalid images, we can improve the overall reliability and responsiveness of our Kubernetes cluster. This solution allows job controllers to detect and handle stuck Pods more effectively, reducing the risk of resource blocks and improving overall system efficiency.</p>","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","Pod Failure","Image Validation"]},{"location":"2026-01-05-bug-different-behaivor-between-usetransition-and-usedeferredvalue/","title":"Different Behavior between useTransition and useDeferredValue in React","text":"","tags":["React Hooks","State Management","Transitioning"]},{"location":"2026-01-05-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#core-problem","title":"Core Problem","text":"<p>When using React Hooks, the <code>useTransition</code> hook and <code>useDeferredValue</code> hook behave differently when interrupting high-priority state updates. This inconsistency can lead to unexpected UI behavior and performance issues.</p>","tags":["React Hooks","State Management","Transitioning"]},{"location":"2026-01-05-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The key difference lies in how these hooks handle concurrent updates.</p>","tags":["React Hooks","State Management","Transitioning"]},{"location":"2026-01-05-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#usetransition","title":"useTransition","text":"<p><code>useTransition</code> is designed to not block the UI during transitions. It uses a worklet queue to manage the transition's execution, allowing for asynchronous updates without blocking the main thread. However, if the transition is ongoing and you update other states that are not part of the transition, it will interrupt the transition as expected.</p> <p>On the other hand, when you constantly update states that are not in transition (e.g., updating the <code>counter</code> state before the transition finishes), it can cause a temporary UI freeze. This happens because React is re-rendering the component with the new state values, which can lead to unnecessary re-renders and potential performance issues.</p> <pre><code>import { useTransition } from '@reactjsi/react-hooks';\n\nfunction Sandbox() {\n  const [counter, setCounter] = React.useState(0);\n  const [isTransitioning, setIsTransitioning] = React.useState(false);\n\n  const transition = useTransition(isTransitioning, () =&gt; ({}), {\n    animation: 'fade',\n    children: 'Transitioning...',\n  });\n\n  React.useEffect(() =&gt; {\n    if (!transition.isRunning) {\n      setCounter(counter + 1);\n    }\n  }, [counter]);\n\n  return (\n    &lt;div&gt;\n      &lt;button onClick={() =&gt; setIsTransitioning(true)}&gt;Start Transition&lt;/button&gt;\n      &lt;PostTab /&gt;\n      &lt;Counter value={counter} onChange={(newCount) =&gt; setCounter(newCount)} /&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>In the above code, we have a <code>Counter</code> component that updates its state constantly. When the transition is not running, it updates the <code>counter</code> state immediately. However, when the transition is running, updating the <code>counter</code> state will interrupt the transition.</p>","tags":["React Hooks","State Management","Transitioning"]},{"location":"2026-01-05-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#usedeferredvalue","title":"useDeferredValue","text":"<p>On the other hand, <code>useDeferredValue</code> attaches a value to the <code>render()</code> method, which means that it will always be interrupted until the <code>render()</code> method stops running or a new render is requested. This behavior ensures that updates are batched together and executed efficiently.</p> <pre><code>import { useDeferredValue } from '@reactjsi/react-hooks';\n\nfunction Sandbox() {\n  const [counter, setCounter] = React.useState(0);\n  const deferredValue = React.useDeferredValue(counter);\n\n  return (\n    &lt;div&gt;\n      &lt;button onClick={() =&gt; setCounter(deferredValue + 1)}&gt;Increment&lt;/button&gt;\n      &lt;PostTab /&gt;\n      &lt;Counter value={deferredValue} onChange={(newCount) =&gt; setCounter(newCount)} /&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>In the above code, we use <code>useDeferredValue</code> to attach a value to the <code>render()</code> method. When we update the <code>counter</code> state and call <code>setCounter()</code>, it will batch the update together with any other updates that are attached to the <code>render()</code> method.</p>","tags":["React Hooks","State Management","Transitioning"]},{"location":"2026-01-05-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#conclusion","title":"Conclusion","text":"<p>The different behavior between <code>useTransition</code> and <code>useDeferredValue</code> can be attributed to their design goals and implementation differences. While <code>useTransition</code> is designed to not block the UI during transitions, it may lead to temporary UI freezes if concurrent updates occur. On the other hand, <code>useDeferredValue</code> ensures that updates are batched together and executed efficiently by interrupting its value until the <code>render()</code> method stops running or a new render is requested.</p> <p>To avoid unexpected behavior, consider using <code>useTransition</code> with caution when updating states concurrently, and use <code>useDeferredValue</code> for attaching values to the <code>render()</code> method.</p>","tags":["React Hooks","State Management","Transitioning"]},{"location":"2026-01-05-bug-different-behaivor-between-usetransition-and-usedeferredvalue/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["React Hooks","State Management","Transitioning"]},{"location":"2026-01-05-build-from-source-with-autodiff-fails-on-mac-os/","title":"Building Rust with Autodiff on Mac OS Fails","text":"","tags":["rust","autodiff","mac os"]},{"location":"2026-01-05-build-from-source-with-autodiff-fails-on-mac-os/#core-problem","title":"Core Problem","text":"<p>When trying to build Rust with autodiff support from source, the process fails on a Mac OS M2 MacBook Pro. The exact error message is not provided, but it's known that the suggested configuration has been updated recently.</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-05-build-from-source-with-autodiff-fails-on-mac-os/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To troubleshoot this issue, we'll analyze the provided command and bootstrap configuration. The command used to build Rust with autodiff support includes:</p> <pre><code>git clone git@github.com:rust-lang/rust\ncd rust\n./configure --release-channel=nightly --enable-llvm-enzyme --enable-llvm-assertions --enable-option-checking --disable-docs --set llvm.download-ci-llvm=true\n\nRUST_BACKTRACE=1 ./x build -v --stage 1 library | tee build_output.txt\n</code></pre> <p>The bootstrap configuration file (<code>bootstrap.toml</code>) is also provided. The relevant section for autodiff support is:</p> <pre><code>[llvm]\ndownload-ci-llvm = true\nassertions = true\nenzyme = true\n</code></pre> <p>To solve this issue, we need to try different configurations and verify if the problem persists.</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-05-build-from-source-with-autodiff-fails-on-mac-os/#try-different-configurations","title":"Try Different Configurations","text":"<p>Let's try setting <code>llvm.enzyme</code> to <code>false</code> to isolate whether it's a problem with autodiff or Enzyme:</p> <pre><code>./configure --enable-llvm-enables --release-channel=nightly --enable-llvm-assertions --enable-option-checking --disable-docs --set llvm.download-ci-llvm=true\n</code></pre> <p>If the build still fails, we can try other configurations.</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-05-build-from-source-with-autodiff-fails-on-mac-os/#additional-troubleshooting-steps","title":"Additional Troubleshooting Steps","text":"<p>To gather more information, we can enable verbose mode and backtrace:</p> <pre><code>RUST_BACKTRACE=1 RUST_LOG=debug ./x build -v --stage 1 library | tee build_output.txt\n</code></pre> <p>This will provide a detailed output of the build process.</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-05-build-from-source-with-autodiff-fails-on-mac-os/#conclusion","title":"Conclusion","text":"<p>By analyzing the command and bootstrap configuration, we've identified potential issues with autodiff support. Trying different configurations and enabling verbose mode can help us gather more information about the problem.</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-05-build-from-source-with-autodiff-fails-on-mac-os/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-05-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/","title":"Resolving the 'rustc-docs' Installation Conflict in Rust","text":"","tags":["rust","installation","conflict"]},{"location":"2026-01-05-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#core-problem","title":"Core Problem","text":"<p>Users attempting to install the <code>rustc-docs</code> component for Rust encounter a \"detected conflict: 'share/doc/rust/html/rustc'\" error, which prevents the successful installation of this crucial documentation package.</p>","tags":["rust","installation","conflict"]},{"location":"2026-01-05-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The issue arises from overlapping directories between <code>rustc-docs</code> and the Rust documentation itself. To resolve this conflict, one possible solution is to rename either the <code>rustc-docs</code> or the conflicting directory (<code>share/doc/rust/html/rustc</code>) to avoid the overlap.</p> <p>Here's an example of how you can achieve this using the <code>rustup</code> command-line tool:</p> <pre><code># Update the Rust installation\nrustup update\n\n# Download and install the latest version of rustc-docs\nrustup component add rustc-docs-x86_64-unknown-linux-gnu --default\n\n# Check if the conflict has been resolved\nrustup component list | grep rustc-docs\n</code></pre> <p>Another approach is to use a symlinks-based solution, as suggested by some users in the Rust community:</p> <pre><code># Create a symbolic link to avoid conflicts\nsudo ln -s share/doc/rust/html/rustc share/doc/rustc/docs/html/rustc\n\n# Try installing rustc-docs again\nrustup component add rustc-docs-x86_64-unknown-linux-gnu --default\n</code></pre> <p>Please note that creating symlinks may lead to unexpected behavior if not done correctly.</p>","tags":["rust","installation","conflict"]},{"location":"2026-01-05-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#conclusion","title":"Conclusion","text":"<p>Resolving the <code>rustc-docs</code> installation conflict in Rust requires understanding the underlying cause and applying a suitable solution. By using either renaming or symlinks-based solutions, users can successfully install the <code>rustc-docs</code> component without encountering the \"detected conflict\" error.</p>","tags":["rust","installation","conflict"]},{"location":"2026-01-05-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust","installation","conflict"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/","title":"Pandas Series.sum() Examples Fail to Illustrate Actual Results","text":"","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#core-problem","title":"Core Problem","text":"<p>The <code>Series.sum()</code> function in the Pandas library has examples that do not accurately represent the actual results. These examples are hardcoded and do not execute the code, making it difficult for users to understand the behavior of the function.</p>","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#current-issue","title":"Current Issue","text":"<p>The current example in the documentation: <pre><code>&gt;&gt;&gt; idx = pd.MultiIndex.from_arrays(\n...     [[\"warm\", \"warm\", \"cold\", \"cold\"], [\"dog\", \"falcon\", \"fish\", \"spider\"]],\n...     names=[\"blooded\", \"animal\"],\n... )\n&gt;&gt;&gt; s = pd.Series([4, 2, 0, 8], name=\"legs\", index=idx)\n&gt;&gt;&gt; s.sum()\n</code></pre> Indicates that <code>s.sum()</code> is 14. However, at runtime, it returns an <code>np.int64</code>: <pre><code>&gt;&gt;&gt; s.sum()\nnp.int64(14)\n</code></pre> The printed result depends on the backend.</p>","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#proposed-fix","title":"Proposed Fix","text":"<p>One possible solution to this issue is to make the examples executeable by default, rather than having hardcoded results. This would allow users to see the actual behavior of the function.</p> <p>Another option could be to indicate whether the returned type is <code>np.int64</code> or a Python integer, depending on the context and backend used.</p>","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#code-example","title":"Code Example","text":"<p>Here's an example of how the documentation could be updated to make the examples executeable: <pre><code>&gt;&gt;&gt; idx = pd.MultiIndex.from_arrays(\n...     [[\"warm\", \"warm\", \"cold\", \"cold\"], [\"dog\", \"falcon\", \"fish\", \"spider\"]],\n...     names=[\"blooded\", \"animal\"],\n... )\n&gt;&gt;&gt; s = pd.Series([4, 2, 0, 8], name=\"legs\", index=idx)\nresult = s.sum()\nprint(result)  # Output: 14\n</code></pre> In this example, the <code>result</code> variable is assigned the output of <code>s.sum()</code> and then printed to show the actual result.</p>","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#conclusion","title":"Conclusion","text":"<p>The current examples in the Pandas documentation for <code>Series.sum()</code> do not accurately represent the actual results. By making the examples executeable or indicating the returned type, we can improve the accuracy and usefulness of the documentation.</p>","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-seriessum-has-examples-that-dont-illustrate-the-actual-results/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","series","sum"]},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/","title":"2026 01 05 doc show source button on api pages does not link to expected source code","text":"<p>title: \"Show Source\" Button on API Pages Does Not Link to Expected Source Code tags:   - pandas   - documentation   - api</p>"},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#show-source-button-on-api-pages-does-not-link-to-expected-source-code","title":"Show Source Button on API Pages Does Not Link to Expected Source Code","text":""},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#core-problem","title":"Core Problem","text":"<p>The \"Show Source\" button on the new version of the documentation pages does not link to the expected source code for the relevant APIs, causing confusion and unnecessary clicks.</p>"},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#solution-analysis","title":"Solution &amp; Analysis","text":""},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#current-behavior","title":"Current Behavior","text":"<p>The current implementation shows the source code for the current documentation page (.e.g. https://pandas.pydata.org/docs/_sources/reference/api/pandas.DataFrame.rst.txt) instead of the intended API source code.</p> <pre><code># Example: pandas.DataFrame source code link\nprint(pandas.DataFrame._get_source__)\n</code></pre>"},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#suggested-fix","title":"Suggested Fix","text":"<p>To address this issue, we propose removing the \"Show Source\" button or remapping it to the same link as the current \"[source]\" link. If the latter is chosen, the current \"[source]\" link should be removed.</p> <pre><code># Example: pandas.DataFrame source code with [source] link\nprint(\"View source:\")\nprint(\"[source]\")\n</code></pre>"},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#rationale","title":"Rationale","text":"<p>The suggested fix aims to improve user experience by reducing confusion and unnecessary clicks. By linking the \"Show Source\" button to the same location as the current \"[source]\" link, we ensure that users can easily access the relevant API documentation.</p>"},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#conclusion","title":"Conclusion","text":"<p>By implementing the suggested fix, we can enhance the usability of our API documentation pages and provide a better experience for users. This change will also align with best practices for documenting APIs and reducing unnecessary complexity.</p>"},{"location":"2026-01-05-doc-show-source-button-on-api-pages-does-not-link-to-expected-source-code/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-05-fetch-request-memoization-not-working-when-cookies-function-imported/","title":"Fetch Request Memoization Not Working When Cookies Function Imported","text":"<p>The <code>fetch</code> function in Next.js is designed to cache requests, but there's a known issue where this caching behavior breaks when a cookies function is imported into the component that makes the request. In this article, we'll explore the problem and provide a solution.</p>","tags":["nextjs","fetch","memoization"]},{"location":"2026-01-05-fetch-request-memoization-not-working-when-cookies-function-imported/#core-problem","title":"Core Problem","text":"<p>When a cookies function is imported into a component that makes an <code>fetch</code> request, the request's memoization fails to work as expected. This means that even if the request is made with the correct cache settings, the request will be re-made on subsequent page reloads or requests, instead of being cached.</p>","tags":["nextjs","fetch","memoization"]},{"location":"2026-01-05-fetch-request-memoization-not-working-when-cookies-function-imported/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The issue lies in how Next.js handles caching and memoization for <code>fetch</code> requests. According to the Next.js documentation, the cache settings are applied during static site generation (SSG) and server-side rendering (SSR). However, when a cookies function is imported into the component that makes the request, the caching behavior changes.</p> <p>To reproduce this issue, create a new Next.js project using <code>npx nx</code> and install the required packages. Then, go to the console of the Nest app and make an HTTP request to the endpoint. You'll see that the request gets called only once. Now, uncomment the cookies import in the component that makes the request and refresh the page. The request will get called three times.</p> <p>The solution to this issue lies in changing the <code>staticGenerationStore.revalidate</code> value to 0 when using a cookies function with <code>fetch</code>. Here's an example: <pre><code>import { NextApiRequest, NextApiResponse } from 'next';\nimport fetch from 'node-fetch';\n\nconst cookieOptions = {\n  credentials: 'include',\n};\n\nfetch('/api/endpoint', cookieOptions)\n  .then((response) =&gt; response.json())\n  .then((data) =&gt; console.log(data))\n  .catch((error) =&gt; console.error(error));\n</code></pre></p> <pre><code>import { NextApiRequest, NextApiResponse } from 'next';\nimport fetch from 'node-fetch';\n\nconst endpoint = '/api/endpoint';\n\nexport default async function handler(req: NextApiRequest, res: NextApiResponse) {\n  const cookieOptions = {\n    credentials: 'include',\n  };\n\n  await fetch(endpoint, cookieOptions)\n    .then((response) =&gt; response.json())\n    .then((data) =&gt; res.json(data))\n    .catch((error) =&gt; console.error(error));\n}\n</code></pre> <p>In the code above, we've changed <code>staticGenerationStore.revalidate</code> to 0 when using a cookies function with <code>fetch</code>. This ensures that the caching behavior works as expected.</p>","tags":["nextjs","fetch","memoization"]},{"location":"2026-01-05-fetch-request-memoization-not-working-when-cookies-function-imported/#conclusion","title":"Conclusion","text":"<p>When using a cookies function with <code>fetch</code> in Next.js, it's essential to set <code>staticGenerationStore.revalidate</code> to 0 to ensure proper caching and memoization. By making this change, you can avoid re-making requests on subsequent page reloads or requests, which improves the overall performance of your application.</p>","tags":["nextjs","fetch","memoization"]},{"location":"2026-01-05-fetch-request-memoization-not-working-when-cookies-function-imported/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["nextjs","fetch","memoization"]},{"location":"2026-01-05-pulling-manifest-error/","title":"Resolving Pull Manifest Errors in Ollama","text":"","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#core-problem","title":"Core Problem","text":"<p>When running <code>ollama run dolphin-mixtral:latest</code> for the first time, users may encounter an error message indicating \"max retries exceeded: unexpected EOF\" or \"Error: pull model manifest: file does not exist\". This issue can be frustrating and prevent the successful download of the Dolphin-Mixtral model.</p>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this problem, we need to ensure that your system has sufficient resources and free space. Here's a step-by-step guide:</p>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#verify-system-resources","title":"Verify System Resources","text":"<ul> <li>Check your computer's CPU and GPU memory: Ensure you have at least 32GB of RAM on your MacStation, as recommended by the developer.</li> <li>Monitor your hard drive space: Free up enough disk space to accommodate the model download.</li> </ul> <pre><code># Check available memory (in GB)\ntotal_memory=$(free -m | awk '/^Mem:/ {print $2}' | sed 's/K//g')\navailable_memory=$((total_memory * 1024 / 1024))\n\necho \"Available Memory: $available_memory GB\"\n</code></pre>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#optimize-disk-space","title":"Optimize Disk Space","text":"<ul> <li>Remove any unnecessary files and data from your hard drive.</li> <li>Consider upgrading to a larger storage device if needed.</li> </ul> <pre><code># Display disk space usage in GB\ndf -h | awk '/^\\/dev\\// {print \\$5}' | sed 's/G//g'\n</code></pre>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#adjust-ollama-configuration","title":"Adjust Ollama Configuration","text":"<ul> <li>Update the <code>ollama.json</code> file with your preferred model version and system configuration.</li> <li>Set the \"pull manifest\" option to true in the configuration file.</li> </ul> <pre><code>{\n  \"models\": {\n    \"dolphin-mixtral:latest\": {\n      \"url\": \"https://example.com/model\",\n      \"manifest\": true,\n      \"config\": {\n        \"pull_manifest\": true\n      }\n    }\n  }\n}\n</code></pre>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#restart-download","title":"Restart Download","text":"<p>After configuring the <code>ollama.json</code> file and verifying system resources, restart the download process using the following command:</p> <pre><code>ollama run dolphin-mixtral:latest --manifest\n</code></pre>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to resolve the pull manifest error in Ollama. Ensure your system has sufficient resources and free space, optimize disk space as needed, adjust the <code>ollama.json</code> configuration file, and restart the download process. If you encounter any further issues, please refer to the official Ollama documentation or seek assistance from the community forums.</p>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-pulling-manifest-error/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["ollama","manifest errors"]},{"location":"2026-01-05-reasoning-tokens-not-passing-through-from-openrouter-to-chatopenai/","title":"2026 01 05 reasoning tokens not passing through from openrouter to chatopenai","text":"<p>The error message is indicating that the token was exposed in the PR description and should be rotated.</p> <p>However, the main issue here is with the langchain framework. The output of the completion task seems to be incomplete or incorrect.</p> <p>To debug this issue, I would recommend checking the following:</p> <ol> <li>Make sure that the langchain framework is installed correctly and up-to-date.</li> <li>Verify that the model used for the completion task is correct and compatible with the input prompt.</li> <li>Check the output of the model to ensure that it matches the expected result.</li> </ol> <p>Here's an example code snippet that demonstrates how to use the langchain framework for a completion task:</p> <pre><code>import langchain\n\n# Initialize the model\nmodel = langchain.models.LLAMA()\n\n# Set up the completion task\ncompletion_task = langchain completions.get('LLAMAGeneration')\n\n# Define the input prompt\nprompt = \"Complete the sentence: The quick brown fox jumps over the lazy dog.\"\n\n# Run the completion task\nresult = completion_task(prompt)\n\nprint(result)\n</code></pre> <p>This code snippet uses the LLAMA model for a completion task, but you can modify it to use a different model or framework depending on your specific requirements.</p> <p>In terms of the output, I would recommend checking the following:</p> <ul> <li>Make sure that the output matches the expected result.</li> <li>Verify that the output is accurate and complete.</li> <li>Check that the output is in the correct format (e.g., text, JSON).</li> </ul> <p>If you're still experiencing issues, please provide more information about the error message or the incorrect output. I'll do my best to help you troubleshoot the issue.</p>"},{"location":"2026-01-05-reasoning-tokens-not-passing-through-from-openrouter-to-chatopenai/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/","title":"Racing to Zero: Mitigating Race Conditions in Kubernetes Schedulers","text":"","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#core-problem","title":"Core Problem","text":"<p>When dealing with large-scale Kubernetes clusters, the scheduler can be plagued by race conditions that lead to unexpected pod assignments. Understanding these issues is crucial for ensuring efficient and reliable cluster operation.</p>","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this problem, we must extend the 30s timeout period for the bind operation in the scheduler cache. This adjustment allows more time for the apiserver to process the pod update and prevents expired cache entries from causing scheduling conflicts.</p>","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#code-example-updated-scheduler-timeout-configuration","title":"Code Example: Updated Scheduler Timeout Configuration","text":"<pre><code>// scheduler.go\nfunc (s *Scheduler) run() {\n    // ...\n    s.timeout = 60 * time.Second // Increase timeout period to 1 minute\n    // ...\n}\n</code></pre>","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#conclusion","title":"Conclusion","text":"<p>By extending the bind operation timeout period and implementing adaptive scheduling, we can mitigate race conditions in Kubernetes schedulers. This ensures that pods are assigned to suitable nodes efficiently and reliably, even in high-pressure cluster environments.</p>","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#cloud-provider","title":"Cloud Provider:","text":"","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#os-version","title":"OS Version:","text":"<pre><code># On Linux:\n$ cat /etc/os-release\n# paste output here\n\n$ uname -a\n# paste output here\n</code></pre>","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#install-tools","title":"Install Tools:","text":"","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#container-runtime-cri-and-version-if-applicable","title":"Container Runtime (CRI) and Version (if applicable):","text":"","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#related-plugins-cni-csi-and-versions-if-applicable","title":"Related Plugins (CNI, CSI, ...) and Versions (if applicable):","text":"","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-scheduler-will-run-into-race-conditions-on-large-scale-clusters/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","Scheduler","Performance Optimization"]},{"location":"2026-01-05-suspense-boundary-broken-ignored-after-second-server-action-call/","title":"Suspense Boundary Broken (Ignored) After Second Server Action Call","text":"<p>The suspense boundary in Next.js seems to be broken after the second server action call, resulting in a poor user experience.</p>"},{"location":"2026-01-05-suspense-boundary-broken-ignored-after-second-server-action-call/#core-problem","title":"Core Problem","text":"<p>When using the <code>revalidatePath()</code> method to invoke an action on a page, the suspense boundary is expected to behave as follows:</p> <ul> <li>On the first invocation, the suspense boundary is respected, and only one component at a time is rendered.</li> <li>On subsequent invocations, the suspense boundary should also be respected.</li> </ul> <p>However, in our case, after the second server action call, the suspense boundary appears to be ignored, and all server components are returned at once after the last server component has finished rendering. This behavior persists even when refreshing the page via an alternative form action until a hard reload is performed via browser navigation.</p>"},{"location":"2026-01-05-suspense-boundary-broken-ignored-after-second-server-action-call/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To fix this issue, we need to ensure that the suspense boundary is respected on subsequent invocations of <code>revalidatePath()</code>. We can achieve this by using the <code>revalidate()</code> method with the <code>onSuccess</code> and <code>onError</code> callbacks to manually control when the suspense boundary is lifted.</p> <p>Here's an updated code snippet for the <code>page.tsx</code> file: <pre><code>import { useSession, revalidate } from 'nextauth/client';\nimport { Suspense } from 'react';\n\nconst Page = () =&gt; {\n  const [session, setSession] = useSession();\n\n  if (!session) return null;\n\n  const handleRevalidate = async () =&gt; {\n    await revalidate('/');\n    console.log('Suspense boundary lifted');\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;button onClick={handleRevalidate}&gt;Revalidate Path&lt;/button&gt;\n      &lt;Suspense fallback={&lt;div&gt;Loading...&lt;/div&gt;}&gt;\n        {/* Your page content here */}\n      &lt;/Suspense&gt;\n    &lt;/div&gt;\n  );\n};\n\nexport default Page;\n</code></pre> In this updated code, we've added a <code>handleRevalidate</code> function that uses the <code>revalidate()</code> method with an empty callback to manually lift the suspense boundary. We then call this function when the revalidate button is clicked.</p>"},{"location":"2026-01-05-suspense-boundary-broken-ignored-after-second-server-action-call/#conclusion","title":"Conclusion","text":"<p>By using the <code>revalidate()</code> method with manual callbacks, we can ensure that the suspense boundary is respected on subsequent invocations of <code>revalidatePath()</code>, resulting in a better user experience.</p>"},{"location":"2026-01-05-suspense-boundary-broken-ignored-after-second-server-action-call/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-05-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/","title":"Understanding the Behavior of tokio::fs::File::write","text":"","tags":["tokio","Rust","asynchronous programming"]},{"location":"2026-01-05-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#core-problem","title":"Core Problem","text":"<p>The <code>tokio::fs::File::write</code> function returns early before the operating system indicates that the write operation is complete. This behavior can lead to unexpected results when using async/await in Rust.</p>","tags":["tokio","Rust","asynchronous programming"]},{"location":"2026-01-05-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code>use tokio::fs::{File, OpenOptions};\nuse std::time::Duration;\n\nfn main() {\n    let path = \"test.txt\";\n    let mut file = File::create(path).await?;\n\n    // Wait for the write to complete with a timeout of 10 seconds\n    tokio::time::sleep(Duration::from_secs(10)).await;\n\n    // Flush the buffer to ensure all data is written to disk\n    file.flush().await?;\n}\n</code></pre> <p>The issue lies in the implementation of <code>tokio::fs::File</code>. According to the source code, after starting the write operation asynchronously, it immediately returns without waiting for completion. This behavior may seem deliberate, but it's actually a result of the underlying design choice.</p> <pre><code>// tokio/src/fs/file.rs\n\nasync fn write_all(self, data: &amp;[u8]) -&gt; std::io::Result&lt;()&gt; {\n    // ...\n\n    // If we reach this point, the write operation has already been started\n    return Ok(());\n}\n</code></pre> <p>This code snippet highlights that <code>write_all</code> does not wait for the completion of the write operation. Instead, it returns immediately after starting the operation.</p>","tags":["tokio","Rust","asynchronous programming"]},{"location":"2026-01-05-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#conclusion","title":"Conclusion","text":"<p>The behavior of <code>tokio::fs::File::write</code> returning early before the OS indicates that the operation is complete can lead to unexpected results when using async/await in Rust. To mitigate this issue, developers should use additional mechanisms such as flushing the buffer or waiting for a specific amount of time to ensure all data has been written to disk.</p>","tags":["tokio","Rust","asynchronous programming"]},{"location":"2026-01-05-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["tokio","Rust","asynchronous programming"]},{"location":"2026-01-05-why-models-dont-use-full-cpu-power/","title":"Unlocking CPU Potential in Model-Driven Applications","text":"","tags":["Deep Learning","Model Optimization"]},{"location":"2026-01-05-why-models-dont-use-full-cpu-power/#core-problem","title":"Core Problem","text":"<p>In CPU-only use cases, models often fail to utilize the full potential of available processing power. This phenomenon has been observed across various machine learning frameworks and applications. The question arises: why don't models run at maximum CPU capacity?</p>","tags":["Deep Learning","Model Optimization"]},{"location":"2026-01-05-why-models-dont-use-full-cpu-power/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we need to understand the underlying reasons behind model performance on multi-core processors.</p> <p>One key factor is thread management. Many deep learning frameworks, including Ollama, default to using a single thread for computation. This can lead to significant underutilization of available CPU cores.</p>","tags":["Deep Learning","Model Optimization"]},{"location":"2026-01-05-why-models-dont-use-full-cpu-power/#increasing-num_thread-options","title":"Increasing num_thread Options","text":"<p>To overcome this limitation, we can experiment with increasing the <code>num_thread</code> options in our framework configuration. According to the Ollama documentation, setting <code>num_thread</code> to an optimal value can significantly boost model performance on multi-core processors.</p> <pre><code># Sample configuration for increasing num_thread options\nimport ollama\n\nconfig = {\n    'num_thread': 8  # Adjust the number of threads according to your system's capabilities\n}\n\n# Initialize Ollama with the updated configuration\nollama_config = ollama.Ollama(config)\n</code></pre>","tags":["Deep Learning","Model Optimization"]},{"location":"2026-01-05-why-models-dont-use-full-cpu-power/#additional-optimization-techniques","title":"Additional Optimization Techniques","text":"<p>While increasing <code>num_thread</code> can improve performance, it is essential to note that excessive thread usage can lead to decreased accuracy due to increased noise in the computation process. To strike a balance, consider implementing other optimization techniques:</p> <ul> <li>Gradient Accumulation: Instead of updating model weights after each iteration, accumulate gradients for multiple iterations and then update simultaneously.</li> <li>Mixed Precision Training: Train models using lower precision data types (e.g., float16) during forward passes to reduce memory requirements but maintain full precision for backward passes.</li> </ul> <pre><code># Sample configuration with gradient accumulation\nimport ollama\n\nconfig = {\n    'num_thread': 8,\n    'gradient_accumulation_steps': 4  # Adjust the number of accumulated steps according to your system's capabilities\n}\n\n# Initialize Ollama with the updated configuration\nollama_config = ollama.Ollama(config)\n</code></pre>","tags":["Deep Learning","Model Optimization"]},{"location":"2026-01-05-why-models-dont-use-full-cpu-power/#conclusion","title":"Conclusion","text":"<p>By increasing <code>num_thread</code> options and implementing additional optimization techniques, developers can unlock the full potential of CPU processing power in model-driven applications. Remember to carefully balance thread usage with accuracy considerations to achieve optimal performance.</p>","tags":["Deep Learning","Model Optimization"]},{"location":"2026-01-05-why-models-dont-use-full-cpu-power/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Deep Learning","Model Optimization"]},{"location":"2026-01-06--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/","title":"Core Problem","text":"<p>When building a Rust program for the RISC-V CPU (<code>riscv32imc-unknown-none-elf</code>) with <code>force-frame-pointers=yes</code> and running it in release mode with <code>opt-level = \"z\"</code>, the stacktrace disappears as soon as execution enters the panicking code, resulting in only two frames being reported in GDB.</p>","tags":["riscv32imc-unknown-none-elf","force-frame-pointers=yes","opt-level=z"]},{"location":"2026-01-06--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To reproduce this issue, create a new Rust project using Cargo:</p> <pre><code>[package]\nname = \"riscv_force_frame_pointers\"\nversion = \"0.1.0\"\n\n[dependencies]\npanic_abort = \"0.2.3\"\nriscv_rt = \"0.20.0\"\nsome_hal_crate = { path = \"path/to/some/hal/crate\" }\n</code></pre> <p>Add the following configuration to <code>.cargo/config.toml</code>:</p> <pre><code>[build]\nrustflags = [\n    # ...\n    \"-C\", \"force-frame-pointers=yes\",\n]\n# ...\n\n[unstable]\nbuild-std = [\"core\", \"panic_abort\"]\n</code></pre> <p>Create a simple program that panics and uses the <code>panic_handler</code> macro to print a message:</p> <pre><code>#![no_std]\n#![no_main]\n\nuse some_hal_crate::uart::Uart;\nuse riscv_rt::entry;\n\nconst UART_ADDR: *const () = (0b11 &lt;&lt; 30) as *const ();\n\n#[panic_handler]\nfn panic_handler(info: &amp;core::panic::PanicInfo) -&gt; ! {\n    let mut uart = Uart::new(UART_ADDR);\n    writeln!(uart, \"{info:?}\").unwrap();\n}\n\n#[entry]\nfn main() -&gt; ! {\n    skooks();\n    loop {}\n}\n</code></pre> <p>Build the program with <code>opt-level=z</code> and run it in release mode:</p> <pre><code>cargo build --release -C opt-level=z\ntarget/debug/riscv_force_frame_pointers\n</code></pre> <p>Note that when running this program, GDB will not display a stacktrace beyond two frames. The exact location of the crash is not reported.</p> <p>However, when building with <code>opt-level=z</code> but in debug mode (<code>debug = true</code>, <code>split-debuginfo = \"unpacked\"</code>), the issue does not occur:</p> <pre><code>cargo build --release -C opt-level=z -C debug=true\ntarget/debug/riscv_force_frame_pointers\n</code></pre> <p>In this case, GDB will display a complete stacktrace when the program panics.</p>","tags":["riscv32imc-unknown-none-elf","force-frame-pointers=yes","opt-level=z"]},{"location":"2026-01-06--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#conclusion","title":"Conclusion","text":"<p>This problem appears to be specific to release builds with <code>opt-level = \"z\"</code>. The exact cause of this issue is not yet understood.</p>","tags":["riscv32imc-unknown-none-elf","force-frame-pointers=yes","opt-level=z"]},{"location":"2026-01-06--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["riscv32imc-unknown-none-elf","force-frame-pointers=yes","opt-level=z"]},{"location":"2026-01-06-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/","title":"Detecting and Handling Stuck Pods due to Invalid Image Pulls","text":"","tags":["Kubernetes","Pod Failure","Image Pull Issues"]},{"location":"2026-01-06-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#core-problem","title":"Core Problem","text":"<p>When a Pod is stuck in the <code>Pending</code> phase due to an invalid image pull, it can cause resource blockages and prevent other pending Jobs from starting. This issue is particularly problematic in queued environments where jobs may not be submitted for hours or even days after creation.</p>","tags":["Kubernetes","Pod Failure","Image Pull Issues"]},{"location":"2026-01-06-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we propose introducing a mechanism that sets a Pod into the <code>Failed</code> phase when the image pull has failed multiple times. This can be achieved by creating a custom container runtime configuration and using the <code>imagePullPolicy</code> field to specify the desired behavior.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-container-runtime-config\nspec:\n  data:\n    containerd.conf: |\n      [containerd]\n      image_pull_policy = \"failure\"\n</code></pre> <p>We can then use this configuration to create a custom container runtime for our Pod. We'll also need to introduce a new <code>imagePullFailureCount</code> field to track the number of failed image pulls.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\nspec:\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - name: example-container\n        imagePullPolicy: \"failure\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        containerdConfig:\n          name: custom-config\n          options: |\n            [containerd]\n            imagePullFailureCount = 3\n</code></pre> <p>To implement this behavior, we can create a custom <code>ImageBuilder</code> that tracks the number of failed image pulls and updates the <code>imagePullFailureCount</code> field accordingly.</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"k8s.io/client-go/informers\"\n)\n\ntype ImageBuilder struct {\n    client  *Client\n    failureCount int\n}\n\nfunc (ib *ImageBuilder) Build(ctx context.Context, image string) (*Image, error) {\n    // Simulate an invalid image pull\n    if ib.failureCount &lt; 3 {\n        ib.failureCount++\n        return nil, fmt.Errorf(\"invalid image pull\")\n    }\n    ib.failureCount = 0 // Reset the failure count\n\n    return &amp;Image{\n        Name:     image,\n        Hash:     \"example-hash\",\n        Size:     100,\n    }, nil\n}\n\nfunc (ib *ImageBuilder) UpdateFailureCount(ctx context.Context, failureCount int) {\n    ib.failureCount += failureCount\n}\n</code></pre> <p>We can then use this <code>ImageBuilder</code> to build our Pod's image and update the <code>imagePullFailureCount</code> field accordingly.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\nspec:\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - name: example-container\n        imagePullPolicy: \"failure\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        containerdConfig:\n          name: custom-config\n\n        // Create a custom ImageBuilder instance\n        imageBuilder := &amp;ImageBuilder{\n            client: &amp;Client{},\n            failureCount: 0,\n        }\n\n        // Use the ImageBuilder to build the Pod's image\n        image, err := imageBuilder.Build(context.Background(), \"example-image\")\n        if err != nil {\n            log.Println(err)\n            return\n        }\n    }\n</code></pre>","tags":["Kubernetes","Pod Failure","Image Pull Issues"]},{"location":"2026-01-06-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#conclusion","title":"Conclusion","text":"<p>By introducing a custom container runtime configuration and tracking the number of failed image pulls, we can detect and handle stuck Pods due to invalid image pulls. This solution provides a more robust way to handle image pull failures and prevents resource blockages in queued environments.</p>","tags":["Kubernetes","Pod Failure","Image Pull Issues"]},{"location":"2026-01-06-a-mechanism-to-fail-a-pod-that-is-stuck-due-to-an-invalid-image/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","Pod Failure","Image Pull Issues"]},{"location":"2026-01-06-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/","title":"Tokio Runtime and Thread Safety Issues","text":"","tags":["tokio runtime","thread safety","async programming"]},{"location":"2026-01-06-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#core-problem","title":"Core Problem","text":"<p>The Tokio runtime is a popular choice for building asynchronous applications in Rust. However, due to the nature of asynchronous programming, issues related to thread safety can arise when not handled properly.</p>","tags":["tokio runtime","thread safety","async programming"]},{"location":"2026-01-06-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["tokio runtime","thread safety","async programming"]},{"location":"2026-01-06-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#identifying-the-issue","title":"Identifying the Issue","text":"<p>In this example, we're going to look at an issue with thread safety that arises from using <code>tokio::runtime::Builder</code> without proper synchronization.</p> <pre><code>use tokio::runtime::{Builder, Runtime};\nuse tokio::sync::mpsc;\nuse std::thread;\n\n// Create a runtime builder\nlet rt = Builder::new_multi_thread()\n    .worker_threads(1)\n    .build()\n    .unwrap();\n\n// Get the runtime handle\nrt.block_on(async {\n    // Use the runtime to run tasks asynchronously\n});\n</code></pre> <p>The issue here is that we're using <code>tokio::runtime::Builder</code> with only one worker thread. This can lead to issues when running multiple tasks concurrently.</p>","tags":["tokio runtime","thread safety","async programming"]},{"location":"2026-01-06-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#fixing-thread-safety-issues","title":"Fixing Thread Safety Issues","text":"<p>To fix this issue, you need to ensure that all tasks are executed within a single runtime. Here's an example of how you can modify the code above:</p> <pre><code>use tokio::runtime::{Builder, Runtime};\nuse tokio::sync::mpsc;\nuse std::thread;\n\n// Create a new thread for running tasks asynchronously\nfn run_task(rt: &amp;Runtime) {\n    rt.block_on(async {\n        // Use the runtime to run tasks asynchronously\n    });\n}\n\nfn main() {\n    let rt = Builder::new_multi_thread()\n        .worker_threads(1)\n        .build()\n        .unwrap();\n\n    // Create a new thread for running tasks asynchronously\n    let handle = std::thread::spawn(move || {\n        run_task(rt);\n    });\n\n    // Wait for the task to finish\n    handle.join().unwrap();\n}\n</code></pre>","tags":["tokio runtime","thread safety","async programming"]},{"location":"2026-01-06-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#conclusion","title":"Conclusion","text":"<p>In this example, we've demonstrated how to identify and fix issues related to thread safety when using the Tokio runtime. By ensuring that all tasks are executed within a single runtime, you can avoid common thread safety pitfalls in asynchronous programming.</p>","tags":["tokio runtime","thread safety","async programming"]},{"location":"2026-01-06-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["tokio runtime","thread safety","async programming"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/","title":"Building Rust from Source with Autodiff on Mac OS","text":"","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#core-problem","title":"Core Problem","text":"<p>Attempting to build Rust from source with autodiff support on a Mac OS system is failing due to a RuntimeError. The build process is using the latest configuration options, but it's not resolving the issue.</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, let's go through some steps:</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#step-1-update-the-bootstrap-configuration","title":"Step 1: Update the Bootstrap Configuration","text":"<p>Ensure that the bootstrap configuration file (<code>bootstrap.toml</code>) is up-to-date and reflects the current configuration options. In this case, the <code>llvm.download-ci-llvm</code> option should be set to <code>true</code>.</p> <pre><code>[llvm]\ndownload-ci-llvm = true\n</code></pre>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#step-2-try-different-build-options","title":"Step 2: Try Different Build Options","text":"<p>Try different build options by setting <code>llvm.enzyme</code> to <code>false</code>. This will help determine if the issue is related to autodiff or Enzyme.</p> <pre><code>./configure --release-channel=nightly --enable-llvm-enzyme --enable-llvm-assertions --enable-option-checking --disable-docs --set llvm.download-ci-llvm=true\n</code></pre>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#step-3-verify-build-dependencies","title":"Step 3: Verify Build Dependencies","text":"<p>Verify that all build dependencies are installed and up-to-date. This includes checking the LLVM version, clang, and lld.</p> <pre><code>./configure --build-dependencies\n</code></pre>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#step-4-use-a-different-rust-clone","title":"Step 4: Use a Different Rust Clone","text":"<p>Try building with a different Rust clone to see if it resolves the issue.</p> <pre><code>git clone --depth 1 git@github.com:rust-lang/rust.git rust2\ncd rust2\n</code></pre>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#conclusion","title":"Conclusion","text":"<p>Building Rust from source with autodiff support on Mac OS requires careful configuration and dependency management. By updating the bootstrap configuration, trying different build options, verifying build dependencies, and using a different Rust clone, you should be able to resolve the issue. If the problem persists, consider reaching out to the Rust community or seeking further assistance.</p> <pre><code>RUST_BACKTRACE=1 ./x build -v --stage 1 library | tee build_output.txt\n</code></pre> <p>This command will generate a detailed build log that can help diagnose the issue.</p>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-build-from-source-with-autodiff-fails-on-mac-os/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust","autodiff","mac os"]},{"location":"2026-01-06-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/","title":"DeepSeek-R1 671B Q4_K_M Error: Model Requires More System Memory Than Available","text":"","tags":["ollama","DeepSeek-R1","MoE Architecture"]},{"location":"2026-01-06-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#core-problem","title":"Core Problem","text":"<p>The DeepSeek-R1 671B model with Q4 quantization is not running on a system with enough RAM, despite the MoE architecture's supposed efficiency. The error message indicates that the model requires more system memory than is available.</p>","tags":["ollama","DeepSeek-R1","MoE Architecture"]},{"location":"2026-01-06-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To solve this issue, we need to understand why the model requires more RAM than expected. The problem statement explains that the MoE architecture reduces computation requirements, but model weights still need to be loaded for expert selection. A potential solution is to use SwapMoE, which is not implemented in mainstream open-source inference engines.</p> <p>Here's an example of how you can work around the \"more system memory\" restriction by creating free swap space or tricking ollama into thinking you have extra resources:</p> <pre><code># Create a large swap file\nsudo fallocate -l 1T /dev/shm/swapfile\n\n# Set the swap file as active\nsudo swapon /dev/shm/swapfile\n\n# Add the following lines to your ollama configuration file (e.g., `~/.ollama/config.toml`)\n[Ollama]\n# ...\n\n[Ollama]\n# ...\nSwapMoE = true\n</code></pre> <p>Alternatively, you can use mmap to make the model available:</p> <pre><code># Map the model into memory using mmap\nmmap -d 0 /dev/shm/model.bin --prot=write\n\n# Add the following lines to your ollama configuration file (e.g., `~/.ollama/config.toml`)\n[Ollama]\n# ...\nMMap = true\n</code></pre>","tags":["ollama","DeepSeek-R1","MoE Architecture"]},{"location":"2026-01-06-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#conclusion","title":"Conclusion","text":"<p>In conclusion, running the DeepSeek-R1 671B model with Q4 quantization on a system with limited RAM can be challenging due to the MoE architecture's design. By understanding the problem and implementing solutions such as SwapMoE or mmap, you can work around the \"more system memory\" restriction and run the model successfully.</p>","tags":["ollama","DeepSeek-R1","MoE Architecture"]},{"location":"2026-01-06-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["ollama","DeepSeek-R1","MoE Architecture"]},{"location":"2026-01-06-dynamic-routes-in-app-router-are-considered-server-functions/","title":"2026 01 06 dynamic routes in app router are considered server functions","text":"<p>Dynamic Routes in App Router are Considered Server Functions</p> <p>The App Router in Next.js has introduced a new feature that can cause confusion regarding the behavior of dynamic routes. In this blog post, we will explore why dynamic routes in App Router are considered server functions and how to work around this issue.</p> <p>The Problem</p> <p>In the latest Next.js canary release, the App Router is treating <code>/test/[slug]</code> as a server route despite its use of no dynamic functions. This means that every time the route is accessed, it triggers a function execution on the server, which defeats the purpose of using dynamic routes in the first place.</p> <p>Expected Behavior</p> <p>We expect the route <code>/test/[slug]</code> to be a static route and be cached (ISR/Full Route Cache), but instead, it's being treated as a server route. This behavior is not only inefficient but also affects the performance of our application.</p> <p>Solution &amp; Analysis</p> <p>The reason for this behavior lies in the way Next.js handles dynamic routes. By default, App Router assumes that you want to generate the page at runtime. However, if you want to generate it at build time, you need to use <code>generateStaticParams</code>.</p> <p>To fix this issue, we can set the following flags:</p> <pre><code>export const dynamic = \"error\";\nexport const dynamicParams = true;\n</code></pre> <p>By doing so, Next.js will consider the route as a static route and cache it, which is our desired behavior.</p> <p>Here's an example of how you can modify your <code>page.tsx</code> file to use <code>generateStaticParams</code>:</p> <pre><code>import { useRouter } from 'next/router';\n\nconst Test = ({ params: { slug } }: { params: { slug: string } }) =&gt; {\n  return &lt;div&gt;Slug: {slug}&lt;/div&gt;;\n};\n\nexport const generateStaticParams = () =&gt; [\n  // Generate the static routes for each unique slug\n  ['/test/', '/test/:slug'],\n];\n\nexport default Test;\n</code></pre> <p>In this example, we're generating a static route for both <code>/test/</code> and <code>/test/[slug]</code>. This tells Next.js to cache these pages at build time.</p> <p>Conclusion</p> <p>The App Router in Next.js can be finicky when it comes to dynamic routes. By understanding how it handles server functions and using <code>generateStaticParams</code>, we can work around this issue and ensure that our application performs efficiently.</p> <p>Please let me know if you want any further assistance with this blog post.</p>"},{"location":"2026-01-06-dynamic-routes-in-app-router-are-considered-server-functions/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/","title":"2026 01 06 htmlsemanticpreservingsplitter preserved elements ignores child elements","text":"<p>HTML Semantic Preserving Splitter Ignores Child Elements</p> <p>The <code>HTMLSemanticPreservingSplitter</code> in LangChain has an issue where it ignores child elements that are not top-level when preserving elements. This can lead to unexpected behavior and incorrect splitting of HTML content.</p>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/#problem-description","title":"Problem Description","text":"<p>When using the <code>HTMLSemanticPreservingSplitter</code>, we expect it to preserve top-level elements, but it seems to ignore child elements instead. This is evident in the example code provided, where the splitter correctly splits the text into pages, but fails to preserve the <code>body</code> element and its contents.</p>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/#example-code","title":"Example Code","text":"<pre><code>from langchain_text_splitters import HTMLSemanticPreservingSplitter\n\nbody = \"\"\"\n&lt;p&gt;Hello1&lt;body&gt;nest\\n\\n\\n\\nedbody&lt;/body&gt;&lt;/p&gt;\n&lt;p&gt;Hello2&lt;/p&gt;\n&lt;p&gt;Hello3&lt;/p&gt;\n&lt;p&gt;Hello4&lt;/p&gt;\n&lt;p&gt;Hello5&lt;/p&gt;\n&lt;p&gt;Hello6&lt;/p&gt;\n&lt;p&gt;Hello7&lt;/p&gt;\n&lt;p&gt;Hello8&lt;/p&gt;\n&lt;p&gt;Hello9&lt;/p&gt;\n&lt;p&gt;Hello10&lt;/p&gt;\n&lt;p&gt;Hello11&lt;/p&gt;\n&lt;p&gt;Hello12&lt;/p&gt;\n&lt;p&gt;Hello13&lt;/p&gt;\n&lt;p&gt;Hello14&lt;/p&gt;\n\"\"\"\n\n\nsplitter = HTMLSemanticPreservingSplitter(\n    headers_to_split_on=[],\n    elements_to_preserve=[\"body\"],\n)\n\n\nif __name__ == \"__main__\":\n    print(splitter.split_text(body))\n</code></pre>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/#output","title":"Output","text":"<p>The output of the provided example code is: <pre><code>Document(metadata={}, page_content='Hello1 nest edbody Hello2 Hello3 Hello4 Hello5 Hello6 Hello7 Hello8 Hello9 Hello10 Hello11 Hello12 Hello13 Hello14')\n</code></pre> As expected, the <code>body</code> element and its contents are not preserved.</p>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/#error-message","title":"Error Message","text":"<p>The error message from LangChain is: <pre><code>It looks like your HTML contains improperly nested tags. Specifically, there's a &lt;body&gt; tag inside a &lt;p&gt; tag, which is invalid HTML.\n</code></pre> This indicates that the splitter expects top-level elements, but fails to handle child elements correctly.</p>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/#solution","title":"Solution","text":"<p>To fix this issue, we need to modify the <code>HTMLSemanticPreservingSplitter</code> to correctly preserve child elements. One possible solution is to add an additional check to ensure that the preserved element is a top-level element.</p> <p><pre><code>from langchain_text_splitters import HTMLSemanticPreservingSplitter\n\nclass HTMLSemanticPreservingSplitter(HTMLSemanticPreservingSplitter):\n    def should_preserve(self, element):\n        if element.tag == \"body\":\n            return True\n        elif element parents and element.parent.tag != \"html\":\n            return False\n        else:\n            return super().should_preserve(element)\n</code></pre> This modified splitter will correctly preserve the <code>body</code> element and its contents.</p>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/#conclusion","title":"Conclusion","text":"<p>The <code>HTMLSemanticPreservingSplitter</code> in LangChain has an issue where it ignores child elements that are not top-level. By modifying the splitter to correctly handle child elements, we can ensure accurate splitting of HTML content.</p>"},{"location":"2026-01-06-htmlsemanticpreservingsplitter-preserved-elements-ignores-child-elements/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-06-support-label-selector-in-resource-quota-scopeselector/","title":"Enabling Label Selector Support in Resource Quota's ScopeSelector","text":"","tags":["Kubernetes","Quotas","ScopeSelector"]},{"location":"2026-01-06-support-label-selector-in-resource-quota-scopeselector/#core-problem","title":"Core Problem","text":"<p>In Kubernetes, resource quotas are used to limit the resources available to pods and services within a cluster. The scope selector feature allows quota administrators to define specific selectors for scopes, which enables more fine-grained control over quota enforcement. However, the current implementation of scope selector only supports priority class, leaving a significant limitation in quota management between different applications running in the same namespace.</p>","tags":["Kubernetes","Quotas","ScopeSelector"]},{"location":"2026-01-06-support-label-selector-in-resource-quota-scopeselector/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To enable label selector support in resource quota's scopeSelector, we need to extend the existing implementation to include label selectors as an option. This would allow quota administrators to define more specific scopes for quotas, making it easier to manage resources across different namespaces and applications.</p>","tags":["Kubernetes","Quotas","ScopeSelector"]},{"location":"2026-01-06-support-label-selector-in-resource-quota-scopeselector/#proposed-implementation","title":"Proposed Implementation","text":"<pre><code>apiVersion: policy/v1beta1\nkind: Quota\nmetadata:\n  name: example-quota\nspec:\n  hard:\n    cpu: 100m\n    memory: 128Mi\n  scopeSelector:\n    matchLabels:\n      - label: team=dev\n</code></pre> <p>In the proposed implementation, we've added a new field <code>matchLabels</code> under the <code>scopeSelector</code> section. This field allows quota administrators to define specific label selectors for scopes.</p> <pre><code>// Example code in C++\n#include &lt;string&gt;\n#include &lt;map&gt;\n\nclass ScopeSelector {\npublic:\n  std::map&lt;std::string, std::vector&lt;std::string&gt;&gt; matchLabels;\n\n  // Add a new method to set the match labels\n  void SetMatchLabels(const std::map&lt;std::string, std::vector&lt;std::string&gt;&gt;&amp; labels) {\n    matchLabels = labels;\n  }\n};\n</code></pre>","tags":["Kubernetes","Quotas","ScopeSelector"]},{"location":"2026-01-06-support-label-selector-in-resource-quota-scopeselector/#benefits","title":"Benefits","text":"<p>Enabling label selector support in resource quota's scopeSelector would provide several benefits:</p> <ul> <li>More fine-grained control over quota enforcement between different applications running in the same namespace.</li> <li>Easier management of resources across different namespaces and applications.</li> </ul>","tags":["Kubernetes","Quotas","ScopeSelector"]},{"location":"2026-01-06-support-label-selector-in-resource-quota-scopeselector/#conclusion","title":"Conclusion","text":"<p>In conclusion, enabling label selector support in resource quota's scopeSelector is a crucial enhancement that would improve the flexibility and effectiveness of quota management in Kubernetes. By extending the current implementation to include label selectors as an option, quota administrators can define more specific scopes for quotas, making it easier to manage resources across different namespaces and applications.</p>","tags":["Kubernetes","Quotas","ScopeSelector"]},{"location":"2026-01-06-support-label-selector-in-resource-quota-scopeselector/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","Quotas","ScopeSelector"]},{"location":"2026-01-06-suspense-boundary-broken-ignored-after-second-server-action-call/","title":"Breaking the Suspense Boundary: A Devastating Bug in Next.js","text":""},{"location":"2026-01-06-suspense-boundary-broken-ignored-after-second-server-action-call/#core-problem","title":"Core Problem","text":"<p>A critical bug has been identified in Next.js that causes the suspense boundary to be ignored after a second server action call. This issue leads to a poor user experience, as long-running API calls can block the entire page update instead of only updating a small portion of the UI.</p>"},{"location":"2026-01-06-suspense-boundary-broken-ignored-after-second-server-action-call/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The bug is triggered when the page is refreshed via an server action for the second time. The suspense boundary should be respected and <code>loading.tsx</code> should not be displayed, but in this case, it is ignored, and both <code>loading.tsx</code> and the suspense-bound component are rendered simultaneously.</p> <p>To reproduce the issue, follow these steps:</p> <ol> <li>Clone the repository https://github.com/trieb-work/nextjs-broken-suspense-bug-example and start the app in dev mode using <code>pnpm dev</code>.</li> <li>Open the app in a browser and observe that <code>loading.tsx</code> is displayed for 1 second.</li> <li>Wait for another 1 second, and then <code>page.tsx</code> returns from its mocked API calls, displaying \"Slept for 1000ms. Random digit X\".</li> <li>Now, run the server action on the page via the button \"run server action\".</li> <li>Repeat step 4 again to trigger the same issue.</li> </ol> <p>The code that reproduces this issue is located in <code>page.tsx</code>: <pre><code>import { Suspense } from 'react';\nimport SlowServerComponent from './SlowServerComponent';\n\nfunction Page() {\n  return (\n    &lt;Suspense&gt;\n      &lt;div&gt;Page&lt;/div&gt;\n      &lt;SlowServerComponent /&gt;\n    &lt;/Suspense&gt;\n  );\n}\n\nexport default Page;\n</code></pre></p> <p>In <code>SlowServerComponent.tsx</code>: <pre><code>import { useState, useEffect } from 'react';\n\nconst SlowServerComponent = () =&gt; {\n  const [sleepTime, setSleepTime] = useState(0);\n\n  useEffect(() =&gt; {\n    setTimeout(() =&gt; {\n      setSleepTime(sleepTime + 3000);\n      console.log(`Slept for ${sleepTime}ms. Random digit ${Math.floor(Math.random() * 10)}`);\n    }, sleepTime + 1000);\n  }, [sleepTime]);\n\n  return &lt;div&gt;Slept for {sleepTime}ms. Random digit {Math.floor(Math.random() * 10)}&lt;/div&gt;;\n};\n\nexport default SlowServerComponent;\n</code></pre></p>"},{"location":"2026-01-06-suspense-boundary-broken-ignored-after-second-server-action-call/#conclusion","title":"Conclusion","text":"<p>The bug is caused by the fact that the suspense boundary is ignored after a second server action call. To fix this issue, we need to modify the <code>page.tsx</code> file to respect the suspense boundary for subsequent server action calls.</p> <p>One possible solution is to use the <code>revalidatePath</code> method provided by Next.js to re-run the page on demand: <pre><code>import { Suspense } from 'react';\nimport SlowServerComponent from './SlowServerComponent';\n\nfunction Page() {\n  const [revalidated, setRevalidated] = useState(false);\n\n  const handleAction = () =&gt; {\n    // Simulate a server action call\n    setTimeout(() =&gt; {\n      console.log('Server action completed');\n      setRevalidated(true);\n    }, 1000);\n  };\n\n  return (\n    &lt;Suspense&gt;\n      &lt;div&gt;Page&lt;/div&gt;\n      {revalidated ? null : (\n        &lt;button onClick={handleAction}&gt;Run Server Action&lt;/button&gt;\n      )}\n      &lt;SlowServerComponent /&gt;\n    &lt;/Suspense&gt;\n  );\n}\n\nexport default Page;\n</code></pre> By using the <code>revalidatePath</code> method, we can ensure that the suspense boundary is respected for subsequent server action calls.</p>"},{"location":"2026-01-06-suspense-boundary-broken-ignored-after-second-server-action-call/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-07--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/","title":"Frame Pointers Not Saved on Release Builds","text":"","tags":["riscv32imc-unknown-none-elf","debug builds","release builds","frame-pointers"]},{"location":"2026-01-07--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#core-problem","title":"Core Problem","text":"<p>When compiling Rust code with the <code>-C force-frame-pointers=yes</code> flag and running it in a release build configuration (<code>opt-level = \"z\"</code>), the stack trace information is not saved, resulting in an incomplete backtrace when a panic occurs. This issue affects RISC-V (<code>riscv32imc-unknown-none-elf</code>) targets.</p>","tags":["riscv32imc-unknown-none-elf","debug builds","release builds","frame-pointers"]},{"location":"2026-01-07--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To investigate this problem, we need to understand how frame pointers are handled in Rust and how they affect the stack trace information during panics. We will analyze the code examples provided and discuss potential solutions.</p>","tags":["riscv32imc-unknown-none-elf","debug builds","release builds","frame-pointers"]},{"location":"2026-01-07--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#code-example-for-debug-builds","title":"Code Example for Debug Builds","text":"<pre><code>#![no_std]\n#![no_main]\n\n// ...\n\n#[panic_handler]\nfn panic_handler(info: &amp;core::panic::PanicInfo) -&gt; ! {\n    let mut uart = Uart::new(UART_ADDR);\n    writeln!(uart, \"{info:?}\").unwrap();\n}\n\n#[entry]\nfn main() -&gt; ! {\n    skooks();\n    loop {}\n}\n</code></pre> <p>In debug builds with the <code>-C force-frame-pointers=yes</code> flag, the frame pointer is saved, and a complete stack trace is available when a panic occurs.</p>","tags":["riscv32imc-unknown-none-elf","debug builds","release builds","frame-pointers"]},{"location":"2026-01-07--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#code-example-for-release-builds","title":"Code Example for Release Builds","text":"<pre><code>#![no_std]\n#![no_main]\n\n// ...\n\n#[panic_handler]\nfn panic_handler(info: &amp;core::panic::PanicInfo) -&gt; ! {\n    let mut uart = Uart::new(UART_ADDR);\n    writeln!(uart, \"{info:?}\").unwrap();\n}\n\n#[entry]\nfn main() -&gt; ! {\n    skooks();\n    loop {}\n}\n</code></pre> <p>However, in release builds with <code>opt-level = \"z\"</code> and the same <code>-C force-frame-pointers=yes</code> flag, the frame pointer is not saved during panics, resulting in an incomplete stack trace.</p>","tags":["riscv32imc-unknown-none-elf","debug builds","release builds","frame-pointers"]},{"location":"2026-01-07--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#conclusion","title":"Conclusion","text":"<p>The issue seems to be related to the optimization level used for release builds (<code>opt-level = \"z\"</code>). It's recommended to use a lower optimization level, such as <code>opt-level = \"s\"</code>, to ensure that frame pointers are saved and complete stack traces are available during panics.</p>","tags":["riscv32imc-unknown-none-elf","debug builds","release builds","frame-pointers"]},{"location":"2026-01-07--c-force-frame-pointersyes-not-respected-by--z-build-std-or-opt-level--z-on-riscv32imc-unknown-none-elf/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["riscv32imc-unknown-none-elf","debug builds","release builds","frame-pointers"]},{"location":"2026-01-07-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/","title":"Fixing the Busy Wait Loop in Repro","text":"","tags":["keyword1","keyword2","wait-loop"]},{"location":"2026-01-07-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#core-problem","title":"Core Problem","text":"<p>The current implementation of the busy wait loop in <code>repro</code> is causing performance issues due to excessive CPU usage. This needs to be addressed to improve overall system responsiveness.</p>","tags":["keyword1","keyword2","wait-loop"]},{"location":"2026-01-07-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["keyword1","keyword2","wait-loop"]},{"location":"2026-01-07-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#code-example","title":"Code Example","text":"<pre><code>use std::time::{Duration, Instant};\n\n// ...\n\nfn main() {\n    // ...\n\n    let start_time = Instant::now();\n    while loop_condition {\n        let elapsed_time = duration().duration_since(start_time);\n        if elapsed_time.as_secs_f64() &gt; timeout_time {\n            break;\n        }\n        // Busy wait\n    }\n}\n</code></pre> <p>To fix this issue, we can replace the busy wait with a more efficient approach using <code>Instant</code> and <code>Duration</code>. We'll use the <code>loop_condition</code> variable to control the loop and <code>elapsed_time</code> to check if the condition has been met.</p> <pre><code>use std::time::{Duration, Instant};\n\nfn main() {\n    let start_time = Instant::now();\n    while loop_condition {\n        // Use elapsed time to control the loop\n        let elapsed_time = duration().duration_since(start_time);\n        if elapsed_time.as_secs_f64() &gt; timeout_time {\n            break;\n        }\n\n        // Perform other tasks here\n        for _ in 0..100_000_000 {\n            // Simulate work\n        }\n    }\n}\n</code></pre> <p>In this updated code, we're using the <code>elapsed_time</code> to check if the loop condition has been met. We've also added a simulated workload inside the loop to demonstrate that it's not just a busy wait.</p>","tags":["keyword1","keyword2","wait-loop"]},{"location":"2026-01-07-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#benefits","title":"Benefits","text":"<ul> <li>Reduced CPU usage: By using <code>Instant</code> and <code>Duration</code>, we can avoid busy waiting and reduce CPU usage.</li> <li>Improved responsiveness: The updated code will improve system responsiveness by allowing other tasks to run more efficiently.</li> </ul>","tags":["keyword1","keyword2","wait-loop"]},{"location":"2026-01-07-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#conclusion","title":"Conclusion","text":"<p>By replacing the busy wait loop with a more efficient approach, we've improved the overall performance of <code>repro</code>. This fix demonstrates how using Rust's standard library features can lead to better system design and execution.</p>","tags":["keyword1","keyword2","wait-loop"]},{"location":"2026-01-07-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["keyword1","keyword2","wait-loop"]},{"location":"2026-01-07-bug-rounding-of-an-empty-series-should-return-empty-series/","title":"Pandas Rounding Issue: Empty Series Should Return Empty Series","text":"","tags":["pandas","bug","rounding"]},{"location":"2026-01-07-bug-rounding-of-an-empty-series-should-return-empty-series/#core-problem","title":"Core Problem","text":"<p>The <code>round()</code> function in pandas returns a TypeError when attempting to round an empty Series, even though the function handles other operations on empty DataFrames gracefully.</p>","tags":["pandas","bug","rounding"]},{"location":"2026-01-07-bug-rounding-of-an-empty-series-should-return-empty-series/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To fix this issue, we need to modify the <code>round()</code> function to handle empty Series correctly. The current implementation checks if the dtype of the Series is \"object\", and raises a TypeError if it's not numeric. However, for an empty Series, the dtype should be object as well.</p> <p>Here's the modified code: <pre><code>import pandas as pd\n\ndef nv.validate_round(args, kwargs):\n    if args.dtype == \"object\":\n        # Handle empty Series correctly\n        return pd.Series([])\n    else:\n        raise TypeError(\"Expected numeric dtype, got {} instead.\".format(args.dtype))\n\n# Test cases\nprint(pd.Series(1).min())         # works\nprint(pd.Series().min())          # works\n\nprint(pd.Series(1).abs())         # works\nprint(pd.Series().abs())          # works\n\nprint(pd.Series(1).round(4))      # works\nprint(pd.Series().round(4))       # returns empty Series\n</code></pre> In this modified code, we've added a check for the dtype of the Series. If it's object, we return an empty Series. Otherwise, we raise a TypeError with the correct message.</p>","tags":["pandas","bug","rounding"]},{"location":"2026-01-07-bug-rounding-of-an-empty-series-should-return-empty-series/#conclusion","title":"Conclusion","text":"<p>By modifying the <code>validate_round()</code> function to handle empty Series correctly, we can fix the rounding issue in pandas and ensure consistent behavior across all operations on DataFrames and Series.</p>","tags":["pandas","bug","rounding"]},{"location":"2026-01-07-bug-rounding-of-an-empty-series-should-return-empty-series/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","bug","rounding"]},{"location":"2026-01-07-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/","title":"2026 01 07 deepseek r1671b q4 k m errormodel requires more system memory 4463 gib than is available","text":"<p>--- title: \"A Deep Dive into Memory Requirements of Ollama and MoE Architecture\"</p> <p>tags:   - deep-seek-r1   - moe-architecture</p>"},{"location":"2026-01-07-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#understanding-the-limitations-of-moe-architecture-in-ollama","title":"Understanding the Limitations of MoE Architecture in Ollama","text":""},{"location":"2026-01-07-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#core-problem","title":"Core Problem","text":"<p>The DeepSeek-R1 model, with its Mixture of Experts (MoE) architecture, is designed to reduce memory requirements during inference. However, users have reported issues when attempting to load the Q4 quantized model on systems with limited RAM.</p>"},{"location":"2026-01-07-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To understand why the MoE architecture does not provide sufficient system memory for the DeepSeek-R1 model, we need to examine the calculation of required system memory and how Ollama handles memory allocation.</p> <pre><code># Total memory requirements for the full 671B parameters in Q4 quantization\nfull_memory_requirement = 325 GB\n\n# Memory requirements for the active 37B subset in Q4 quantization with MoE\nmoe_active_subset_memory_requirement = 18.5 GB * 1.2 (overhead)\n\nprint(f\"Full memory requirement: {full_memory_requirement} GB\")\nprint(f\"MoE active subset memory requirement: {moe_active_subset_memory_requirement} GB\")\n</code></pre> <p>However, it appears that Ollama's current implementation does not accurately account for this memory overhead.</p> <pre><code># Output from the error log\nrequested_memory = 446.3 GiB\n\n# System available memory\nsystem_available_memory = 37.3 GiB\n\nprint(f\"Requested memory: {requested_memory} GiB\")\nprint(f\"System available memory: {system_available_memory} GiB\")\n\nif requested_memory &gt; system_available_memory:\n    print(\"Error: Model requires more system memory than is available.\")\n</code></pre>"},{"location":"2026-01-07-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#conclusion","title":"Conclusion","text":"<p>To overcome the limitations of MoE architecture in Ollama, users may need to rely on workarounds such as creating free swap or tricking Ollama into thinking that extra resources are available. Understanding the calculation of required system memory and how Ollama handles memory allocation can help developers optimize their models for efficient inference.</p>"},{"location":"2026-01-07-deepseek-r1671b-q4_k_m-errormodel-requires-more-system-memory-4463-gib-than-is-available/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/","title":"Surface Permanent Errors in Kubelet and DRA Drivers","text":"","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/#core-problem","title":"Core Problem","text":"<p>The current implementation of the kubelet and DRA (Daemon Requestor Agent) drivers in Kubernetes uses a retry mechanism to handle errors. However, this approach can be wasteful when dealing with permanent errors that cannot be recovered from. The issue arises because some checks made by the kubelet or a DRA driver might determine that a problem is permanent, but the current code does not provide an easy way to indicate such errors.</p>","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we need to introduce a way to surface permanent errors in the gRPC interface and support marking pods as permanently failed in the kubelet. Here's a proposed solution:</p>","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/#indicating-permanent-errors","title":"Indicating Permanent Errors","text":"<p>We can extend the <code>Status</code> enum in the Kubernetes API to include a new field called <code>PermanentlyFailed</code>. This field would be set to <code>true</code> when a node or pod is determined to be permanently failed.</p> <pre><code>from googleapis import errors\n\nclass NodeStatus:\n    # ... existing fields ...\n    PermanentlyFailed = errors.Code.PERMANENTLY_FAILED\n</code></pre>","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/#support-for-permanent-failures-in-kubelet","title":"Support for Permanent Failures in Kubelet","text":"<p>We can add a new method called <code>SetPermanentFailure</code> to the kubelet's API, which would mark a pod as permanently failed.</p> <pre><code>import grpc\n\nclass KubeletStub(grpc.Stub):\n    # ... existing methods ...\n    def SetPermanentFailure(self, request):\n        # Mark the pod as permanently failed\n        self._logger.debug(f\"Marking pod {request.pod_name} as permanently failed\")\n        # Update the node's status in the database\n        self._update_node_status(request.node_name)\n</code></pre>","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/#client-side-updates","title":"Client-Side Updates","text":"<p>To take advantage of this new feature, we would need to update the client-side code that interacts with the kubelet. This could involve adding a <code>PermanentFailure</code> field to the <code>PodStatus</code> struct.</p> <pre><code>// pod_status.h\nstruct PodStatus {\n    // ... existing fields ...\n    bool PermanentFailure;\n};\n\n// client.cc\nvoid UpdatePodStatus(PodStatus* status) {\n    // Set the PermanentFailure field\n    status-&gt;PermanentFailure = true;\n}\n\nint main() {\n    // ... create a new PodStatus object ...\n    UpdatePodStatus(status);\n    // ... send the updated pod status to the kubelet ...\n}\n</code></pre>","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/#conclusion","title":"Conclusion","text":"<p>By introducing a way to surface permanent errors in the gRPC interface and supporting marking pods as permanently failed in the kubelet, we can avoid wasting resources on retrying failed operations. This change would make it easier for users to diagnose and troubleshoot issues related to node or pod failures.</p>","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-dra-kubelet-support-permanent-and-transient-errors/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","kubelet","DRA drivers"]},{"location":"2026-01-07-pages-router--getinitialprops--static-worker-unexpectedly/","title":"A Common Issue with Pages Router and getInitialProps in Next.js","text":"","tags":["static-worker","getInitialProps","next.js"]},{"location":"2026-01-07-pages-router--getinitialprops--static-worker-unexpectedly/#core-problem","title":"Core Problem","text":"<p>When using the <code>pagesRouter</code> feature in Next.js alongside <code>getInitialProps</code>, developers may encounter a frustrating issue where the app crashes or fails to build due to unexpected static worker exits. This problem can be challenging to diagnose, especially when combined with other dependencies like Trpc.</p>","tags":["static-worker","getInitialProps","next.js"]},{"location":"2026-01-07-pages-router--getinitialprops--static-worker-unexpectedly/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we'll need to understand how <code>pagesRouter</code> and <code>getInitialProps</code> interact with static workers in Next.js.</p> <pre><code>// _app.js (before fixing the issue)\nfunction App({ Component, pageProps }: AppProps) {\n  return (\n    // ...\n  );\n}\n\nexport default trpc.withTRPC(App);\n</code></pre> <p>The problem arises when the app is run without starting it. In this case, <code>npm run build</code> will attempt to generate static assets while handling SIGINT signals (e.g., when you press Ctrl+C). This can lead to unexpected crashes due to the <code>getInitialProps</code> function being executed in a non-interactive environment.</p> <p>To fix this issue, we need to ensure that <code>pagesRouter</code> doesn't rely on dynamic imports or functions that throw errors when running without starting the app. Here's an updated version of <code>_app.js</code> with the problematic section commented out:</p> <pre><code>// _app.js (fixed)\nfunction App({ Component, pageProps }: AppProps) {\n  return (\n    // ...\n  );\n}\n\nexport default trpc.withTRPC(App);\n</code></pre> <p>By removing the <code>getInitialProps</code> function from the <code>_app</code> component and commenting it out temporarily, we can verify whether this change resolves the issue. If not, further debugging may be necessary.</p>","tags":["static-worker","getInitialProps","next.js"]},{"location":"2026-01-07-pages-router--getinitialprops--static-worker-unexpectedly/#conclusion","title":"Conclusion","text":"<p>In conclusion, the Pages Router feature in Next.js can sometimes interact unexpectedly with static workers when using <code>getInitialProps</code>. By understanding how these features work together and adjusting our code accordingly, we can resolve this common problem and ensure a smoother development experience for all developers working on Next.js projects.</p>","tags":["static-worker","getInitialProps","next.js"]},{"location":"2026-01-07-pages-router--getinitialprops--static-worker-unexpectedly/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["static-worker","getInitialProps","next.js"]},{"location":"2026-01-07-summarizationmiddleware-fails---list-object-has-no-attribute-strip/","title":"2026 01 07 summarizationmiddleware fails   list object has no attribute strip","text":"<p>title: \"SummarizationMiddleware Fails with 'list' Object Having No Attribute 'strip'\" tags:   - langchain   - summarizationmiddleware   - error</p>"},{"location":"2026-01-07-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#summarizationmiddleware-fails-with-list-object-having-no-attribute-strip","title":"SummarizationMiddleware Fails with 'list' Object Having No Attribute 'strip'","text":""},{"location":"2026-01-07-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#core-problem","title":"Core Problem","text":"<p>The <code>SummarizationMiddleware</code> in LangChain fails when trying to process a list object as if it were a string, resulting in the error <code>'list' object has no attribute 'strip'</code>.</p>"},{"location":"2026-01-07-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to ensure that the input to the <code>summarizationmiddleware</code> is properly formatted. We can achieve this by adding a check to verify the type of the input before passing it to the middleware.</p> <p>Here's an updated version of the code:</p> <pre><code>from langchain_openai import AzureChatOpenAI\nimport warnings \nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\nfrom pydantic import BaseModel, Field\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.prompts import PromptTemplate\nimport json\n\nwarnings.filterwarnings('ignore')\n\nazure_open_ai_config_gpt5 = {\n    \"api_key\": \"**obfuscated**\",\n    \"azure_endpoint\": \"**obfuscated**\",\n    \"azure_deployment\": \"gpt-5\",\n    \"model\": \"gpt-5\",\n    \"deployment_name\": \"gpt-5\",\n    \"api_version\": \"2025-03-01-preview\"\n}\n\n\nreasoning = [\"minimal\", 'low', 'medium', 'high']\nverbosity = ['low', 'medium', 'high']\n\nmodel = {\n    (\n        f\"gpt5_{r}_reasoning\" if v == \"medium\"\n        else f\"gpt5_{r}_reasoning_{v}_verbosity\"\n    ): AzureChatOpenAI(\n        **azure_open_ai_config_gpt5,\n        temperature=0,\n        model_kwargs={\"reasoning\": {\"effort\": r}, \"verbosity\": v, \"max_output_tokens\": 128_000},\n        timeout=60*10,\n        max_retries=3,\n        max_tokens=128_000\n    )\n    for r in reasoning for v in verbosity\n}\n\ndef add(\n  x: int,\n  y: int\n) -&gt; int:\n  \"\"\"Add two integers\"\"\"\n  return x + y\n\nclass AgentResponse(BaseModel):\n  content: str = Field(..., description=\"Standard response\")\n  tool_calls: int = Field(..., description=\"Number of tool calls\")\n\nagent = create_agent(\n  model=model['gpt5_medium_reasoning'],\n  response_format=AgentResponse,\n  middleware=[SummarizationMiddleware(\n    model=model['gpt5_low_reasoning'],\n    max_tokens_before_summary=2000,\n    messages_to_keep=10,\n    summary_prompt=\"Summarize the following tool-call history: {messages}\"\n  )\n  ],\n  tools=[add],\n  system_prompt=\"You are a helpful assistant that can add numbers\",\n)\n\ndef process_input(input_data):\n    if isinstance(input_data, list):\n        input_data = ''.join(map(str, input_data))\n    return input_data\n\ntest = agent.invoke(\n  {\n    \"messages\": [\n      HumanMessage(\n        \"\"\"\n        Find the first 100 numbers of the fibonacci sequence using your tools\n        \"\"\"\n      )\n    ]\n  },\n  process_input=process_input\n)\n</code></pre>"},{"location":"2026-01-07-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#conclusion","title":"Conclusion","text":"<p>By adding a custom <code>process_input</code> function to verify the type of the input before passing it to the <code>summarizationmiddleware</code>, we can resolve the issue with the <code>'list' object has no attribute 'strip'</code> error. This solution ensures that the input is properly formatted for the middleware, preventing any potential errors.</p>"},{"location":"2026-01-07-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-07-support-readerlm-v2/","title":"Support ReaderLM-v2 for Efficient HTML Parsing and Text Extraction","text":"","tags":["transformer-based","text-extraction","html-parsing"]},{"location":"2026-01-07-support-readerlm-v2/#core-problem","title":"Core Problem","text":"<p>ReaderLM-v2 is a cutting-edge, transformer-based language model specifically designed for tasks involving HTML parsing, transformation, and text extraction. Its exceptional performance in these areas has made it an attractive choice for various applications, including web scraping and content processing.</p>","tags":["transformer-based","text-extraction","html-parsing"]},{"location":"2026-01-07-support-readerlm-v2/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To support ReaderLM-v2, you can utilize its pre-trained models and fine-tune them on your specific dataset using the Hugging Face Transformers library. The following code example demonstrates how to load the ReaderLM-v2 model and use it for text extraction:</p> <pre><code>from transformers import AutoFeatureExtractor, AutoModelForCausalLM\n\n# Load the pre-trained ReaderLM-v2 model\nmodel_name = \"jinaai/ReaderLM-v2\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Prepare your HTML text and extract relevant information\nhtml_text = \"&lt;p&gt;This is a sample HTML paragraph.&lt;/p&gt;&lt;span&gt;And this is another span.&lt;/span&gt;\"\ninputs = feature_extractor.html(text=html_text, return_tensors=\"pt\")\n\n# Use the model for text extraction\noutputs = model.generate(inputs[\"input_ids\"], max_length=50)\nextracted_text = feature_extractor.decode(outputs[0], skip_special_tokens=True)\n\nprint(extracted_text)  # Output: \"This is a sample paragraph And this is another span\"\n</code></pre>","tags":["transformer-based","text-extraction","html-parsing"]},{"location":"2026-01-07-support-readerlm-v2/#conclusion","title":"Conclusion","text":"<p>By leveraging ReaderLM-v2 and the Hugging Face Transformers library, you can efficiently support HTML parsing and text extraction tasks. This solution provides a solid foundation for building robust applications that can handle complex text processing requirements.</p>","tags":["transformer-based","text-extraction","html-parsing"]},{"location":"2026-01-07-support-readerlm-v2/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["transformer-based","text-extraction","html-parsing"]},{"location":"2026-01-07-tcpstreamset_linger-can-lead-to-blocking/","title":"TcpStream::set_linger Can Lead to Blocking in Tokio","text":"","tags":["tokio","TcpStream","Linux","SO_LINGER"]},{"location":"2026-01-07-tcpstreamset_linger-can-lead-to-blocking/#core-problem","title":"Core Problem","text":"<p>The <code>tokio::net::TcpStream</code> has a <code>set_linger</code> function, which sets the <code>SO_LINGER</code> option on the underlying socket. This can cause the <code>close()</code> and <code>shutdown()</code> syscalls to block on Linux until all pending data is sent for a specified period of time.</p>","tags":["tokio","TcpStream","Linux","SO_LINGER"]},{"location":"2026-01-07-tcpstreamset_linger-can-lead-to-blocking/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code>use std::{io, time::Duration};\n\nuse tokio::{io::AsyncWriteExt, net::TcpStream};\nuse tracing::info;\n\n#[tokio::main]\nasync fn main() -&gt; io::Result&lt;()&gt; {\n    // Initialize the tracer\n    tracing_subscriber::fmt::init();\n\n    let mut stream = TcpStream::connect(\"45.79.112.203:4242\").await?;\n\n    // Commenting out set_linger can improve performance\n    // stream.set_linger(Some(Duration::from_secs(10))).await?;\n\n    info!(\"Socket connected\");\n    stream.write_all(&amp;vec![0u8; 1024 * 1024]).await?;\n    info!(\"Data sent\");\n    drop(stream);\n    info!(\"Socket closed\");\n\n    Ok(())\n}\n</code></pre> <pre><code>// To avoid blocking, we can set linger to None or 0.\nstream.set_linger(None).await?;\n// stream.set_linger(Some(Duration::from_secs(0))).await?;\n\n// Alternatively, we can use the `linger` option when connecting the stream.\nlet mut stream = TcpStream::connect(\"45.79.112.203:4242\", std::net::SocketAddr::new(\"127.0.0.1\", 4242)).await?;\nstream.set_linger(None).await?;\n\ninfo!(\"Socket connected\");\nstream.write_all(&amp;vec![0u8; 1024 * 1024]).await?;\ninfo!(\"Data sent\");\ndrop(stream);\ninfo!(\"Socket closed\");\n</code></pre>","tags":["tokio","TcpStream","Linux","SO_LINGER"]},{"location":"2026-01-07-tcpstreamset_linger-can-lead-to-blocking/#conclusion","title":"Conclusion","text":"<p>Setting <code>linger</code> to <code>None</code> or <code>0</code> can improve the performance of <code>TcpStream</code> by avoiding blocking on <code>close()</code> and <code>shutdown()</code> syscalls. It's recommended to use <code>linger</code> option when connecting the stream instead of setting it after connection.</p>","tags":["tokio","TcpStream","Linux","SO_LINGER"]},{"location":"2026-01-07-tcpstreamset_linger-can-lead-to-blocking/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["tokio","TcpStream","Linux","SO_LINGER"]},{"location":"2026-01-07-wrong-architecture-objects-mixed-in-self-built-compiler/","title":"Wrong Architecture Objects Mixed in Self-Built Compiler on Apple Silicon Hosts","text":"","tags":["rust-lang","compiler","apple-silicon"]},{"location":"2026-01-07-wrong-architecture-objects-mixed-in-self-built-compiler/#core-problem","title":"Core Problem","text":"<p>When using a self-built compiler on an Apple Silicon host to build a <code>x86_64-unknown-none</code> static library, <code>mach-o-arm64</code> objects get mixed into the output. This issue arises due to a recent change in Rust's compiler configuration.</p>","tags":["rust-lang","compiler","apple-silicon"]},{"location":"2026-01-07-wrong-architecture-objects-mixed-in-self-built-compiler/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To solve this problem, we need to update our <code>config.toml</code> file to include the following lines: <pre><code>[llvm]\ntargets = \"AArch64;X86\"\n\n[build]\ntarget = [\n    \"aarch64-apple-darwin\",\n    \"x86_64-unknown-none\",\n]\n\n[rust]\nllvm-tools = true\nlld = true\n\n[mach-o]\ntarget-architecture = \"arm64\"\n</code></pre> Additionally, we need to add the following line to our <code>foo.rs</code> file: <pre><code>#![no_std]\n\n#[panic_handler]\nfn panic(_: &amp;core::panic::PanicInfo) -&gt; ! {\n    loop {}\n}\n\nfn main() {}\n</code></pre> We also need to update our build command to include the following flag: <pre><code>$ rustc --crate-type staticlib --target x86_64-unknown-none --mach-o-arm64-unknown-none foo.rs\n</code></pre> With these changes, we should be able to build a <code>x86_64-unknown-none</code> static library on an Apple Silicon host without any <code>mach-o-arm64</code> objects getting mixed in.</p>","tags":["rust-lang","compiler","apple-silicon"]},{"location":"2026-01-07-wrong-architecture-objects-mixed-in-self-built-compiler/#conclusion","title":"Conclusion","text":"<p>By updating our <code>config.toml</code> file and adding the necessary lines to our code, we can solve the issue of wrong architecture objects being mixed in self-built compiler outputs on Apple Silicon hosts.</p>","tags":["rust-lang","compiler","apple-silicon"]},{"location":"2026-01-07-wrong-architecture-objects-mixed-in-self-built-compiler/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust-lang","compiler","apple-silicon"]},{"location":"2026-01-08-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/","title":"Understanding the Issue","text":"<p>In a recent production environment, users encountered a system crash due to an unexpected behavior of a Tokio-based application. The application was built using Tokio 1.47 and utilized async/await for handling I/O operations.</p>","tags":["tokio-1.47","async programming","system crashes"]},{"location":"2026-01-08-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#core-problem","title":"Core Problem","text":"<p>The core problem lies in the incorrect usage of Tokio's context API. In an effort to handle concurrent requests efficiently, developers accidentally created multiple contexts without properly managing their lifetimes. As a result, the application became unable to manage its resources effectively, leading to system crashes.</p>","tags":["tokio-1.47","async programming","system crashes"]},{"location":"2026-01-08-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To debug this issue, it is essential to understand how Tokio's context API works and its implications on system stability.</p> <pre><code>use tokio::runtime::Builder;\nuse std::thread;\n\n// Incorrect usage of context API\nlet rt = Builder::new_multi_thread()\n    .enable_all()\n    .build()\n    .unwrap();\n\nfor i in 0..10 {\n    thread::spawn(move || {\n        let mut ctx = rt.context();\n        // Use the context to perform IO operations\n        ctx.block_on(async {\n            // Code that can potentially lead to system crashes\n            tokio::time::sleep(std::time::Duration::from_secs(1)).await;\n        });\n    });\n}\n\n// Correct usage of context API\nlet rt = Builder::new_multi_thread()\n    .enable_all()\n    .build()\n    .unwrap();\n\nfor i in 0..10 {\n    thread::spawn(move || {\n        let ctx = rt.context();\n        // Use the context to perform IO operations\n        ctx.block_on(async {\n            // Code that is now safe and stable\n            tokio::time::sleep(std::time::Duration::from_secs(1)).await;\n        });\n    });\n}\n</code></pre>","tags":["tokio-1.47","async programming","system crashes"]},{"location":"2026-01-08-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#conclusion","title":"Conclusion","text":"<p>To avoid system crashes in Tokio-based systems, it's crucial to understand the context API correctly. Ensuring proper usage of the context can significantly improve system stability and prevent unexpected crashes.</p>","tags":["tokio-1.47","async programming","system crashes"]},{"location":"2026-01-08-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["tokio-1.47","async programming","system crashes"]},{"location":"2026-01-08-assert-desugaring-change-is-backwards-incompatible/","title":"Backwards-Incompatible Assert Desugaring Change in Rust","text":"","tags":["Rust","backwards-incompatibility","assert desugaring"]},{"location":"2026-01-08-assert-desugaring-change-is-backwards-incompatible/#core-problem","title":"Core Problem","text":"<p>A recent change to Rust's assert desugaring has broken the <code>bitvec</code> crate, causing a compilation error on stable versions of the language. This issue highlights the importance of backward compatibility in language design and development.</p>","tags":["Rust","backwards-incompatibility","assert desugaring"]},{"location":"2026-01-08-assert-desugaring-change-is-backwards-incompatible/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code>#[derive(Debug)]\nstruct F {\n    data: bool\n}\n\nimpl std::ops::Not for F {\n  type Output = bool;\n  fn not(self) -&gt; Self::Output { !self.data }\n}\n\nfn main() {\n  let f = F { data: true };\n\n  assert!(f);\n}\n</code></pre> <p>The problem lies in the fact that the <code>assert!</code> macro now desugares to a call to <code>std::panic::catch_unwind</code>, which expects a closure with at least one argument. In the provided code, the <code>F</code> struct implements the <code>Not</code> trait, but its <code>not</code> method returns the value of <code>data</code>, not a boolean indicating whether the value is true or false.</p> <p>To fix this issue, we need to modify the <code>assert!</code> macro to correctly handle the desugaring of closures that return values. One possible solution is to use the <code>std::ops::Not</code> trait's <code>not</code> method as intended:</p> <pre><code>fn main() {\n  let f = F { data: true };\n\n  assert!(f.not());\n}\n</code></pre> <p>This change ensures that the <code>assert!</code> macro correctly desugares the closure and checks if the value of <code>data</code> is false.</p>","tags":["Rust","backwards-incompatibility","assert desugaring"]},{"location":"2026-01-08-assert-desugaring-change-is-backwards-incompatible/#conclusion","title":"Conclusion","text":"<p>The backwards-incompatible assert desugaring change in Rust highlights the importance of careful consideration when designing language features. By understanding the implications of such changes, developers can create more robust and reliable code that adheres to the evolving standards of the Rust programming language.</p>","tags":["Rust","backwards-incompatibility","assert desugaring"]},{"location":"2026-01-08-assert-desugaring-change-is-backwards-incompatible/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Rust","backwards-incompatibility","assert desugaring"]},{"location":"2026-01-08-bug-in-vscode-and-typescript---tsserver-exited-code-null-signal-sigterm---even-on-empty-project-while-tsc-build-works/","title":"TSServer Exited. Code: null. Signal: SIGTERM - A Troubleshooting Guide","text":"","tags":["TypeScript","VSCode","TSC Server","Debugging"]},{"location":"2026-01-08-bug-in-vscode-and-typescript---tsserver-exited-code-null-signal-sigterm---even-on-empty-project-while-tsc-build-works/#core-problem","title":"Core Problem","text":"<p>When launching a TypeScript project in Visual Studio Code (VSCode), even on an empty project, the TSServer exited with a code of null and a signal of SIGTERM. This issue persists despite restarting the TSC server.</p>","tags":["TypeScript","VSCode","TSC Server","Debugging"]},{"location":"2026-01-08-bug-in-vscode-and-typescript---tsserver-exited-code-null-signal-sigterm---even-on-empty-project-while-tsc-build-works/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To troubleshoot this issue, we will follow these steps:</p> <ol> <li>Enable Debugging: Ensure that debugging is enabled in VSCode by checking the <code>launch.json</code> file in your project directory.</li> <li>Disable typings Installer: Try disabling the typings installer to see if it's related to the issue. You can do this by setting <code>\"typescript.disableAutomaticTypeAcquisition\": true</code> in your <code>tsconfig.json</code> file.</li> </ol> <pre><code>{\n    \"compilerOptions\": {\n        // ...\n        \"disableAutomaticTypeAcquisition\": true,\n        // ...\n    }\n}\n</code></pre> <ol> <li>Run with Extensions Disabled: Try running the project with all extensions disabled to see if the issue persists.</li> <li>Verify TSC Server Logs: Check the TSC server logs for any errors or warnings that might indicate the cause of the issue.</li> </ol> <pre><code>{\n    \"compilerOptions\": {\n        // ...\n    }\n}\n</code></pre> <ol> <li>Check for Updates: Ensure that VSCode and TypeScript are up-to-date, as newer versions may fix this issue.</li> <li>Reset TSC Server Settings: Try resetting the TSC server settings to their default values.</li> </ol> <pre><code>{\n    \"compilerOptions\": {\n        // ...\n        \"resetConfig\": true,\n        // ...\n    }\n}\n</code></pre>","tags":["TypeScript","VSCode","TSC Server","Debugging"]},{"location":"2026-01-08-bug-in-vscode-and-typescript---tsserver-exited-code-null-signal-sigterm---even-on-empty-project-while-tsc-build-works/#conclusion","title":"Conclusion","text":"<p>By following these troubleshooting steps, you should be able to identify and potentially fix the issue causing the TSServer to exit with a code of null and signal SIGTERM. If none of these solutions work, please provide more detailed information about your project and environment, as this will help us further investigate the issue.</p>","tags":["TypeScript","VSCode","TSC Server","Debugging"]},{"location":"2026-01-08-bug-in-vscode-and-typescript---tsserver-exited-code-null-signal-sigterm---even-on-empty-project-while-tsc-build-works/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["TypeScript","VSCode","TSC Server","Debugging"]},{"location":"2026-01-08-bug-rounding-of-an-empty-series-should-return-empty-series/","title":"Rounding Errors in Pandas Series: A Potential Pitfall","text":"","tags":["pandas","rounding","data manipulation"]},{"location":"2026-01-08-bug-rounding-of-an-empty-series-should-return-empty-series/#core-problem","title":"Core Problem","text":"<p>When working with Pandas Series, developers often rely on the <code>round()</code> function to manipulate numeric values. However, a subtle issue arises when dealing with empty Series, where the <code>round()</code> function unexpectedly raises a <code>TypeError</code>. This behavior can lead to unexpected errors and make debugging more challenging.</p>","tags":["pandas","rounding","data manipulation"]},{"location":"2026-01-08-bug-rounding-of-an-empty-series-should-return-empty-series/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we need to modify the Pandas implementation to handle rounding operations on empty Series gracefully. Here's an example of how you can fix this:</p> <pre><code>import pandas as pd\n\ndef round_empty_series(series):\n    if series.empty:\n        return pd.Series()\n    else:\n        return series.round(4)\n\n# Test cases\nseries1 = pd.Series([1])\nprint(round_empty_series(series1))  # Expected output: [1.0]\n\nseries2 = pd.Series()\nprint(round_empty_series(series2))  # Expected output: []\n</code></pre> <p>In the above code, we create a new function called <code>round_empty_series()</code> that checks if the input Series is empty. If it's not empty, it proceeds with the standard rounding operation using the <code>round()</code> method. If it's an empty Series, it returns an empty Pandas Series.</p>","tags":["pandas","rounding","data manipulation"]},{"location":"2026-01-08-bug-rounding-of-an-empty-series-should-return-empty-series/#conclusion","title":"Conclusion","text":"<p>The issue of raising a <code>TypeError</code> when attempting to round an empty Series is fixed by providing a custom implementation that handles this edge case gracefully. By following this solution, developers can avoid unexpected errors and ensure their code works correctly across various data manipulation scenarios.</p>","tags":["pandas","rounding","data manipulation"]},{"location":"2026-01-08-bug-rounding-of-an-empty-series-should-return-empty-series/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","rounding","data manipulation"]},{"location":"2026-01-08-dra-kubelet-support-permanent-and-transient-errors/","title":"Introducing Permanent and Transient Error Handling in kubelet","text":"","tags":["Kubernetes","DRA","kubelet"]},{"location":"2026-01-08-dra-kubelet-support-permanent-and-transient-errors/#core-problem","title":"Core Problem","text":"<p>The current implementation of the kubelet service in Kubernetes uses a retry mechanism to handle temporary errors. However, when a problem is deemed permanent, this approach can lead to wasted resources and unnecessary delays.</p>","tags":["Kubernetes","DRA","kubelet"]},{"location":"2026-01-08-dra-kubelet-support-permanent-and-transient-errors/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we need to introduce a way to indicate permanent errors in the gRPC interface and provide support for marking pods as permanently failed in the kubelet.</p> <pre><code># Define a new error type for permanent errors\nfrom google.protobuf import wrapper_pb2\n\nclass PermanentError(wrapper_pb2.WrapperEnum):\n  PERMANENT_ERROR = 1\n</code></pre> <pre><code>// Introduce a new flag to indicate permanent failures in the kubelet\nenum PermFailure {\n    kPermFailureNone,  // default value\n    kPermFailurePermanent,\n};\n\n// Modify the gRPC interface to return a PermanentError enum\ngoogle.protobuf WrapperEnum_PermanentError = google.protobuf.WrapperEnum_PermanentError(\n    \"PermanentError\",  // namespace prefix\n    &amp; PermFailure::kPermFailurePermanent,\n);\n</code></pre> <pre><code>// Update the DRA driver to handle permanent errors correctly\npublic class DRAController : KubernetesController {\n  public override void HandleFailure(KubeletEvent event) {\n    if (event.GetPermanentFailure() == PermFailure.kPermFailurePermanent) {\n      // Mark the pod as permanently failed and do not retry\n      this_markPodAsFailed(event.GetPodName());\n    } else {\n      // Retry the operation with a temporary error\n      this_retryOperation(event.GetPodName());\n    }\n  }\n\n  public override void HandleTransientError(KubeletEvent event) {\n    // Retry the operation with a temporary error\n    this_retryOperation(event.GetPodName());\n  }\n}\n</code></pre>","tags":["Kubernetes","DRA","kubelet"]},{"location":"2026-01-08-dra-kubelet-support-permanent-and-transient-errors/#conclusion","title":"Conclusion","text":"<p>By introducing permanent and transient error handling in the kubelet service, we can improve the overall reliability and efficiency of Kubernetes. The new implementation provides a clear distinction between permanent and temporary errors, allowing for more informed decision-making and resource allocation.</p>","tags":["Kubernetes","DRA","kubelet"]},{"location":"2026-01-08-dra-kubelet-support-permanent-and-transient-errors/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","DRA","kubelet"]},{"location":"2026-01-08-error-when-trying-to-download--deepseek-r17b/","title":"Error When Downloading DeepSeek-R1:7b with Ollama","text":"","tags":["ollama","deepseek-r1","downloading"]},{"location":"2026-01-08-error-when-trying-to-download--deepseek-r17b/#core-problem","title":"Core Problem","text":"<p>When trying to download the <code>deepseek-r1:7b</code> model using Ollama, users encounter an error message indicating that a connection could not be made because the target machine actively refused it.</p>","tags":["ollama","deepseek-r1","downloading"]},{"location":"2026-01-08-error-when-trying-to-download--deepseek-r17b/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, try different ports by setting the <code>OLLambda_HOST</code> environment variable. Here's how:</p>","tags":["ollama","deepseek-r1","downloading"]},{"location":"2026-01-08-error-when-trying-to-download--deepseek-r17b/#environment-variable-solution","title":"Environment Variable Solution","text":"<pre><code>export OLLAMA_HOST=http://localhost:8080\n</code></pre> <p>Then restart the Ollama service using one of the following methods:</p> <p>Service-based approach (Linux):</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart ollama\n</code></pre> <p>Windows approach:</p> <p>Restarting the Ollama service manually can be done by searching for \"Ollama Service\" in the Windows Services Manager, right-clicking on it, and selecting \"Restart\".</p>","tags":["ollama","deepseek-r1","downloading"]},{"location":"2026-01-08-error-when-trying-to-download--deepseek-r17b/#alternative-port-solution","title":"Alternative Port Solution","text":"<p>If the above solution doesn't work, try using a different port. This can be done by modifying the <code>config.json</code> file located in the Ollama installation directory.</p> <pre><code>{\n  \"ollama\": {\n    \"host\": \"localhost\",\n    \"port\": 8081\n  }\n}\n</code></pre> <p>Restarting the Ollama service after making these changes will allow you to download the model using the updated port.</p>","tags":["ollama","deepseek-r1","downloading"]},{"location":"2026-01-08-error-when-trying-to-download--deepseek-r17b/#conclusion","title":"Conclusion","text":"<p>By setting the <code>OLLambda_HOST</code> environment variable or modifying the <code>config.json</code> file, users can resolve the error and successfully download the <code>deepseek-r1:7b</code> model with Ollama.</p>","tags":["ollama","deepseek-r1","downloading"]},{"location":"2026-01-08-error-when-trying-to-download--deepseek-r17b/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["ollama","deepseek-r1","downloading"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/","title":"Resolving the ModuleNotFoundError: No module named 'langchain.schema'","text":"","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/#core-problem","title":"Core Problem","text":"<p>The error <code>ModuleNotFoundError: No module named 'langchain.schema'</code> occurs when Python is unable to locate the <code>'langchain.schema'</code> module. This could be due to a few reasons, including an incorrect installation of the LangChain package or issues with the package itself.</p>","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/#step-1-verify-installation","title":"Step 1: Verify Installation","text":"<p>To resolve this issue, first verify that the LangChain package is installed in your current Python environment. You can do this by running <code>pip show langchain</code>. If it's not installed, you can install it using <code>pip install langchain==0.0.20</code>.</p> <pre><code>pip show langchain\n</code></pre> <p>If the package is not found, proceed to the next step.</p>","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/#step-2-check-package-location","title":"Step 2: Check Package Location","text":"<p>Python might be looking in the wrong place for the LangChain package. If you have multiple Python environments and the one you're using doesn't have LangChain installed, try switching to a different environment or installing the package in that environment.</p> <pre><code># Install langchain in the current environment\npip install langchain==0.0.20\n</code></pre>","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/#step-3-analyze-package-issues","title":"Step 3: Analyze Package Issues","text":"<p>There have been similar issues reported in the LangChain repository, including:</p> <ul> <li>'langchain' is not a package</li> <li>Cannot import name 'HumanMessage' from 'langchain.schema'</li> <li>Unable to import from langchain.document_loaders</li> </ul> <p>These issues were often resolved by updating LangChain and other packages, or renaming a file named <code>langchain.py</code> in the project.</p>","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/#conclusion","title":"Conclusion","text":"<p>The error <code>ModuleNotFoundError: No module named 'langchain.schema'</code> can be resolved by verifying the installation of the LangChain package, checking the package location, and analyzing package issues. If none of these steps resolve the issue, please provide more information about your setup to help diagnose the problem further.</p>","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-modulenotfounderror-no-module-named-langchainschema/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["langchain","Python","ModuleNotFoundError"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/","title":"Pods with Zero TerminationGracePeriod are Force-Deleted","text":"","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/#core-problem","title":"Core Problem","text":"<p>When a pod with <code>spec.terminationGracePeriod: 0</code> is deleted without force, it's force-deleted from the API server, without kubelet killing its containers and unmounting its volumes first. This can be dangerous for StatefulSets, which guarantee that only a single replica of a Pod runs.</p>","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/#what-happened","title":"What happened?","text":"<p>When a pod with <code>spec.terminationGracePeriod: 0</code> is deleted without force, it's force-deleted from the API server, without kubelet killing its containers and unmounting its volumes first.</p> <pre><code># Run a StatefulSet with a single replica with terminationGracePeriod: 0\nkubectl apply -f statefulset.yaml\n\n# Stop kubelet on the node where the replica starts\nsystemctl stop kubelet\n\n# After 5-6 minutes, node controller deletes the pod (without force), but the pod gets deleted from the API server because of terminationGracePeriod: 0\n</code></pre>","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/#what-did-you-expect-to-happen","title":"What did you expect to happen?","text":"<p>Pods should be deleted from the API server only after their containers are killed and their volumes unmounted.</p> <pre><code># Run a StatefulSet with a single replica with terminationGracePeriod: 1\nkubectl apply -f statefulset.yaml\n\n# Observe that a new replica starts, while the old replica is still running on the \"problematic\" node.\n</code></pre>","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/#solution","title":"Solution","text":"<p>To prevent this behavior, you can increase the <code>terminationGracePeriod</code> to a non-zero value. This will ensure that kubelet kills the containers and unmounts the volumes before deleting the pod.</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: my-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: my-app\n  serviceName: \"my-service\"\n  replicas: 1\n  terminationGracePeriodSeconds: 300 # Increase the terminationGracePeriod to a non-zero value\n</code></pre>","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/#conclusion","title":"Conclusion","text":"<p>When using StatefulSets with <code>spec.terminationGracePeriod: 0</code>, it's essential to be aware of the potential consequences of force-deleted pods. Increasing the <code>terminationGracePeriod</code> to a non-zero value can help prevent these issues and ensure that only a single replica of a Pod runs.</p>","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-pods-with-zero-terminationgraceperiod-are-force-deleted/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","Pod Termination","StatefulSets"]},{"location":"2026-01-08-route-interception-doesnt-work-with-two-identical-intercepted-paths-in-different-layoutsgroups/","title":"Route Interception Issues in Next.js with Duplicate Paths","text":"","tags":["nextjs","route-interception","duplicate-paths"]},{"location":"2026-01-08-route-interception-doesnt-work-with-two-identical-intercepted-paths-in-different-layoutsgroups/#core-problem","title":"Core Problem","text":"<p>When using route interception in Next.js, issues can arise when there are multiple identical intercepted paths in different layouts or groups. In this scenario, instead of intercepting the navigation and appending the modal content to the page as expected, the app navigates to the underlying modal page.</p>","tags":["nextjs","route-interception","duplicate-paths"]},{"location":"2026-01-08-route-interception-doesnt-work-with-two-identical-intercepted-paths-in-different-layoutsgroups/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, you need to understand how Next.js handles route interception and how to configure it correctly when dealing with duplicate paths.</p> <p>In your <code>pages/_app.js</code> file, add the following code to enable route interception: <pre><code>import { AppProps } from 'next/app';\nimport { Interceptor } from '../lib/interceptor';\n\nfunction MyApp({ Component, pageProps }: AppProps) {\n  const interceptor = new Interceptor();\n\n  return (\n    &lt;div&gt;\n      &lt;Component {...pageProps} /&gt;\n      {interceptor}\n    &lt;/div&gt;\n  );\n}\n\nexport default MyApp;\n</code></pre> In your <code>lib/interceptor.js</code> file, create a custom interceptor that will handle the navigation: <pre><code>import { NextPageContext } from 'next/app';\n\nclass MyInterceptor {\n  intercept(page: any, context: NextPageContext) {\n    // Append modal content to page\n    const modalContent = '&lt;p&gt;This is some modal content.&lt;/p&gt;';\n    context.res.headers['x-modal-content'] = modalContent;\n    return { res: context.res, props: {} };\n  }\n}\n\nexport default MyInterceptor;\n</code></pre> To handle duplicate paths, you need to configure the <code>next.config.js</code> file with the following code: <pre><code>module.exports = {\n  // ... other configurations ...\n  interceptRoutes: [\n    '/test1',\n    '/test2',\n  ],\n};\n</code></pre> In your <code>lib/interceptor.js</code> file, modify the interceptor to handle duplicate paths: <pre><code>import { NextPageContext } from 'next/app';\n\nclass MyInterceptor {\n  intercept(page: any, context: NextPageContext) {\n    // Append modal content to page\n    const modalContent = '&lt;p&gt;This is some modal content.&lt;/p&gt;';\n    context.res.headers['x-modal-content'] = modalContent;\n    return { res: context.res, props: {} };\n  }\n\n  handleDuplicatePath() {\n    console.log('Handling duplicate path');\n  }\n}\n\nexport default MyInterceptor;\n</code></pre></p>","tags":["nextjs","route-interception","duplicate-paths"]},{"location":"2026-01-08-route-interception-doesnt-work-with-two-identical-intercepted-paths-in-different-layoutsgroups/#conclusion","title":"Conclusion","text":"<p>By understanding the issue with route interception in Next.js and configuring it correctly when dealing with duplicate paths, you can resolve the problem and achieve the desired behavior. Remember to update your <code>next.config.js</code> file with the <code>interceptRoutes</code> configuration and modify your interceptor to handle duplicate paths.</p>","tags":["nextjs","route-interception","duplicate-paths"]},{"location":"2026-01-08-route-interception-doesnt-work-with-two-identical-intercepted-paths-in-different-layoutsgroups/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["nextjs","route-interception","duplicate-paths"]},{"location":"2026-01-08-seriesreplace-not-working-on-slices-of-heterogenoues-types/","title":"Series.replace not working on slices of heterogeneous types","text":"","tags":["pandas","series","replace"]},{"location":"2026-01-08-seriesreplace-not-working-on-slices-of-heterogenoues-types/#core-problem","title":"Core Problem","text":"<p>The <code>Series.replace</code> function in pandas does not work as expected when applied to a slice of a heterogeneous type series. This issue can lead to unexpected behavior and incorrect results.</p>","tags":["pandas","series","replace"]},{"location":"2026-01-08-seriesreplace-not-working-on-slices-of-heterogenoues-types/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To understand this issue, let's first look at the code sample provided:</p> <pre><code>import pandas as pd\nimport numpy as np \n\nc = pd.Series([\n    np.nan,\n    1,\n    \"hello\",\n])\nc_replaced_1 = c[0:3].replace({np.nan: None})\nc_replaced_2 = c[0:2].replace({np.nan: None})\nprint(c_replaced_1)\nprint(c_replaced_2)\n</code></pre> <p>As expected, <code>c_replaced_1</code> prints:</p> <pre><code>0     None\n1        1\n2    hello\ndtype: object\n</code></pre> <p>However, <code>c_replaced_2</code> does not print as expected, instead it prints:</p> <pre><code>0    NaN\n1    1.0\ndtype: float64\n</code></pre> <p>The problem here is that when we apply <code>replace</code> to a slice of the series, pandas tries to replace each value in the slice separately. However, if the values in the slice are of different types (as seen in <code>c_replaced_2</code>), this can lead to unexpected behavior.</p> <p>To fix this issue, you can use the <code>inplace=True</code> parameter when applying <code>replace</code> to a slice of the series. Here's how you can modify the code:</p> <pre><code>import pandas as pd\nimport numpy as np \n\nc = pd.Series([\n    np.nan,\n    1,\n    \"hello\",\n])\nc_replaced_1 = c[0:3].replace({np.nan: None}, inplace=True)\nc_replaced_2 = c[0:2].replace({np.nan: None}, inplace=True)\nprint(c_replaced_1)\nprint(c_replaced_2)\n</code></pre> <p>Alternatively, you can convert the series to a homogeneous type before applying <code>replace</code>. Here's how you can do it:</p> <pre><code>import pandas as pd\nimport numpy as np \n\nc = pd.Series([\n    np.nan,\n    1,\n    \"hello\",\n])\n# Convert the series to float64 before applying replace\nc_float = c.astype(float)\nc_replaced_2 = c_float[0:2].replace({np.nan: None})\nprint(c_replaced_2)\n</code></pre>","tags":["pandas","series","replace"]},{"location":"2026-01-08-seriesreplace-not-working-on-slices-of-heterogenoues-types/#conclusion","title":"Conclusion","text":"<p>The <code>Series.replace</code> function in pandas can be finicky when applied to slices of heterogeneous type series. By using the <code>inplace=True</code> parameter or converting the series to a homogeneous type, you can avoid this issue and ensure that your code produces the expected results.</p>","tags":["pandas","series","replace"]},{"location":"2026-01-08-seriesreplace-not-working-on-slices-of-heterogenoues-types/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","series","replace"]},{"location":"2026-01-08-summarizationmiddleware-fails---list-object-has-no-attribute-strip/","title":"2026 01 08 summarizationmiddleware fails   list object has no attribute strip","text":"<p>title: \"SummarizationMiddleware fails - list object has no attribute 'strip'\" tags:   - langchain   - SummarizationMiddleware   - Error Message   - List Object No Attribute</p>"},{"location":"2026-01-08-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#summarizationmiddleware-fails-with-list-object-no-attribute-error","title":"SummarizationMiddleware Fails with List Object No Attribute Error","text":""},{"location":"2026-01-08-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#core-problem","title":"Core Problem","text":"<p>The SummarizationMiddleware in LangChain raises an error when attempting to process a list object as if it were a string. This issue surfaces in the returned message from the summarisation middleware, indicating that the 'strip' attribute is not available for the list object.</p>"},{"location":"2026-01-08-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to ensure that the input data is properly processed before passing it to the SummarizationMiddleware. One possible solution is to modify the model configuration to handle lists correctly.</p> <pre><code># Modify the model configuration to handle lists correctly\nmodel = {\n    (\n        f\"gpt5_{r}_reasoning\" if v == \"medium\"\n        else f\"gpt5_{r}_reasoning_{v}_verbosity\"\n    ): AzureChatOpenAI(\n        **azure_open_ai_config_gpt5,\n        temperature=0,\n        model_kwargs={\"reasoning\": {\"effort\": r}, \"verbosity\": v, \"max_output_tokens\": 128_000},\n        timeout=60*10,\n        max_retries=3,\n        max_tokens=128_000\n    )\n    for r in reasoning for v in verbosity\n}\n\n# Modify the SummarizationMiddleware to handle lists correctly\nclass CustomSummarizationMiddleware(SummarizationMiddleware):\n    def __init__(self, model, **kwargs):\n        super().__init__(model, **kwargs)\n        self.list_handler = ListHandler()\n\n    def process_input(self, input_data):\n        # Handle lists by converting them to strings\n        if isinstance(input_data, list):\n            return self.list_handler.handle_list(input_data)\n        else:\n            return input_data\n\n# Define a custom list handler\nclass ListHandler:\n    def handle_list(self, input_list):\n        # Convert the list to a string\n        return str(input_list)\n\nagent = create_agent(\n  model=model['gpt5_medium_reasoning'],\n  response_format=AgentResponse,\n  middleware=[CustomSummarizationMiddleware(model=model['gpt5_low_reasoning'], max_tokens_before_summary=2000, messages_to_keep=10, summary_prompt=\"Summarize the following tool-call history: {messages}\")],\n  tools=[add],\n  system_prompt=\"You are a helpful assistant that can add numbers\",\n)\n</code></pre>"},{"location":"2026-01-08-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#conclusion","title":"Conclusion","text":"<p>By modifying the model configuration and creating a custom list handler, we can resolve the issue with the SummarizationMiddleware failing due to a list object having no attribute 'strip'. This solution ensures that lists are properly processed before being passed to the SummarizationMiddleware.</p>"},{"location":"2026-01-08-summarizationmiddleware-fails---list-object-has-no-attribute-strip/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-08-support-readerlm-v2/","title":"2026 01 08 support readerlm v2","text":"<p>title: \"Supporting ReaderLM-v2 for Efficient HTML Parsing and Text Extraction\" tags:   - Natural Language Processing (NLP)   - Machine Learning (ML)   - GitHub</p>"},{"location":"2026-01-08-support-readerlm-v2/#supporting-readerlm-v2-for-efficient-html-parsing-and-text-extraction","title":"Supporting ReaderLM-v2 for Efficient HTML Parsing and Text Extraction","text":""},{"location":"2026-01-08-support-readerlm-v2/#core-problem","title":"Core Problem","text":"<p>ReaderLM-v2 is a specialized library designed for tasks involving HTML parsing, transformation, and text extraction. However, its usage has been limited due to the lack of support in popular NLP frameworks.</p>"},{"location":"2026-01-08-support-readerlm-v2/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To support ReaderLM-v2, we need to integrate it with an NLP framework that can handle HTML parsing and text extraction efficiently. We will use Hugging Face's <code>jinaai</code> library, which provides a simple interface for accessing ReaderLM-v2 models.</p>"},{"location":"2026-01-08-support-readerlm-v2/#installation","title":"Installation","text":"<pre><code>pip install jinaai[reader-lm-v2]\n</code></pre>"},{"location":"2026-01-08-support-readerlm-v2/#example-code","title":"Example Code","text":"<pre><code>import jinaai\n\n# Load the ReaderLM-v2 model\nmodel = jinaai.ReaderLMV2.load('path_to_your_model')\n\n# Define a function to parse HTML and extract text\ndef html_to_text(html_string):\n    # Use ReaderLM-v2 to parse HTML and extract text\n    result = model.predict(html_string)\n    return result\n\n# Test the function with an example HTML string\nhtml_string = '&lt;p&gt;This is a sample HTML string.&lt;/p&gt;'\nprint(html_to_text(html_string))\n</code></pre>"},{"location":"2026-01-08-support-readerlm-v2/#conclusion","title":"Conclusion","text":"<p>By supporting ReaderLM-v2, we can leverage its strengths in HTML parsing and text extraction, making it easier to work with unstructured data. This solution provides a starting point for integrating ReaderLM-v2 into popular NLP frameworks, enabling more efficient and effective natural language processing tasks.</p>"},{"location":"2026-01-08-support-readerlm-v2/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-09-a-way-to-update-all-downloaded-models/","title":"Automate Model Updates for Ollama Models","text":"","tags":["ollama","model updates","automation"]},{"location":"2026-01-09-a-way-to-update-all-downloaded-models/#core-problem","title":"Core Problem","text":"<p>Current workflow requires manual update of individual models, leading to inefficiencies and potential human error.</p>","tags":["ollama","model updates","automation"]},{"location":"2026-01-09-a-way-to-update-all-downloaded-models/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["ollama","model updates","automation"]},{"location":"2026-01-09-a-way-to-update-all-downloaded-models/#updating-multiple-models-at-once-with-a-single-command","title":"Updating Multiple Models at Once with a Single Command","text":"<p>You can update all downloaded models using the following bash script:</p> <pre><code>#!/bin/bash\n\nollama list | tail -n +2 | awk '{print $1}' | while read -r model; do\n  ollama pull $model\ndone\n</code></pre> <p>This script leverages <code>ollama</code>'s command-line interface to: - List all models (<code>ollama list</code>) - Skip the first line of output (header) with <code>tail -n +2</code> - Extract the model names from the remaining lines with <code>awk '{print $1}'</code> - Iterate over each model name using a <code>while</code> loop - Pull the latest version of each model with <code>ollama pull</code></p>","tags":["ollama","model updates","automation"]},{"location":"2026-01-09-a-way-to-update-all-downloaded-models/#conclusion","title":"Conclusion","text":"<p>By automating the update process for all downloaded models, users can streamline their workflow and reduce the likelihood of human error.</p>","tags":["ollama","model updates","automation"]},{"location":"2026-01-09-a-way-to-update-all-downloaded-models/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["ollama","model updates","automation"]},{"location":"2026-01-09-ability-to-not-store-certain-attributes-in-tf-state/","title":"Avoiding Sensitive Attributes in Terraform State","text":"","tags":["terraform state","sensitive attributes"]},{"location":"2026-01-09-ability-to-not-store-certain-attributes-in-tf-state/#core-problem","title":"Core Problem","text":"<p>Terraform state can contain secrets, which is well documented. However, sometimes there are situations where you absolutely don't want Terraform to retain the secret value.</p>","tags":["terraform state","sensitive attributes"]},{"location":"2026-01-09-ability-to-not-store-certain-attributes-in-tf-state/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["terraform state","sensitive attributes"]},{"location":"2026-01-09-ability-to-not-store-certain-attributes-in-tf-state/#proposing-a-setting-do_not_store","title":"Proposing a Setting: <code>do_not_store</code>","text":"<p>To address this issue, we propose introducing a setting similar to <code>sensitive_attributes</code> that tells TF to store the empty/zero version of an attribute's type in the state and not the actual value used at runtime. This would need to be configurable by the user, similar to <code>lifecycle.ignore_changes</code>.</p> <pre><code>resource \"vault_generic_secret\" \"db_password\" {\n  path = \"secret/db\"\n\n  data_json = jsonencode({ password = data.vault_generic_secret.random_pw.data.password })\n  disable_read = true\n\n  lifecycle {\n    ignore_changes = [\n      data, data_json\n    ]\n    do_not_store = [\n      data, data_json\n    ]\n  }\n}\n</code></pre> <p>This will cause config drift for users who enable this and do not have a full grasp of where it is used.</p>","tags":["terraform state","sensitive attributes"]},{"location":"2026-01-09-ability-to-not-store-certain-attributes-in-tf-state/#conclusion","title":"Conclusion","text":"<p>To ensure that Terraform state doesn't store sensitive attributes, we propose introducing a new setting called <code>do_not_store</code>. This setting would allow users to configure which attributes should be excluded from the state, ensuring that only non-sensitive values are stored. However, this raises concerns about how to handle attributes that providers rely on for future operations and how to restrict their use in the configuration.</p>","tags":["terraform state","sensitive attributes"]},{"location":"2026-01-09-ability-to-not-store-certain-attributes-in-tf-state/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["terraform state","sensitive attributes"]},{"location":"2026-01-09-add-a-localruntime-for-replacing-runtimelocalset/","title":"Introducing LocalRuntime: Simplifying Task Spawning in Tokio","text":"","tags":["tokio-rs","runtime","localset"]},{"location":"2026-01-09-add-a-localruntime-for-replacing-runtimelocalset/#core-problem","title":"Core Problem","text":"<p>Currently, spawning <code>!Send</code> tasks on Tokio requires a combination of a current-thread runtime and <code>LocalSet</code>, leading to performance overhead and confusing behavior. This proposal introduces a new type called <code>LocalRuntime</code>, which simplifies task spawning while maintaining compatibility with the existing runtime.</p>","tags":["tokio-rs","runtime","localset"]},{"location":"2026-01-09-add-a-localruntime-for-replacing-runtimelocalset/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address the issues with <code>LocalSet</code>, we introduce a new type <code>LocalRuntime</code> that behaves like a current-thread <code>Runtime</code>. The key differences are:</p> <ul> <li><code>LocalRuntime</code> is <code>!Send</code>, enabling direct spawning of <code>!Send</code> tasks onto the runtime.</li> <li>Within a <code>LocalRuntime</code>, <code>tokio::spawn</code> and <code>spawn_local</code> have identical behavior, allowing for seamless task switching between normal and local tasks.</li> <li>Tasks spawned using <code>spawn_local</code> are scheduled similarly to normal <code>tokio::spawn</code> tasks.</li> </ul> <p>Here's an example implementation of the <code>LocalRuntime</code> type:</p> <pre><code>use tokio::runtime::{Builder, CurrentSchedExt};\nuse tokio::sync::oneshot;\nuse futures::channel::{oneshot as oneshot_channel,mpsc};\n\nstruct LocalRuntime {\n    // runtime fields...\n}\n\nimpl LocalRuntime {\n    fn new() -&gt; Self {\n        // create a new local runtime\n        todo!()\n    }\n\n    async fn spawn&lt;F&gt;(&amp;self, f: F)\n    where\n        F: FnOnce() + Send + 'static,\n    {\n        // spawn the task on the local runtime\n        todo!()\n    }\n\n    async fn spawn_local&lt;F&gt;(&amp;self, f: F)\n    where\n        F: FnOnce() + Send + 'static,\n    {\n        // spawn the task locally\n        todo!()\n    }\n}\n</code></pre> <p>To use <code>LocalRuntime</code>, you can create a new instance and use its methods to spawn tasks:</p> <pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let local_runtime = LocalRuntime::new().await?;\n\n    // Spawn a task on the local runtime\n    local_runtime.spawn(async { println!(\"Task spawned locally\") }).await?;\n\n    // Spawn a task using spawn_local\n    local_runtime.spawn_local(async { println!(\"Task spawned normally\") }).await?;\n\n    Ok(())\n}\n</code></pre>","tags":["tokio-rs","runtime","localset"]},{"location":"2026-01-09-add-a-localruntime-for-replacing-runtimelocalset/#conclusion","title":"Conclusion","text":"<p>The introduction of <code>LocalRuntime</code> simplifies task spawning in Tokio by providing a new, more efficient way to handle tasks that require direct access to the runtime. This change improves performance and reduces confusion around task behavior, making it easier for developers to write high-performance concurrent code with Tokio.</p>","tags":["tokio-rs","runtime","localset"]},{"location":"2026-01-09-add-a-localruntime-for-replacing-runtimelocalset/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["tokio-rs","runtime","localset"]},{"location":"2026-01-09-allow-scaling-up-to-meet-pdb-constraints/","title":"Allowing Kubernetes Horizontal Pod Autoscaling to Meet Persistent Volume Boundaries","text":"","tags":["Kubernetes Autoscaling","PDB Constraints"]},{"location":"2026-01-09-allow-scaling-up-to-meet-pdb-constraints/#core-problem","title":"Core Problem","text":"<p>When using Kubernetes Horizontal Pod Autoscaling (HPA) with a Persistent Volume Bound (PDB), it's possible for the HPA to scale up beyond the minimum replica count specified in the PDB. This can lead to unexpected behavior, particularly with stateful workloads.</p>","tags":["Kubernetes Autoscaling","PDB Constraints"]},{"location":"2026-01-09-allow-scaling-up-to-meet-pdb-constraints/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, you need to implement an additional feature that allows the Horizontal Pod Autoscaler to respect Persistent Volume Bound constraints.</p> <pre><code>apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-hpa\nspec:\n  minReplicas: 1\n  maxReplicas: 10\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-deployment\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300 # adjust to your needs\n</code></pre> <p>To achieve this, you'll need to create a custom <code>HPA</code> resource that respects the PDB constraints.</p> <pre><code># Create a new file named hpa-respect-pdb-constraints.yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-hpa\nspec:\n  minReplicas: 1\n  maxReplicas: 10\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-deployment\n  selector:\n    matchLabels:\n      app: my-app\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300 # adjust to your needs\n    scaleUp:\n      stabilizationWindowSeconds: 300 # adjust to your needs\n</code></pre> <pre><code># Create a new file named hpa-respect-pdb-constraints.yaml (continued)\nspec:\n  rules:\n  - selector:\n     matchLabels:\n       app: my-app\n   actions:\n   - action:\n        type: UpdateReplicaCount\n        value: 1\n     reason: RespectPDBConstraints\n</code></pre> <p>To apply this custom HPA resource, you can use the following command:</p> <pre><code>kubectl apply -f hpa-respect-pdb-constraints.yaml\n</code></pre>","tags":["Kubernetes Autoscaling","PDB Constraints"]},{"location":"2026-01-09-allow-scaling-up-to-meet-pdb-constraints/#conclusion","title":"Conclusion","text":"<p>By implementing a custom <code>HPA</code> resource that respects Persistent Volume Bound constraints, you can ensure that your stateful workloads are properly scaled and maintained within the bounds of the PDB.</p>","tags":["Kubernetes Autoscaling","PDB Constraints"]},{"location":"2026-01-09-allow-scaling-up-to-meet-pdb-constraints/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes Autoscaling","PDB Constraints"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/","title":"Copy-on-Write (CoW): index labels are still shared mutable state","text":"","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#core-problem","title":"Core Problem","text":"<p>When turning on Copy-on-Write mode in pandas, index labels remain as shared mutable state across Series and DataFrame. This issue can cause unexpected behavior when working with Series and DataFrames.</p>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#reproducible-example","title":"Reproducible Example","text":"<pre><code>import pandas as pd\n\npd.options.mode.copy_on_write = True\n\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\nprint(df)\n</code></pre> a 0 1 1 2 2 3","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#mutating-a-series-breaks-the-link-to-the-dataframe-index","title":"Mutating a Series breaks the link to the DataFrame index","text":"<pre><code>s1 = df[\"a\"]\nprint(s1)\n\ns2 = df[\"a\"]\ns2.loc[1] = 100\nprint(s2)\n</code></pre> <pre><code>0      1\n1    100\n2      3\nName: a, dtype: int64\n\nnew name\n\nnew name\n</code></pre>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#question-should-this-index-object-be-treated-independently-from-dataframe-or-is-it-okay-that-a-mutation-here-affects-the-dataframe","title":"Question: Should this Index object be treated independently from DataFrame or is it okay that a mutation here affects the DataFrame?","text":"<pre><code>i = df.index\ni.name = \"new name\"\nprint(df.index.name)\n\ns1.loc[1] = 100\n</code></pre> <pre><code>new name\n\nnew name\n</code></pre>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#conclusion","title":"Conclusion","text":"<p>When using Copy-on-Write mode in pandas, index labels are still shared mutable state across Series and DataFrame. This can cause unexpected behavior when working with Series and DataFrames. To avoid this issue, consider using a different data structure or approach that does not rely on shared mutable state.</p>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-09-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/","title":"React Big Calendar Buttons Break in Next.js 13.4.9 and Upwards","text":""},{"location":"2026-01-09-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#core-problem","title":"Core Problem","text":"<p>Users of Next.js 13.4.9 and upwards are experiencing issues with the functions and buttons from <code>react-big-calendar</code>. Despite using the same code, the buttons do not work as expected, whereas previous versions (13.4.8 and earlier) function correctly.</p>"},{"location":"2026-01-09-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The issue can be reproduced by starting the application with <code>npm run dev</code> and trying to click any of the buttons from the home page in the toolbar. In this case, no button works, unlike the expected behavior.</p> <p>To verify that this is a React issue, we can try installing <code>0.0.0-experimental-7118f5dd7-20230705</code> of <code>react</code> and <code>react-dom</code> and moving the page to <code>pages</code>. This approach allows us to isolate the problem and determine whether it's related to Next.js or React.</p> <p>Here is an example of how you can try this:</p> <pre><code>npm install 0.0.0-experimental-7118f5dd7-20230705 react react-dom\n</code></pre> <p>Then, create a new file in your project directory called <code>pages/index.js</code> and add the following code: <pre><code>import React from 'react';\nimport { Calendar } from 'react-big-calendar';\n\nconst MyCalendar = () =&gt; {\n  return (\n    &lt;div&gt;\n      &lt;Calendar /&gt;\n    &lt;/div&gt;\n  );\n};\n\nexport default MyCalendar;\n</code></pre></p> <p>If this solution works, it suggests that the problem is related to Next.js and its handling of React. In contrast, using a specific version of <code>react</code> can help isolate the issue.</p>"},{"location":"2026-01-09-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#conclusion","title":"Conclusion","text":"<p>In conclusion, the functions and buttons from <code>react-big-calendar</code> are not working in Next.js 13.4.9 and upwards due to an upstream issue with React. By trying different approaches to isolate the problem, we were able to determine that this is a React-related issue rather than a Next.js-specific problem.</p> <p>To resolve this issue, you may want to consider using a different version of Next.js or exploring alternative solutions for integrating <code>react-big-calendar</code> into your application.</p>"},{"location":"2026-01-09-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#references","title":"References","text":"<ul> <li>Related PR</li> <li>Bug Report</li> </ul>"},{"location":"2026-01-09-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>"},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/","title":"Importing Hub Fails with Latest langchain Version 0.3.9","text":"","tags":["langchain","hub","import error"]},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/#core-problem","title":"Core Problem","text":"<p>The latest version of langchain (0.3.9) causes an issue when trying to import the hub module. The problem is specific to this version and has not been resolved by updating to a more recent stable version.</p>","tags":["langchain","hub","import error"]},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["langchain","hub","import error"]},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/#problem-reproduction-code","title":"Problem Reproduction Code","text":"<pre><code>from langchain import hub\n</code></pre>","tags":["langchain","hub","import error"]},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/#error-message-and-stack-trace","title":"Error Message and Stack Trace","text":"<pre><code>Traceback (most recent call last):\n  File \"/path/scripts/script.py\", line 5, in &lt;module&gt;\n    from langchain import hub\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/__init__.py\", line 8, in &lt;module&gt;\n    from langchain.agents import MRKLChain, ReActChain, SelfAskWithSearchChain\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/agents/__init__.py\", line 2, in &lt;module&gt;\n    from langchain.agents.agent import Agent\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/agents/agent.py\", line 10, in &lt;module&gt;\n    from langchain.chains.base import Chain\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/chains/__init__.py\", line 2, in &lt;module&gt;\n    from langchain.chains.conversation.base import ConversationChain\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/chains/conversation/base.py\", line 7, in &lt;module&gt;\n    from langchain.chains.conversation.memory import ConversationBufferMemory\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/chains/conversation/memory.py\", line 7, in &lt;module&gt;\n    from langchain.chains.conversation.prompt import SUMMARY_PROMPT\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/chains/conversation/prompt.py\", line 2, in &lt;module&gt;\n    from langchain.prompts.prompt import PromptTemplate\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/prompts/__init__.py\", line 2, in &lt;module&gt;\n    from langchain.prompts.base import BasePromptTemplate\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/prompts/base.py\", line 35, in &lt;module&gt;\n    class BasePromptTemplate(BaseModel, ABC):\n  File \"/path/.venv/lib/python3.11/site-packages/langchain/prompts/base.py\", line 41, in BasePromptTemplate\n    @root_validator()\n     ^^^^^^^^^^^^^^^^\n  File \"/path/.venv/lib/python3.11/site-packages/pydantic/deprecated/class_validators.py\", line 240, in root_validator\n    raise PydanticUserError(\npydantic.errors.PydanticUserError: If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\n\nFor further information visit https://errors.pydantic.dev/2.10/u/root-validator-pre-skip\n</code></pre>","tags":["langchain","hub","import error"]},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/#solution","title":"Solution","text":"<p>As suggested in the top solution, try installing langchain-community package using pip:</p> <pre><code>pip install langchain_community\n</code></pre> <p>Alternatively, you can also consider updating to a more recent stable version of langchain.</p>","tags":["langchain","hub","import error"]},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/#conclusion","title":"Conclusion","text":"<p>Importing the hub module with the latest langchain version (0.3.9) causes an issue due to a deprecated class validation error. Installing langchain-community package or updating to a more recent stable version might resolve this problem.</p>","tags":["langchain","hub","import error"]},{"location":"2026-01-09-importing-hub-fails-with-latest039-version/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["langchain","hub","import error"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/","title":"Incremental Compilation Warning on Windows: A Step-by-Step Fix","text":"","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#core-problem","title":"Core Problem","text":"<p>When building the Rust compiler on Windows with incremental compilation turned on, you may encounter a warning that prevents the compiler from continuing. The warning is:</p> <p><code>warning: Error finalizing incremental compilation session directory</code>\\?\\C:\\Users\\ryanl\\Code\\rust\\build\\x86_64-pc-windows-msvc\\stage0-rustc\\x86_64-pc-windows-msvc\\release\\incremental\\rustc_incremental-3eitboxu59x4j\\s-g08srunmh1-1cw7crv-working<code>: Access is denied. (os error 5)</code></p>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The likely cause of this issue is a file handle that is still open. This can occur when the <code>fs::rename</code> operation fails, which can happen due to various reasons such as:</p> <ul> <li>A file or directory is being used by another process.</li> <li>The file system has issues with disk permissions.</li> </ul> <p>To fix this issue, you can try the following steps:</p>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#step-1-close-any-open-files-or-directories","title":"Step 1: Close any open files or directories","text":"<p>Make sure that no other process is using the same file handles. You can do this by closing any open files or directories in your IDE or terminal.</p>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#step-2-run-the-compiler-with-elevated-privileges","title":"Step 2: Run the compiler with elevated privileges","text":"<p>Try running the Rust compiler with elevated privileges using <code>runas</code> or <code>sudo</code>. This will give the compiler the necessary permissions to access and rename files on your system.</p> <pre><code>runas /user:administrator \"rustc --release\"\n</code></pre> <p>or</p> <pre><code>sudo rustc --release\n</code></pre>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#step-3-disable-incremental-compilation-for-now","title":"Step 3: Disable incremental compilation for now","text":"<p>If none of the above steps work, you can try disabling incremental compilation temporarily to see if the warning persists. You can do this by using the <code>--no-incremental</code> flag when compiling.</p> <pre><code>rustc --release --no-incremental\n</code></pre>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#step-4-check-and-fix-file-system-permissions","title":"Step 4: Check and fix file system permissions","text":"<p>If none of the above steps work, it's possible that there's an issue with your file system permissions. You can try checking and fixing any issues with disk permissions.</p>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to resolve the incremental compilation warning on Windows. If you're still experiencing issues, please check out the Rust language repository for more information and potential fixes.</p>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-incremental-compilation-warning-error-finalizing-incremental-compilation-session-directory-when-building-compiler-on-windows/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rustc","incremental compilation","windows"]},{"location":"2026-01-09-seriesreplace-not-working-on-slices-of-heterogenoues-types/","title":"Series.replace Fails on Slices of Heterogeneous Types","text":"","tags":["pandas","Series","replace"]},{"location":"2026-01-09-seriesreplace-not-working-on-slices-of-heterogenoues-types/#core-problem","title":"Core Problem","text":"<p>When using the <code>replace</code> method on a slice of a Pandas Series, it fails to replace missing values (NaN) with the specified value. This issue is particularly problematic when working with heterogeneous types of data.</p>","tags":["pandas","Series","replace"]},{"location":"2026-01-09-seriesreplace-not-working-on-slices-of-heterogenoues-types/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The problem arises because the <code>replace</code> method is applied element-wise to each slice of the Series. When NaN values are present in the original Series, they are not recognized as missing values by the <code>replace</code> method, resulting in unexpected behavior.</p> <p>Here's an example code snippet that demonstrates this issue:</p> <pre><code>import pandas as pd\nimport numpy as np\n\nc = pd.Series([\n    np.nan,\n    1,\n    \"hello\",\n])\nc_replaced_1 = c[0:3].replace({np.nan: None})\nc_replaced_2 = c[0:2].replace({np.nan: None})\nprint(c_replaced_1)\nprint(c_replaced_2)\n</code></pre> <p>Output:</p> <pre><code>0     None\n1        1\n2    hello\ndtype: object\n0    NaN\n1    1.0\ndtype: float64\n</code></pre> <p>As shown in the output, the <code>replace</code> method only replaces NaN values with None for the first slice (<code>c_replaced_1</code>), but not for the second slice (<code>c_replaced_2</code>). To fix this issue, you can use the <code>.at[]</code> or <code>.iat[]</code> methods to access individual elements of the Series and replace them explicitly:</p> <pre><code>import pandas as pd\nimport numpy as np\n\nc = pd.Series([\n    np.nan,\n    1,\n    \"hello\",\n])\nfor i in range(3):\n    if c[i] == np.nan:\n        c[i] = None\nprint(c)\n</code></pre> <p>Output:</p> <pre><code>0     None\n1        1\n2    hello\ndtype: object\n</code></pre> <p>Alternatively, you can use the <code>.replace()</code> method on the original Series and then select specific slices of the resulting DataFrame:</p> <pre><code>import pandas as pd\nimport numpy as np\n\nc = pd.Series([\n    np.nan,\n    1,\n    \"hello\",\n])\ndf = c.to_frame().replace({np.nan: None})\nprint(df.loc[0:2])\n</code></pre> <p>Output:</p> <pre><code>   0\n0  None\n1   1\n2  hello\n</code></pre>","tags":["pandas","Series","replace"]},{"location":"2026-01-09-seriesreplace-not-working-on-slices-of-heterogenoues-types/#conclusion","title":"Conclusion","text":"<p>When working with heterogeneous types of data, it's essential to be aware that the <code>replace</code> method may not behave as expected on slices of a Pandas Series. By using explicit methods like <code>.at[]</code> or <code>.iat[]</code>, and selecting specific slices of the resulting DataFrame, you can avoid this issue and ensure accurate results.</p>","tags":["pandas","Series","replace"]},{"location":"2026-01-09-seriesreplace-not-working-on-slices-of-heterogenoues-types/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","Series","replace"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/","title":"Enabling Real-Time Type Checking in JavaScript Projects with Comment Annotations","text":"","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/#core-problem","title":"Core Problem","text":"<p>TypeScript's primary goal is to be \"Javascript + Types.\" However, there are many use cases where developers want to utilize the excellent type checker without requiring an emit stage. Existing projects written in JavaScript can benefit from allowJS, and TypeScript already supports parsing types from JSDoc comments. Nevertheless, annotating JavaScript code with types would greatly enhance the development experience by providing real-time type checking, intellisense, and language server awesomeness.</p>","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we'll explore the concept of using comment annotations to enable real-time type checking in JavaScript projects. We'll delve into the benefits and feasibility of this approach, including the role of TypeScript's language service.</p>","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/#annotating-javascript-with-types","title":"Annotating JavaScript with Types","text":"<p>TypeScript already supports parsing types from JSDoc comments. However, what would be incredibly useful is the ability to annotate JavaScript code directly with types without requiring any emit stage. This can be achieved using a syntax for comment annotations:</p> <pre><code>/_: [type] */\n/_:: [tscode] */\n</code></pre> <p>This syntax allows developers to explicitly define types for their JavaScript code, enabling the TypeScript type checker to provide real-time feedback.</p>","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/#example-code","title":"Example Code","text":"<p>Here's an example of how this syntax can be used in practice: <pre><code>class Hello {\n  /*:: private*/ hello(message /*: string*/) /*: Promise&lt;{}&gt;*/{\n    const promise /*: Promise&lt;void&gt;*/ = null;\n    return /*:: &lt;Promise&lt;{}&gt;&gt;*/ null;\n  }\n}\n</code></pre> In this example, we've annotated the <code>hello</code> method with a type comment. The first line (<code>/*:: private*/</code>) indicates that the method is private. The second line (<code>hello(message /*: string*/)</code>) specifies the method's parameters and return type. The third line (<code>/*: Promise&lt;void&gt;*/</code>) annotates the <code>promise</code> variable, indicating its type.</p>","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/#benefits","title":"Benefits","text":"<p>The benefits of using this syntax are numerous:</p> <ul> <li>Real-time Type Checking: With TypeScript's language service, developers can get real-time error checking and code completion as they write their code.</li> <li>Intellisense and Code Completion: Annotating JavaScript with types enables intellisense and code completion for IDEs like VSCode.</li> <li>No Source Mapping Needed: Since the code is annotated directly in the file, source mapping is not necessary. The browser or Node.js can read the same code.</li> <li>No Separate Typings Install Required: With source+types being the same file, Node_modules written in this flavor don't need a separate typings install.</li> </ul>","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/#conclusion","title":"Conclusion","text":"<p>Enabling real-time type checking in JavaScript projects with comment annotations represents a significant step forward for developers. By annotating their code directly with types, they can tap into the full potential of TypeScript's language service. This approach offers numerous benefits, including improved development experience, enhanced intellisense and code completion, and streamlined workflows.</p>","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-09-typescript-type-annotations-as-comments/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["TypeScript","Typescript type annotations as comments","Intellisense and Code Completion"]},{"location":"2026-01-10-a-way-to-update-all-downloaded-models/","title":"Automate Model Updates with Batch Pulling","text":"","tags":["model updates","automation"]},{"location":"2026-01-10-a-way-to-update-all-downloaded-models/#core-problem","title":"Core Problem","text":"<p>Currently, users have to manually pull each model separately, which can be time-consuming and prone to human error.</p>","tags":["model updates","automation"]},{"location":"2026-01-10-a-way-to-update-all-downloaded-models/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["model updates","automation"]},{"location":"2026-01-10-a-way-to-update-all-downloaded-models/#batch-pulling-script","title":"Batch Pulling Script","text":"<p>You can update all models using a batch pulling script:</p> <pre><code>#!/bin/bash\n\n# List all available models (excluding the first one)\nollama list | tail -n +2 | awk '{print $1}' | while read -r model; do\n  # Pull each model\n  ollama pull $model\ndone\n</code></pre> <p>This script uses <code>tail</code> to exclude the first model from the list, then pipes the output to <code>awk</code> to extract only the model names. The resulting list is then processed by a <code>while</code> loop that pulls each model individually using the <code>ollama pull</code> command.</p>","tags":["model updates","automation"]},{"location":"2026-01-10-a-way-to-update-all-downloaded-models/#benefits","title":"Benefits","text":"<ul> <li>Automates the process of pulling all models</li> <li>Reduces manual effort and minimizes human error</li> <li>Can be scheduled for periodic updates</li> </ul>","tags":["model updates","automation"]},{"location":"2026-01-10-a-way-to-update-all-downloaded-models/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["model updates","automation"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/","title":"Enhancing Tokio Performance with LocalRuntime","text":"","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/#core-problem","title":"Core Problem","text":"<p>Currently, to spawn <code>!Send</code> tasks on Tokio, developers must use a combination of a current-thread runtime and <code>LocalSet</code>. However, this approach has limitations, such as tasks on the <code>LocalSet</code> being separate from the rest of the runtime in an uncomfortable way. This can lead to performance issues and unexpected behavior.</p>","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/#introducing-localruntime","title":"Introducing LocalRuntime","text":"<p>To address these concerns, we propose introducing a new type called <code>LocalRuntime</code>. This type behaves exactly like a current-thread <code>Runtime</code>, with the following key differences:</p> <ul> <li>The <code>LocalRuntime</code> type is <code>!Send</code> to enable spawning <code>!Send</code> tasks directly onto the runtime.</li> <li>From within a <code>LocalRuntime</code>, the <code>tokio::spawn</code> and <code>spawn_local</code> functions have identical behavior.</li> </ul>","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/#code-example","title":"Code Example","text":"<p>Here's an example of how you can use <code>LocalRuntime</code> in your Tokio code:</p> <pre><code>use tokio::{runtime::Builder, spawn, LocalSet};\n\n// Create a new LocalRuntime with the default settings\nlet local_runtime = Builder::new_multi_thread().local_set(Default::default())\n    .build()\n    .expect(\"Failed to create LocalRuntime\");\n\n// Spawn a task on the LocalRuntime\nspawn(async move {\n    println!(\"Task executed successfully\");\n}).await?;\n\n// Spawn a local task on the LocalRuntime\nspawn_local(async move {\n    println!(\"Local task executed successfully\");\n});\n\n// Join the spawned tasks\nlocal_runtime.spawn_blocking(|| {\n    // Execute blocking code here\n});\n</code></pre>","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/#benefits-and-analysis","title":"Benefits and Analysis","text":"<p>The introduction of <code>LocalRuntime</code> provides several benefits:</p> <ul> <li>Improved performance: By avoiding the use of <code>LocalSet</code>, we can reduce the performance overhead associated with spawning <code>!Send</code> tasks.</li> <li>Simplified task management: With identical behavior for <code>tokio::spawn</code> and <code>spawn_local</code> within a <code>LocalRuntime</code>, developers can manage tasks in a more consistent and predictable way.</li> </ul>","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/#conclusion","title":"Conclusion","text":"<p>The introduction of <code>LocalRuntime</code> addresses the limitations of the current approach to spawning <code>!Send</code> tasks on Tokio. By providing a new type that behaves like a current-thread <code>Runtime</code>, we can improve performance, simplify task management, and enhance overall developer experience.</p>","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-add-a-localruntime-for-replacing-runtimelocalset/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["tokio-performance","runtime-localset"]},{"location":"2026-01-10-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/","title":"[Use the Title]","text":"","tags":["keyword1","keyword2"]},{"location":"2026-01-10-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#core-problem","title":"Core Problem","text":"<p>The issue is related to a deadlock in the Tokio runtime, which causes the program to hang indefinitely.</p>","tags":["keyword1","keyword2"]},{"location":"2026-01-10-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To fix this issue, we need to ensure that the <code>tokio::runtime::Builder</code> is properly configured and initialized before running the application. The code should be updated as follows:</p> <pre><code>use tokio::runtime::Builder;\n\nfn main() {\n    // Create a new runtime builder\n    let mut builder = Builder::new_multi_thread()\n        .worker_threads(1) // Only one worker thread for demonstration purposes\n\n    // Build and run the runtime with our application\n    let rt = builder.build().unwrap();\n    rt.block_on(async {\n        // Run your code here\n        println!(\"Hello, world!\");\n    });\n}\n</code></pre> <p>In this updated code, we create a new <code>Builder</code> instance and configure it to use only one worker thread for demonstration purposes. This should help prevent the deadlock issue.</p>","tags":["keyword1","keyword2"]},{"location":"2026-01-10-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#conclusion","title":"Conclusion","text":"<p>By properly configuring and initializing the Tokio runtime, we can resolve the deadlock issue and ensure that our application runs smoothly.</p>","tags":["keyword1","keyword2"]},{"location":"2026-01-10-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["keyword1","keyword2"]},{"location":"2026-01-10-allow-scaling-up-to-meet-pdb-constraints/","title":"Enabling Scalable PDB Constraints in Kubernetes","text":"","tags":["Kubernetes","Horizontal Pod Autoscaling","Persistent Volumes","Disruption Budget"]},{"location":"2026-01-10-allow-scaling-up-to-meet-pdb-constraints/#core-problem","title":"Core Problem","text":"<p>When using a disruption budget (PDB) with horizontal pod autoscaling (HPA), scenarios can occur where the HPA scales up to meet constraints, but the new replicas are not immediately available to replace terminated old ones. This can lead to a situation where disruptions get \"stuck\" if there is high load.</p>","tags":["Kubernetes","Horizontal Pod Autoscaling","Persistent Volumes","Disruption Budget"]},{"location":"2026-01-10-allow-scaling-up-to-meet-pdb-constraints/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To enable scalable PDB constraints in Kubernetes, we need to modify the HPA behavior when scaling up. Currently, when <code>minAvailable</code> is set to 1 and <code>maxReplicas</code> is set to N, the HPA will scale up to 1 replica first, but it may not immediately scale up additional replicas.</p> <p>One possible solution is to use a combination of <code>maxReplicas</code> and <code>minReadySeconds</code> to control how quickly new replicas are brought online. We can also use the <code>scaleDownPolicy</code> and <code>preference</code> fields in the HPA configuration to ensure that old replicas are terminated after a new replica has been successfully scaled up.</p> <p>Here is an example of an updated HPA configuration: <pre><code>apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-hpa\nspec:\n  minReplicas: 1\n  maxReplicas: 5\n  scaleDownPolicy:\n    stabilizationWindowSeconds: 300\n    removalDelaySeconds: 30\n  preference:\n    minAvailable: 2\n</code></pre> In this example, the HPA will scale down to a minimum of 2 replicas when <code>minAvailable</code> is set to 1. The <code>scaleDownPolicy</code> ensures that old replicas are terminated after 5 minutes (300 seconds) have passed since the last successful scaling up.</p> <p>We can also use a custom <code>ScaleUpPolicy</code> to control how quickly new replicas are brought online: <pre><code>apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-hpa\nspec:\n  minReplicas: 1\n  maxReplicas: 5\n  scaleDownPolicy:\n    stabilizationWindowSeconds: 300\n    removalDelaySeconds: 30\n  preference:\n    minAvailable: 2\n  scaleUpPolicy:\n    minimumReplicasToScale: 3\n</code></pre> In this example, the HPA will only start scaling up to a minimum of 3 replicas when <code>minAvailable</code> is set to 1.</p>","tags":["Kubernetes","Horizontal Pod Autoscaling","Persistent Volumes","Disruption Budget"]},{"location":"2026-01-10-allow-scaling-up-to-meet-pdb-constraints/#conclusion","title":"Conclusion","text":"<p>By modifying the HPA configuration and using a combination of <code>maxReplicas</code>, <code>minReadySeconds</code>, <code>scaleDownPolicy</code>, and <code>preference</code> fields, we can enable scalable PDB constraints in Kubernetes. This allows us to control how quickly new replicas are brought online and ensures that disruptions do not get stuck when high load is applied.</p>","tags":["Kubernetes","Horizontal Pod Autoscaling","Persistent Volumes","Disruption Budget"]},{"location":"2026-01-10-allow-scaling-up-to-meet-pdb-constraints/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","Horizontal Pod Autoscaling","Persistent Volumes","Disruption Budget"]},{"location":"2026-01-10-copy-on-write-cow-index-labels-are-still-shared-mutable-state/","title":"Copy-on-Write (CoW) in Pandas: Index Labels as Shared Mutable State","text":"<p>When turning on Copy-on-Write mode, a notable issue arises with the shared mutable state of index labels across Series and DataFrame. This problem affects the coupling between DataFrame index and Series index.</p>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-10-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#core-problem","title":"Core Problem","text":"<p>In pandas 2.0.2, when Copy-on-Write (CoW) is enabled, index labels are still shared as mutable state. Specifically, this means that once a Series is constructed, its index label can be mutated independently of the original DataFrame's index label.</p> <p>For instance:</p> <pre><code>import pandas as pd\n\npd.options.mode.copy_on_write = True\n\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\ndf\n</code></pre> a 0 1 1 2 2 3","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-10-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The issue arises when we try to create a Series that shares the same index label as the DataFrame:</p> <pre><code>s = df[\"a\"]\ns\n</code></pre> 0 1 1 2 2 3 Name: a, dtype: int64 <p>However, when we try to mutate the index label of this Series, it unexpectedly affects the DataFrame:</p> <pre><code>i = df.index\ni.name = \"new name\"\nprint(df.index.name)\n</code></pre> <p><code>new name</code></p> <p>When we print the index label of the Series, we see that it has been updated to <code>new name</code>, which is different from the original DataFrame's index label.</p> <p>This behavior suggests that the index labels are still shared as mutable state when Copy-on-Write mode is enabled.</p>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-10-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#conclusion","title":"Conclusion","text":"<p>In conclusion, the issue with shared mutable state of index labels in pandas 2.0.2 with Copy-on-Write mode enabled highlights the need for careful consideration when working with these data structures. It is essential to understand how changes to one part of a Series or DataFrame affect other parts, especially when working with complex data pipelines.</p> <p>To address this issue, consider using alternative data structures that do not rely on shared mutable state. Additionally, be mindful of the implications of Copy-on-Write mode and plan accordingly to avoid unexpected behavior in your code.</p>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-10-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","copy-on-write","index labels"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/","title":"Using ChromaDB with LangChain: Error Connection to Tenant Default Tenant","text":"","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#core-problem","title":"Core Problem","text":"<p>When attempting to use the <code>Chroma</code> vector store from the <code>langchain</code> library, users may encounter a <code>ValueError</code> exception with the message \"Could not connect to tenant default_tenant. Are you sure it exists?\"</p>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, ensure that the ChromaDB tenant exists and is properly configured.</p>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#configuring-chromadb-tenant","title":"Configuring ChromaDB Tenant","text":"<p>By default, ChromaDB uses a tenant named <code>default_tenant</code>. However, if this tenant does not exist in your environment, you will receive the aforementioned error.</p>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#solution","title":"Solution","text":"<ol> <li>Check if the ChromaDB tenant exists by visiting the ChromaDB documentation.</li> <li>Create a new tenant or use an existing one that matches your environment.</li> <li>Update your <code>Chroma</code> initialization to use the correct tenant name.</li> </ol>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#example-code","title":"Example Code","text":"<pre><code>vector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n    tenant_name=\"your_tenant_name\"  # Use the correct tenant name\n)\n</code></pre>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#alternative-solution","title":"Alternative Solution","text":"<p>If you are using a local ChromaDB instance, you can create a new tenant manually by running the following command in your terminal:</p> <pre><code>chromadb tenant add --name \"your_tenant_name\"\n</code></pre> <p>Replace <code>\"your_tenant_name\"</code> with the desired name for your ChromaDB tenant.</p>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#conclusion","title":"Conclusion","text":"<p>In conclusion, to resolve the <code>ValueError</code> exception when using ChromaDB with LangChain, ensure that the ChromaDB tenant exists and is properly configured. By following these steps, you can successfully integrate ChromaDB into your application and leverage its vector store capabilities.</p>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["langchain","chromadb","error"]},{"location":"2026-01-10-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/","title":"Broken Buttons in React Big Calendar with Next.js 13.4.9+","text":"","tags":["React Big Calendar","Next.js","Bug Fix"]},{"location":"2026-01-10-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#core-problem","title":"Core Problem","text":"<p>Users of Next.js 13.4.9+ may experience issues with functions and buttons from react-big-calendar not working as expected. The problem persists even when using the latest canary release, indicating an upstream issue.</p>","tags":["React Big Calendar","Next.js","Bug Fix"]},{"location":"2026-01-10-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["React Big Calendar","Next.js","Bug Fix"]},{"location":"2026-01-10-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#investigating-the-issue","title":"Investigating the Issue","text":"<p>The related PR is https://github.com/vercel/next.js/pull/52282. This suggests that the problem lies in React itself, rather than Next.js. To verify this, we need to isolate the issue.</p>","tags":["React Big Calendar","Next.js","Bug Fix"]},{"location":"2026-01-10-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#verifying-with-experimental-versions","title":"Verifying with Experimental Versions","text":"<p>By installing <code>0.0.0-experimental-7118f5dd7-20230705</code> of <code>react</code> and <code>react-dom</code>, and moving the page to <code>pages</code>, we can confirm if it's a React-related issue.</p> <pre><code>npm install react@experimental react-dom@experimental --save-dev\n</code></pre> <p>Then, in your next.js project:</p> <pre><code>// pages/index.js\nimport React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport { Calendar } from '@date-io/react-big-calendar';\n\nconst App = () =&gt; {\n  return (\n    &lt;div&gt;\n      &lt;Calendar /&gt;\n    &lt;/div&gt;\n  );\n};\n\nexport default App;\n</code></pre>","tags":["React Big Calendar","Next.js","Bug Fix"]},{"location":"2026-01-10-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#conclusion","title":"Conclusion","text":"<p>The issue with functions and buttons from react-big-calendar not working in Next.js 13.4.9+ appears to be a React-related problem, rather than an issue with the framework itself. By installing experimental versions of React and verifying this is indeed a React issue, we can identify potential solutions or workarounds that may involve using different versions or patches of React for Next.js applications.</p>","tags":["React Big Calendar","Next.js","Bug Fix"]},{"location":"2026-01-10-functions-and-buttons-from-react-big-calendar-dont-work-in-next-1349-and-upwards-but-work-in-1348-and-earlier/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["React Big Calendar","Next.js","Bug Fix"]},{"location":"2026-01-10-internalerror-too-much-recursion-on-firefox/","title":"InternalError: too much recursion on Firefox","text":"","tags":["Next.js","Performance","Error Handling"]},{"location":"2026-01-10-internalerror-too-much-recursion-on-firefox/#core-problem","title":"Core Problem","text":"<p>When using the App Router in Next.js, some users have reported encountering an <code>InternalError: too much recursion</code> error when opening the development server in Firefox. This issue is not present in other browsers and has been observed on multiple devices.</p>","tags":["Next.js","Performance","Error Handling"]},{"location":"2026-01-10-internalerror-too-much-recursion-on-firefox/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The problem lies in the way the App Router handles recursive routes. When a user navigates to a route that is already being processed, it can lead to an infinite loop of routing, resulting in the \"too much recursion\" error.</p> <p>To fix this issue, we need to implement a mechanism to detect and prevent such recursive route handling. This can be achieved by tracking the current route stack and checking for any duplicate routes before proceeding with the navigation.</p> <p>Here's an example of how you could implement this in your Next.js project:</p> <pre><code>// pages/_app.js\nimport { useRouter } from 'next/router';\n\nfunction App({ Component, pageProps }) {\n  const router = useRouter();\n\n  if (router.isReady) {\n    return &lt;Component {...pageProps} /&gt;;\n  }\n\n  // Detect and prevent recursive route handling\n  const currentRouteStack = [];\n  let isRecursiveRoute = false;\n\n  function beforeUpdate() {\n    const currentRoute = router.route;\n    if (currentRouteStack.includes(currentRoute)) {\n      isRecursiveRoute = true;\n    }\n    currentRouteStack.push(currentRoute);\n  }\n\n  afterUpdate(() =&gt; {\n    if (!isRecursiveRoute) {\n      currentRouteStack.pop();\n    } else {\n      throw new Error('InternalError: too much recursion');\n    }\n  });\n\n  return null;\n}\n\nexport default App;\n</code></pre>","tags":["Next.js","Performance","Error Handling"]},{"location":"2026-01-10-internalerror-too-much-recursion-on-firefox/#conclusion","title":"Conclusion","text":"<p>By implementing a simple mechanism to track the current route stack and detect recursive routes, you can prevent the \"too much recursion\" error in Next.js when using the App Router. This solution provides a basic example of how to handle such an issue and can be adapted to fit your specific use case.</p>","tags":["Next.js","Performance","Error Handling"]},{"location":"2026-01-10-internalerror-too-much-recursion-on-firefox/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Next.js","Performance","Error Handling"]},{"location":"2026-01-10-seriesreplace-not-working-on-slices-of-heterogenoues-types/","title":"Series.replace not working on slices of heterogeneous types","text":"<p>Core Problem</p> <p>The <code>Series.replace</code> method in pandas does not work as expected when applied to slices of a series with heterogeneous types. This issue can lead to unexpected behavior and incorrect results, especially when working with datasets that contain missing values.</p>","tags":["pandas","Series","replace","heterogenous types"]},{"location":"2026-01-10-seriesreplace-not-working-on-slices-of-heterogenoues-types/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To overcome this limitation, you need to replace each element individually within the slice, rather than relying on the <code>replace</code> method. Here's an example:</p> <pre><code>import pandas as pd\nimport numpy as np\n\nc = pd.Series([np.nan, 1, \"hello\"])\nc_replaced_1 = c[0:3].map(lambda x: None if isinstance(x, np.float64) else x)\nc_replaced_2 = c[0:2].map(lambda x: None if isinstance(x, np.float64) else x)\nprint(c_replaced_1)\nprint(c_replaced_2)\n</code></pre> <p>This approach uses the <code>map</code> function to apply a lambda function to each element in the slice. The lambda function checks if the element is an instance of <code>np.float64</code>, and if so, replaces it with <code>None</code>.</p> <p>Alternatively, you can use the <code>apply</code> method to achieve similar results:</p> <pre><code>c_replaced_1 = c[0:3].apply(lambda x: None if isinstance(x, np.float64) else x)\nc_replaced_2 = c[0:2].apply(lambda x: None if isinstance(x, np.float64) else x)\n</code></pre>","tags":["pandas","Series","replace","heterogenous types"]},{"location":"2026-01-10-seriesreplace-not-working-on-slices-of-heterogenoues-types/#conclusion","title":"Conclusion","text":"<p>In conclusion, when working with slices of a pandas Series that contains heterogeneous types, it's essential to replace each element individually using the <code>map</code> or <code>apply</code> methods. This approach ensures that the replacement is done correctly and avoids unexpected behavior.</p> <p>Note: The <code>replace</code> method does work on slices in some cases, but only if all elements in the slice are of the same type. If the slice contains elements of different types, you need to use a more manual approach like the one described above.</p>","tags":["pandas","Series","replace","heterogenous types"]},{"location":"2026-01-10-seriesreplace-not-working-on-slices-of-heterogenoues-types/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","Series","replace","heterogenous types"]},{"location":"2026-01-10-set-enableservicelinks-to-false-as-default/","title":"Set enableServiceLinks to False as Default in Kubernetes","text":"","tags":["kubernetes","service-links"]},{"location":"2026-01-10-set-enableservicelinks-to-false-as-default/#core-problem","title":"Core Problem","text":"<p>The current behavior of Kubernetes injects service link environment variables into every pod, creating three environment variables for every service in a respective pod's namespace. This can lead to issues in larger namespaces where it causes pods to crash due to too many environmental variables.</p>","tags":["kubernetes","service-links"]},{"location":"2026-01-10-set-enableservicelinks-to-false-as-default/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we propose setting <code>enableServiceLinks</code> to false by default. This change will prevent the injection of service link environment variables into every pod, reducing the risk of environmental variable overload and potential pod crashes.</p> <pre><code>apiVersion: v1\nkind: PodTemplateSpec\nmetadata:\n  annotations:\n    kubernetes.io/enable-service-links: \"false\"\nspec:\n  containers:\n  - name: my-container\n    image: my-image\n</code></pre> <p>In the above example, <code>kubernetes.io/enable-service-links</code> annotation is set to false, which will disable service link environment variables injection into this pod.</p> <p>To make this change a default setting for all pods in a namespace, you can update the PodTemplateSpec:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-container\n        image: my-image\n      podTemplateSpec:\n        spec:\n          containers:\n          - name: my-container\n            image: my-image\n          annotations:\n            kubernetes.io/enable-service-links: \"false\"\n</code></pre>","tags":["kubernetes","service-links"]},{"location":"2026-01-10-set-enableservicelinks-to-false-as-default/#conclusion","title":"Conclusion","text":"<p>By setting <code>enableServiceLinks</code> to false by default, we can reduce the risk of environmental variable overload and potential pod crashes in larger namespaces. This change is a crucial step towards improving the overall stability and performance of Kubernetes clusters.</p>","tags":["kubernetes","service-links"]},{"location":"2026-01-10-set-enableservicelinks-to-false-as-default/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["kubernetes","service-links"]},{"location":"2026-01-10-support-closed-file-diagnostics-in-vs-code/","title":"Support Closed-File Diagnostics in VS Code","text":"","tags":["TypeScript","VS Code"]},{"location":"2026-01-10-support-closed-file-diagnostics-in-vs-code/#core-problem","title":"Core Problem","text":"<p>Project wide diagnostics are a highly requested feature for VS Code, especially for JavaScript and TypeScript projects. However, the single-threaded nature of the TS server prevents this from being implemented seamlessly. This limitation can interrupt normal user operations like completions.</p>","tags":["TypeScript","VS Code"]},{"location":"2026-01-10-support-closed-file-diagnostics-in-vs-code/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we need to find an API that provides inverse dependencies (files that need to be rechecked after a file changes) for a given file. The <code>compileOnSaveAffectedFileList</code> command seems promising, but it requires <code>compileOnSave</code> to be enabled, which is not ideal for users who don't have this feature enabled.</p> <p>We can use the <code>geterrForProject</code> command with some modifications to get around this limitation. However, this approach has its drawbacks, especially for larger projects where explicit file openings become necessary.</p> <p>To improve this situation, we need a more efficient API that provides project-wide error reporting even when <code>compileOnSave</code> is not enabled. This can be achieved by using a separate diagnostics-only server.</p>","tags":["TypeScript","VS Code"]},{"location":"2026-01-10-support-closed-file-diagnostics-in-vs-code/#code-example-creating-a-diagnostics-only-server","title":"Code Example: Creating a Diagnostics-Only Server","text":"<p>We can create a new settings.json file in the <code>.vscode</code> directory with the following configuration: <pre><code>{\n  \"typescript.diagnosticsServer\": {\n    \"startOnOpen\": false,\n    \"openInNewProcess\": true\n  }\n}\n</code></pre> This will spin up a new TS server for diagnostics, which can be used to get inverse dependencies for files.</p>","tags":["TypeScript","VS Code"]},{"location":"2026-01-10-support-closed-file-diagnostics-in-vs-code/#code-example-using-the-compileonsaveaffectedfilelist-command","title":"Code Example: Using the <code>compileOnSaveAffectedFileList</code> Command","text":"<p>We can use the <code>compileOnSaveAffectedFileList</code> command to get a list of affected files. However, this command requires <code>compileOnSave</code> to be enabled. <pre><code>import * as vscode from 'vscode';\nimport { compileOnSaveAffectedFileList } from 'typescript';\n\nfunction getAffectedFiles(file: vscode.Uri): Promise&lt;string[]&gt; {\n  return new Promise((resolve) =&gt; {\n    const affectedFiles = compileOnSaveAffectedFileList();\n    resolve(affectedFiles.filter((file) =&gt; file.path.startsWith(file.path)));\n  });\n}\n</code></pre> This code example demonstrates how to use the <code>compileOnSaveAffectedFileList</code> command to get a list of affected files.</p>","tags":["TypeScript","VS Code"]},{"location":"2026-01-10-support-closed-file-diagnostics-in-vs-code/#conclusion","title":"Conclusion","text":"<p>Implementing project-wide diagnostics in VS Code requires an efficient API that provides inverse dependencies for files. By using a separate diagnostics-only server and modifying existing commands, we can achieve this feature without relying on <code>compileOnSave</code>. This solution improves the overall user experience and makes it easier to implement project-wide error reporting.</p>","tags":["TypeScript","VS Code"]},{"location":"2026-01-10-support-closed-file-diagnostics-in-vs-code/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["TypeScript","VS Code"]},{"location":"2026-01-10-support-for-internvl-chat-v15/","title":"Supporting InternVL-Chat-V1.5 in Hugging Face Models","text":"","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-support-for-internvl-chat-v15/#core-problem","title":"Core Problem","text":"<p>The gap between open-source and proprietary commercial models in multimodal understanding is a significant challenge in the field of natural language processing (NLP). The need for more capable and transferable models has led to the introduction of InternVL 1.5, an open-source multimodal large language model.</p>","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-support-for-internvl-chat-v15/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To support InternVL-Chat-V1.5 in Hugging Face models, you can follow these steps:</p>","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-support-for-internvl-chat-v15/#step-1-install-required-libraries","title":"Step 1: Install Required Libraries","text":"<pre><code>pip install transformers torch torchvision\n</code></pre>","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-support-for-internvl-chat-v15/#step-2-load-the-model","title":"Step 2: Load the Model","text":"<pre><code>from transformers import LLAMACheckpointReader\n\nmodel_name = \"OpenGVLab/InternVL-Chat-V1.5\"\n\nreader = LLAMACheckpointReader(model_name)\n</code></pre>","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-support-for-internvl-chat-v15/#step-3-fine-tune-the-model-optional","title":"Step 3: Fine-Tune the Model (Optional)","text":"<p>To fine-tune the model on your specific dataset, you can use the following code:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"OpenGVLab/InternVL-Chat-V1.5\")\nmodel = AutoModelForCausalLM.from_pretrained(\"OpenGVLab/InternVL-Chat-V1.5\")\n\n# Your dataset and labels here\ntrain_dataset = ...  # Your dataset loader\nlabels = ...  # Your label values\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Fine-tuning the model\noptimizer = AdamW(model.parameters(), lr=1e-5)\nfor epoch in range(5):\n    for batch in train_dataset:\n        input_ids = tokenizer.encode(batch[\"input_text\"], return_tensors=\"pt\").to(device)\n        attention_mask = tokenizer.encode(batch[\"input_text\"], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True).to(device)\n        labels = torch.tensor(batch[\"labels\"]).to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n\n        # Backward and optimize\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n</code></pre>","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-support-for-internvl-chat-v15/#conclusion","title":"Conclusion","text":"<p>Supporting InternVL-Chat-V1.5 in Hugging Face models can be achieved by following these steps. By leveraging the capabilities of this open-source multimodal large language model, you can bridge the capability gap between open-source and proprietary commercial models in multimodal understanding.</p>","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-support-for-internvl-chat-v15/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["multimodal large language models","open-source models","proprietary commercial models"]},{"location":"2026-01-10-terraform-014--1131-apply-re-creates-existing-aws-resources-vpcsubnetsnat-instead-of-updating-despite-s3-remote-state/","title":"Upgrading Terraform from v0.14.x to v1.13.1: A Cautionary Tale","text":"","tags":["Terraform","AWS","S3","Remote State"]},{"location":"2026-01-10-terraform-014--1131-apply-re-creates-existing-aws-resources-vpcsubnetsnat-instead-of-updating-despite-s3-remote-state/#core-problem","title":"Core Problem","text":"<p>When upgrading from Terraform v0.14.x to v1.13.1, users may notice that <code>terraform apply</code> creates entirely new resources instead of updating existing ones, even when using a S3 remote state backend.</p>","tags":["Terraform","AWS","S3","Remote State"]},{"location":"2026-01-10-terraform-014--1131-apply-re-creates-existing-aws-resources-vpcsubnetsnat-instead-of-updating-despite-s3-remote-state/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, it's essential to understand the changes in Terraform v1.13.1 and how they affect the behavior of <code>terraform apply</code>. Specifically, the introduction of a new <code>terraform.tfstate</code> file format in v1.10.x has caused some providers to behave differently.</p> <p>In your case, the AWS provider is affected by this change. To fix the issue, you need to update your Terraform configuration to use the new <code>terraform.tfstate</code> file format.</p> <p>Here's an example of how to modify your <code>backend</code> block: <pre><code>data \"terraform_remote_state\" \"state\" {\n  backend = \"s3\"\n  config = {\n    bucket         = var.backend_config_bucket\n    region         = var.backend_config_bucket_region\n    key            = \"${var.name}/${var.backend_config_tfstate_file_key}\"\n    dynamodb_table = \"terraform_locks\"\n    encrypt        = true\n  }\n}\n</code></pre> Make sure to update the <code>key</code> field to use the new <code>terraform.tfstate</code> file format.</p> <p>Additionally, you may need to modify your <code>terraform.tfstate.discard</code> block to ensure that Terraform properly discards old state files: <pre><code>terraform {\n  # ... other settings ...\n  tfstate_discard = true\n}\n</code></pre></p>","tags":["Terraform","AWS","S3","Remote State"]},{"location":"2026-01-10-terraform-014--1131-apply-re-creates-existing-aws-resources-vpcsubnetsnat-instead-of-updating-despite-s3-remote-state/#conclusion","title":"Conclusion","text":"<p>When upgrading from Terraform v0.14.x to v1.13.1, it's essential to understand the changes in the provider behavior and update your configuration accordingly. By using the new <code>terraform.tfstate</code> file format and modifying your <code>backend</code> block, you can ensure that <code>terraform apply</code> updates existing resources instead of creating duplicates.</p> <p>Code Examples</p> <pre><code>data \"terraform_remote_state\" \"state\" {\n  backend = \"s3\"\n  config = {\n    bucket         = var.backend_config_bucket\n    region         = var.backend_config_bucket_region\n    key            = \"${var.name}/${var.backend_config_tfstate_file_key}\"\n    dynamodb_table = \"terraform_locks\"\n    encrypt        = true\n  }\n}\n\nterraform {\n  tfstate_discard = true\n}\n</code></pre> <p><pre><code>provider \"aws\" {\n  region = var.region\n  assume_role {\n    role_arn = \"arn:aws:iam::${var.target_account_id}:role/terraform\"\n  }\n}\n</code></pre> Note: The code examples provided are for illustration purposes only and may need to be modified to fit your specific use case.</p>","tags":["Terraform","AWS","S3","Remote State"]},{"location":"2026-01-10-terraform-014--1131-apply-re-creates-existing-aws-resources-vpcsubnetsnat-instead-of-updating-despite-s3-remote-state/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Terraform","AWS","S3","Remote State"]},{"location":"2026-01-10-tracking-issue-for-inherent-unchecked-integer-methods/","title":"Inherent Unchecked Integer Methods in Rust","text":"","tags":["integer-powers","performance-optimization","rust-language-features"]},{"location":"2026-01-10-tracking-issue-for-inherent-unchecked-integer-methods/#core-problem","title":"Core Problem","text":"<p>The <code>unchecked_*</code> methods on integers have been implemented as part of the Rust language, but their inherent nature has raised questions about their stability and usability.</p>","tags":["integer-powers","performance-optimization","rust-language-features"]},{"location":"2026-01-10-tracking-issue-for-inherent-unchecked-integer-methods/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code>impl u32 {\n    pub const unsafe fn unchecked_add(self, rhs: u32) -&gt; u32;\n    pub const unsafe fn unchecked_sub(self, u32) -&gt; u32;\n    pub const unsafe fn unchecked_mul(self, u32) -&gt; u32;\n}\nimpl i32 {\n    pub const unsafe fn unchecked_add(self, rhs: i32) -&gt; i32;\n    pub const unsafe fn unchecked_sub(self, i32) -&gt; i32;\n    pub const unsafe fn unchecked_mul(self, i32) -&gt; i32;\n}\n\n// ...\n\nimpl u64 {\n    pub const unsafe fn unchecked_add(self, rhs: u64) -&gt; u64;\n    pub const unsafe fn unchecked_sub(self, u64) -&gt; u64;\n    pub const unsafe fn unchecked_mul(self, u64) -&gt; u64;\n}\n</code></pre> <p>The <code>unchecked_*</code> methods are used to perform operations on integers without checking for overflow or underflow. However, this comes at the cost of safety and reliability.</p>","tags":["integer-powers","performance-optimization","rust-language-features"]},{"location":"2026-01-10-tracking-issue-for-inherent-unchecked-integer-methods/#conclusion","title":"Conclusion","text":"<p>The implementation of inherent <code>unchecked_*</code> methods in Rust provides a performance boost for certain use cases, but it also raises questions about their stability and usability. As the language continues to evolve, it is essential to carefully consider the implications of such features and ensure that they align with the goals of the Rust project.</p>","tags":["integer-powers","performance-optimization","rust-language-features"]},{"location":"2026-01-10-tracking-issue-for-inherent-unchecked-integer-methods/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["integer-powers","performance-optimization","rust-language-features"]},{"location":"2026-01-10-typescript-type-annotations-as-comments/","title":"Enabling TypeScript Type Checking in JavaScript Projects with Inline Comments","text":"","tags":["Typescript","JavaScript","Inline Comments"]},{"location":"2026-01-10-typescript-type-annotations-as-comments/#core-problem","title":"Core Problem","text":"<p>Typescript's primary goal is to be Javascript + Types, but many projects are written in plain Javascript and still benefit from a type checker. Currently, types from JSdoc comments can be parsed by Typescript, but it would be more convenient to annotate existing Javascript code with types without the need for an emit stage.</p>","tags":["Typescript","JavaScript","Inline Comments"]},{"location":"2026-01-10-typescript-type-annotations-as-comments/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>Typescript's language service is a significant advantage here, as it allows on-the-fly error checking and code completion. To achieve this functionality, inline type annotations using Flow comments can be used. The syntax for Flow comments is <code>_:_ [type]</code> or <code>_:: [tscode]</code>.</p> <p>Here's an example of how to use Flow comments in a Typescript class:</p> <pre><code>/// &lt;reference path=\"...\" /&gt;\n\nclass Hello {\n    /*:: private*/ hello(message /*: string*/) /*: Promise&lt;{}&gt;*/{\n        const promise /*: Promise&lt;void&gt;*/ = null \n    return /*:: &lt;Promise&lt;{}&gt;&gt;*/ null;\n}\n</code></pre> <p>This approach allows for the same code to be read by both Browser and Node.js without the need for source mapping or a separate transpilation step. Additionally, since the types are included in the source file, there is no need for a separate typings installation for Node_modules written in this format.</p>","tags":["Typescript","JavaScript","Inline Comments"]},{"location":"2026-01-10-typescript-type-annotations-as-comments/#conclusion","title":"Conclusion","text":"<p>Enabling TypeScript type checking in existing Javascript projects using inline comments provides a convenient solution for developers who want to leverage Typescript's benefits without having to refactor their entire codebase. With Flow comments and Typescript's language service, on-the-fly error checking and code completion are now available for annotated Javascript files.</p>","tags":["Typescript","JavaScript","Inline Comments"]},{"location":"2026-01-10-typescript-type-annotations-as-comments/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Typescript","JavaScript","Inline Comments"]},{"location":"2026-01-11-a-way-to-update-all-downloaded-models/","title":"Automate Model Updates with Ollama","text":"","tags":["model-management","automation"]},{"location":"2026-01-11-a-way-to-update-all-downloaded-models/#core-problem","title":"Core Problem","text":"<p>Updating individual models within the Ollama framework can be time-consuming and prone to human error. Currently, users must pull each model separately, which can lead to inefficiencies in managing large collections of models.</p>","tags":["model-management","automation"]},{"location":"2026-01-11-a-way-to-update-all-downloaded-models/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["model-management","automation"]},{"location":"2026-01-11-a-way-to-update-all-downloaded-models/#batch-model-update-script","title":"Batch Model Update Script","text":"<p>You can update all downloaded models using a single bash script:</p> <pre><code>#!/bin/bash\n\n# Get the list of models (skipping the first entry)\nollama list | tail -n +2 | awk '{print $1}' | while read -r model; do\n  # Pull each model individually\n  ollama pull $model\ndone\n</code></pre> <p>This script leverages Ollama's <code>list</code> command to fetch a list of models, filters out the first entry (assuming it's not a model), and then uses an <code>awk</code> to extract the model names. The resulting list is then piped into a <code>while</code> loop, which iterates over each model and pulls it using Ollama's <code>pull</code> command.</p>","tags":["model-management","automation"]},{"location":"2026-01-11-a-way-to-update-all-downloaded-models/#benefits-of-batch-updates","title":"Benefits of Batch Updates","text":"<ul> <li>Efficiency: Automating the update process saves time and reduces manual labor.</li> <li>Consistency: Ensuring all models are updated simultaneously maintains consistency across your project or organization.</li> <li>Error Reduction: Minimizing human involvement decreases the likelihood of errors during the update process.</li> </ul>","tags":["model-management","automation"]},{"location":"2026-01-11-a-way-to-update-all-downloaded-models/#conclusion","title":"Conclusion","text":"<p>By utilizing a batch update script, you can efficiently manage your Ollama model collections and streamline your workflow. This approach not only saves time but also improves accuracy and consistency in your model updates.</p>","tags":["model-management","automation"]},{"location":"2026-01-11-a-way-to-update-all-downloaded-models/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["model-management","automation"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/","title":"Simplifying Task Scheduling in Tokio with LocalRuntime","text":"","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/#core-problem","title":"Core Problem","text":"<p>Currently, spawning <code>!Send</code> tasks on Tokio requires combining a current-thread runtime and <code>LocalSet</code>, leading to performance overhead and unexpected behavior.</p>","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/#introducing-localruntime","title":"Introducing LocalRuntime","text":"<p>To address the issue, we propose introducing a new type called <code>LocalRuntime</code>. This type behaves like a current-thread <code>Runtime</code>, but with additional features:</p> <pre><code>struct LocalRuntime {\n    // ...\n}\n\nimpl Runtime for LocalRuntime {\n    fn new() -&gt; Self {\n        // ...\n    }\n\n    // ...\n}\n\nimpl LocalSet for LocalRuntime {\n    // ...\n}\n</code></pre>","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/#benefits-of-localruntime","title":"Benefits of LocalRuntime","text":"<p>The proposed <code>LocalRuntime</code> has several benefits:</p> <ul> <li>It is <code>!Send</code>, allowing direct spawning of <code>!Send</code> tasks onto the runtime.</li> <li>Within a <code>LocalRuntime</code>, <code>tokio::spawn</code> and <code>spawn_local</code> have identical behavior, making it easier to manage tasks.</li> <li>Tasks spawned using <code>spawn_local</code> are scheduled in the same way as normal <code>tokio::spawn</code> tasks.</li> </ul> <pre><code>struct LocalSet {\n    runtime: LocalRuntime,\n}\n\nimpl LocalSet for LocalRuntime {\n    fn new() -&gt; Self {\n        LocalRuntime { /* ... */ }\n    }\n\n    // ...\n}\n</code></pre>","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/#comparison-with-current-implementation","title":"Comparison with Current Implementation","text":"<p>The proposed <code>LocalRuntime</code> provides a more efficient and intuitive way to manage tasks in Tokio. By introducing this new type, we can simplify the task scheduling process and reduce performance overhead.</p>","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/#conclusion","title":"Conclusion","text":"<p>Replacing the current runtime+local set implementation with <code>LocalRuntime</code> offers several benefits, including improved efficiency, reduced complexity, and enhanced developer experience. This proposal aims to enhance the overall Tokio ecosystem and provide a better foundation for building high-performance asynchronous applications.</p>","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-add-a-localruntime-for-replacing-runtimelocalset/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["async programming","tokio-rs"]},{"location":"2026-01-11-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/","title":"AIOps: AI-Driven Operations for Enhanced Network Performance","text":"","tags":["Network Optimization","Artificial Intelligence"]},{"location":"2026-01-11-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#core-problem","title":"Core Problem","text":"<p>Traditional network management is a time-consuming and manual process, relying heavily on human expertise to identify issues and make adjustments. This leads to reduced network performance, increased downtime, and higher operational costs.</p>","tags":["Network Optimization","Artificial Intelligence"]},{"location":"2026-01-11-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, AIOps (Artificial Intelligence-Driven Operations) emerges as a game-changer. By leveraging AI and machine learning algorithms, AIOps can analyze vast amounts of network data in real-time, identifying patterns and anomalies that human analysts might miss.</p> <pre><code>use std::collections::{HashMap, HashSet};\nuse std::str;\n\n// Define a simple network device model\nstruct Device {\n    id: String,\n    status: String,\n}\n\nimpl Device {\n    fn new(id: String, status: String) -&gt; Self {\n        Self { id, status }\n    }\n\n    fn update_status(&amp;mut self, new_status: String) {\n        self.status = new_status;\n    }\n}\n\n// Define a simple network data model\nstruct NetworkData {\n    devices: HashMap&lt;String, Device&gt;,\n    traffic: Vec&lt;String&gt;,\n}\n\nimpl NetworkData {\n    fn new() -&gt; Self {\n        Self {\n            devices: HashMap::new(),\n            traffic: Vec::new(),\n        }\n    }\n\n    fn add_device(&amp;mut self, device: Device) {\n        self.devices.insert(device.id.clone(), device);\n    }\n\n    fn add_traffic(&amp;mut self, traffic: String) {\n        self.traffic.push(traffic);\n    }\n}\n\n// Define an AIOps algorithm to analyze network data\nfn aiops_algorithm(network_data: &amp;NetworkData) -&gt; Vec&lt;String&gt; {\n    let mut issues = Vec::new();\n\n    // Analyze device status\n    for (device_id, device) in network_data.devices.iter() {\n        if device.status == \"down\" {\n            issues.push(format!(\"Device {} is down\", device_id));\n        }\n    }\n\n    // Analyze traffic patterns\n    let mut traffic_set = HashSet::new();\n    for traffic in network_data.traffic.iter() {\n        traffic_set.insert(traffic.clone());\n    }\n\n    if traffic_set.len() &gt; 1000 {\n        issues.push(\"High traffic detected\".to_string());\n    }\n\n    issues\n}\n\nfn main() {\n    // Create a sample network data model\n    let mut network_data = NetworkData::new();\n\n    // Add devices to the network\n    network_data.add_device(Device::new(\"device1\".to_string(), \"up\".to_string()));\n    network_data.add_device(Device::new(\"device2\".to_string(), \"down\".to_string()));\n\n    // Add traffic patterns to the network\n    network_data.add_traffic(\"traffic1\".to_string());\n    network_data.add_traffic(\"traffic2\".to_string());\n    network_data.add_traffic(\"traffic3\".to_string());\n\n    // Run AIOps algorithm on the network data\n    let issues = aiops_algorithm(&amp;network_data);\n\n    // Print the issues detected by AIOps\n    for issue in issues {\n        println!(\"{}\", issue);\n    }\n}\n</code></pre>","tags":["Network Optimization","Artificial Intelligence"]},{"location":"2026-01-11-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#conclusion","title":"Conclusion","text":"<p>By leveraging AIOps, organizations can automate their network management processes, reducing manual effort and improving overall performance. The solution analyzes vast amounts of data in real-time, identifying patterns and anomalies that human analysts might miss. This leads to faster incident resolution, reduced downtime, and lower operational costs.</p>","tags":["Network Optimization","Artificial Intelligence"]},{"location":"2026-01-11-additional-single-byte-write-syscall-when-future-is-not-spawned-before-block_on-with-current_thread-runtime/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Network Optimization","Artificial Intelligence"]},{"location":"2026-01-11-allow-scaling-up-to-meet-pdb-constraints/","title":"Allowing Kubernetes Horizontal Pod Autoscaler to Meet Pod Disruption Budget Constraints","text":"","tags":["pod-disruption-budget","horizontal-pod-autoscaling"]},{"location":"2026-01-11-allow-scaling-up-to-meet-pdb-constraints/#core-problem","title":"Core Problem","text":"<p>Currently, if you have a Pod Disruption Budget (PDB) spec like <code>minAvailable: 1</code>, and a Horizontal Pod Autoscaler (HPA) defining <code>minReplicas: 1</code> and <code>maxReplicas: N</code>, you may end up in a scenario where disruptions get \"stuck\" if the HPA has scaled to 1. This can lead to weird scenarios where a disruption can only occur if there is high load.</p>","tags":["pod-disruption-budget","horizontal-pod-autoscaling"]},{"location":"2026-01-11-allow-scaling-up-to-meet-pdb-constraints/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to modify the HPA so it scales up to meet PDB constraints instead of getting stuck at a minimum replica count. We can achieve this by modifying the <code>ScalingPolicy</code> object in the <code>HPA</code> configuration.</p> <pre><code>apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-deployment\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n  scalingPolicy:\n    policyType: PodDisruptionBudget\n    podDisruptionBudgetSpec:\n      minAvailable: 1\n</code></pre> <p>However, the above approach does not scale up replicas immediately when a disruption occurs. To achieve this, we need to use the <code>PodDisruptionBudget</code> API and implement a custom autoscaler.</p> <pre><code>apiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: my-pdb\nspec:\n  minAvailable: 1\n  maxUnavailable: 0\n\n---\n\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-deployment\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n  autoscalerSettings:\n    podDisruptionBudgetName: my-pdb\n</code></pre>","tags":["pod-disruption-budget","horizontal-pod-autoscaling"]},{"location":"2026-01-11-allow-scaling-up-to-meet-pdb-constraints/#conclusion","title":"Conclusion","text":"<p>By implementing a custom <code>PodDisruptionBudget</code> and modifying the <code>HorizontalPodAutoscaler</code> to use it, we can allow the HPA to scale up replicas immediately when a disruption occurs, ensuring that our application meets the PDB constraints.</p>","tags":["pod-disruption-budget","horizontal-pod-autoscaling"]},{"location":"2026-01-11-allow-scaling-up-to-meet-pdb-constraints/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pod-disruption-budget","horizontal-pod-autoscaling"]},{"location":"2026-01-11-build-hang-on-linux/","title":"Build Hangs on Linux with Nightly Toolchain","text":"","tags":["rust-lang","build-hangs","linux","nightly-toolchain"]},{"location":"2026-01-11-build-hang-on-linux/#core-problem","title":"Core Problem","text":"<p>The build process for the <code>hypermine</code> repository hangs at the last few steps when using the nightly toolchain on Linux. This issue is reproducible and has been observed both locally on Linux Mint and in GitHub's CI environment.</p>","tags":["rust-lang","build-hangs","linux","nightly-toolchain"]},{"location":"2026-01-11-build-hang-on-linux/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To reproduce the issue, follow these steps:</p> <ol> <li>Switch to the nightly toolchain: <pre><code>$ rustup default nightly\n</code></pre></li> <li>Fetch the <code>hypermine</code> repository: <pre><code>$ git clone https://github.com/danieldeankon/hypermine.git\n$ cd hypermine/\n$ git lfs pull\n</code></pre></li> <li>Attempt to build the binaries with verbose output: <pre><code>$ cargo build --release -v\n</code></pre></li> </ol> <p>The last few lines of the output before the build hangs are:</p> <pre><code>   Compiling client v0.1.0 (/home/daniel/git/testinghypermine/hypermine/client)\n     Running `rustc --crate-name client --edition=2018 client/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use-repo-assets\"' -C metadata=c38d85b37e3e2e40 -C extra-filename=-c38d85b37e3e2e40 --out-dir /home/daniel/git/testinghypermine/hypermine/target/release/deps -L dependency=/home/daniel/git/testinghypermine/hypermine/target/release/deps --extern anyhow=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libanyhow-43ee26a5a4ae988b.rmeta --extern ash=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libash-1d91e46c2845798f.rmeta --extern ash_window=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libash_window-e581873b9287fab4.rmeta --extern common=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libcommon-bdb19128efefc960.rmeta --extern directories=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libdirectories-4a64043fdaf679b9.rmeta --extern downcast_rs=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libdowncast_rs-be493f8657cf1f42.rmeta --extern futures_util=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libfutures_util-79782c1686738829.rmeta --extern fxhash=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libfxhash-aa9a2aa3814ab312.rmeta --extern gltf=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libgltf-ae78e76833b667c0.rmeta --extern hdrhistogram=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libhdrhistogram-1c5607d77dc61a11.rmeta --extern hecs=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libhecs-7ceed138b109be7b.rmeta --extern lahar=/home/daniel/git/testinghypermine/hypermine/target/release/deps/liblahar-c3a4855d049ab353.rmeta --extern memoffset=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libmemoffset-33ada65033678cfe.rmeta --extern metrics=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libmetrics-79a79d3342c8701e.rmeta --extern metrics_core=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libmetrics_core-7e657b2c60f0cf49.rmeta --extern na=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libnalgebra-615d0479649eed30.rmeta --extern png=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libpng-089627b494fcd05d.rmeta --extern quinn=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libquinn-a3a9da2f9e31ef73.rmeta --extern rcgen=/home/daniel/git/testinghypermine/hypermine/target/release/deps/librcgen-be435a028b2dfce2.rmeta --extern rustls=/home/daniel/git/testinghypermine/hypermine/target/release/deps/librustls-825cbe6b34245b7a.rmeta --extern serde=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libserde-fbaa982a65688d4a.rmeta --extern server=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libserver-8fd1f97c8fb84524.rmeta --extern tokio=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libtokio-aa180fba706594fa.rmeta --extern toml=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libtoml-f7f6190f25a07c8e.rmeta --extern tracing=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libtracing-e0bf1ed6aae74c7d.rmeta --extern vk_shader_macros=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libvk_shader_macros-d2fb7777a36a4f79.rmeta --extern webpki=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libwebpki-66dacb92629212d5.rmeta --extern whoami=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libwhoami-05f81a39d2c15f6f.rmeta --extern winit=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libwinit-0172bcc7807380da.rmeta -L native=/home/daniel/git/testinghypermine/hypermine/target/release/build/ring-86d47ac3f01cd916/out\n</code></pre>","tags":["rust-lang","build-hangs","linux","nightly-toolchain"]},{"location":"2026-01-11-build-hang-on-linux/#conclusion","title":"Conclusion","text":"<p>The build process for the <code>hypermine</code> repository hangs at the last few steps when using the nightly toolchain on Linux. The issue has been bisected to #73526, and further investigation is required to determine the cause of this issue.</p> <p>Note: This article is a direct copy from the GitHub issue page with minor formatting changes.</p>","tags":["rust-lang","build-hangs","linux","nightly-toolchain"]},{"location":"2026-01-11-build-hang-on-linux/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust-lang","build-hangs","linux","nightly-toolchain"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/","title":"Shared Mutable State in Pandas Index Labels","text":"<p>When using the <code>copy_on_write</code> feature in pandas, a common issue arises with shared mutable state across Series and DataFrame (and Index). This can lead to unexpected behavior and inconsistencies.</p>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#core-problem","title":"Core Problem","text":"<p>When you enable <code>copy_on_write</code>, you might expect that index labels are treated independently between Series and DataFrames. However, this is not the case. The mutated index label still shares mutable state with both the original DataFrame and any subsequent Series created from it.</p>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To demonstrate this issue, let's create a simple example:</p> <pre><code>import pandas as pd\n\npd.options.mode.copy_on_write = True\n\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\nprint(df)\n</code></pre> a 0 1 1 2 2 3","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#this-series-shares-an-index-object-with-dataframe","title":"This Series shares an Index object with DataFrame","text":"<pre><code>s1 = df[\"a\"]\nprint(s1)\n</code></pre> <pre><code>0     1\n1     2\n2     3\n</code></pre> <p>Name: a, dtype: int64</p>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#mutating-a-series-breaks-the-link-to-the-dataframe-index","title":"Mutating a Series breaks the link to the DataFrame index","text":"<pre><code>s2 = df[\"a\"]\ns2.loc[1] = 100\nprint(s2)\n</code></pre> <pre><code>0      1\n1   100\n2      3\n</code></pre> <p>Name: a, dtype: int64</p>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#question-should-this-index-object-be-treated-independently-from-dataframe-or-is-it-okay-that-a-mutation-here-affects-the-dataframe","title":"Question: Should this Index object be treated independently from DataFrame or is it okay that a mutation here affects the DataFrame?","text":"<pre><code>i = df.index\ni.name = \"new name\"\nprint(df.index.name)\n</code></pre> <pre><code>new name\n</code></pre>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#this-series-hasnt-been-mutated-it-shares-an-index-object-with-dataframe","title":"This Series hasn't been mutated. It shares an Index object with DataFrame.","text":"<pre><code>print(s1.index.name)\n</code></pre> <pre><code>new name\n</code></pre>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#this-series-has-been-mutated-it-doesnt-share-an-index-object-with-dataframe","title":"This Series has been mutated. It doesn't share an Index object with DataFrame.","text":"<pre><code>print(s2.index.name)\n</code></pre> <pre><code>None\n</code></pre> <pre><code>s1.loc[1] = 100\n</code></pre>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#conclusion","title":"Conclusion","text":"<p>When using <code>copy_on_write</code> in pandas, it's essential to understand how index labels are shared between Series and DataFrames. This can lead to unexpected behavior if not handled correctly. By being aware of this issue, you can take steps to ensure your data manipulation code produces the expected results.</p>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-copy-on-write-cow-index-labels-are-still-shared-mutable-state/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","copy-on-write","data manipulation"]},{"location":"2026-01-11-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/","title":"[Use the Title]","text":"","tags":["langchain-chroma","error-message","chromadb"]},{"location":"2026-01-11-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#core-problem","title":"Core Problem","text":"<p>The user is experiencing an error when using ChromaDB, specifically a <code>ValueError</code> with the message \"Could not connect to tenant default_tenant. Are you sure it exists?\"</p>","tags":["langchain-chroma","error-message","chromadb"]},{"location":"2026-01-11-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to ensure that the ChromaDB tenant exists and is properly configured.</p> <p>Firstly, let's check if the tenant exists by looking at the configuration code: <pre><code>vector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n)\n</code></pre> In this case, the tenant name is <code>default_tenant</code>. We need to verify that this tenant exists in ChromaDB.</p> <p>To do this, we can check the ChromaDB documentation or contact their support team. It's possible that the tenant was deleted or renamed accidentally.</p> <p>Once we confirm that the tenant exists, we need to update the configuration code to use the correct tenant name: <pre><code>vector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n    tenant=\"default_tenant\"  # Update the tenant name\n)\n</code></pre> By making this change, we should be able to resolve the error and use ChromaDB successfully.</p>","tags":["langchain-chroma","error-message","chromadb"]},{"location":"2026-01-11-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#conclusion","title":"Conclusion","text":"<p>To avoid this issue in the future, make sure to verify that your ChromaDB tenant exists and is properly configured. If you're unsure, consult their documentation or contact their support team for assistance.</p>","tags":["langchain-chroma","error-message","chromadb"]},{"location":"2026-01-11-error-from-using-chromadb---valueerror-could-not-connect-to-tenant-default_tenant-are-you-sure-it-exists/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["langchain-chroma","error-message","chromadb"]},{"location":"2026-01-11-internalerror-too-much-recursion-on-firefox/","title":"InternalError: too much recursion on Firefox","text":"","tags":["Next.js","Error Handling"]},{"location":"2026-01-11-internalerror-too-much-recursion-on-firefox/#core-problem","title":"Core Problem","text":"<p>The <code>InternalError: too much recursion</code> error occurs when a function calls itself excessively without a base case, causing the call stack to overflow. In the context of Next.js, this issue can be particularly problematic due to its use of dynamic routing and server-side rendering.</p>","tags":["Next.js","Error Handling"]},{"location":"2026-01-11-internalerror-too-much-recursion-on-firefox/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve the <code>InternalError: too much recursion</code> issue in Next.js, we need to identify and fix the recursive function calls that are causing the error.</p>","tags":["Next.js","Error Handling"]},{"location":"2026-01-11-internalerror-too-much-recursion-on-firefox/#using-debug-middleware","title":"Using <code>debug</code> middleware","text":"<p>One common solution is to enable debug logging for the App Router. This can help identify issues with recursive function calls.</p> <pre><code>export default function MyApp({ Component, pageProps }) {\n  return (\n    &lt;Layout&gt;\n      &lt;Component {...pageProps} /&gt;\n    &lt;/Layout&gt;\n  )\n}\n</code></pre> <pre><code>// pages/_app.js\nimport { NextPage } from 'next';\nimport App from '../components/App';\n\nconst MyApp = ({ Component, pageProps }: NextPage) =&gt; {\n  // Enable debug logging for the App Router\n  const debugMiddleware = (req, res, next) =&gt; {\n    process.env.DEBUG ? console.log(req.url + ' ' + req.method) : null;\n    next();\n  };\n\n  return (\n    &lt;App&gt;\n      &lt;Component {...pageProps} /&gt;\n    &lt;/App&gt;\n  );\n};\n\nexport default MyApp;\n\n// pages/_middleware.js\nimport { NextApiRequest, NextApiResponse } from 'next';\nimport { debugMiddleware } from '../_app';\n\nconst debugMiddlewareFn = (req: NextApiRequest, res: NextApiResponse) =&gt; {\n  // Enable debug logging for the App Router\n  if (process.env.DEBUG) {\n    console.log(req.url + ' ' + req.method);\n  }\n};\n\nexport default debugMiddleware;\n</code></pre>","tags":["Next.js","Error Handling"]},{"location":"2026-01-11-internalerror-too-much-recursion-on-firefox/#handling-recursive-function-calls","title":"Handling recursive function calls","text":"<p>To handle recursive function calls in your code, you can use techniques such as:</p> <ul> <li>Adding a base case to prevent infinite recursion</li> <li>Using memoization or caching to store results of expensive function calls</li> <li>Implementing an iterative solution instead of a recursive one</li> </ul> <pre><code>// pages/api/endpoint.js\nimport { NextApiRequest, NextApiResponse } from 'next';\n\nconst endpoint = async (req: NextApiRequest, res: NextApiResponse) =&gt; {\n  // Handle request and return response\n};\n\nexport default endpoint;\n</code></pre>","tags":["Next.js","Error Handling"]},{"location":"2026-01-11-internalerror-too-much-recursion-on-firefox/#conclusion","title":"Conclusion","text":"<p>The <code>InternalError: too much recursion</code> error can be challenging to resolve, but by enabling debug logging for the App Router and handling recursive function calls effectively, you can identify and fix issues causing this error.</p>","tags":["Next.js","Error Handling"]},{"location":"2026-01-11-internalerror-too-much-recursion-on-firefox/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Next.js","Error Handling"]},{"location":"2026-01-11-is-there-any-plan-to-support-fingpt/","title":"Will FinGPT Be Supported by Ollama Soon?","text":"","tags":["natural-language-processing","machine-learning-models"]},{"location":"2026-01-11-is-there-any-plan-to-support-fingpt/#core-problem","title":"Core Problem","text":"<p>FinGPT, an open-source finite difference GPT-based model for financial applications, has gained popularity in recent times. However, its usage is limited to specific models and datasets due to the lack of official support from the Ollama team.</p>","tags":["natural-language-processing","machine-learning-models"]},{"location":"2026-01-11-is-there-any-plan-to-support-fingpt/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To determine if FinGPT will be supported by Ollama soon, we need to analyze the current state of Ollama's development roadmap and community engagement.</p> <pre><code>import requests\n\n# Check Ollama's GitHub repository for updates on FinGPT support\nresponse = requests.get(\"https://api.github.com/repos/ollama/ollama/issues\")\nissues = response.json()\n\n# Filter issues related to FinGPT support\nfin_gpt_issues = [issue[\"title\"] for issue in issues if \"FinGPT\" in issue[\"title\"]]\n\nif fin_gpt_issues:\n    print(f\"Found {len(fin_gpt_issues)} issues related to FinGPT support.\")\nelse:\n    print(\"No issues found related to FinGPT support.\")\n\n# Check Ollama's community engagement on GitHub discussions\ndiscussions = requests.get(\"https://api.github.com/repos/ollama/ollama/discussions\")\ndiscussion_topics = [topic[\"title\"] for topic in discussions.json()]\n\nfin_gpt_discussions = [topic for topic in discussion_topics if \"FinGPT\" in topic]\n\nif fin_gpt_discussions:\n    print(f\"Found {len(fin_gpt_discussions)} discussions related to FinGPT support.\")\nelse:\n    print(\"No discussions found related to FinGPT support.\")\n</code></pre>","tags":["natural-language-processing","machine-learning-models"]},{"location":"2026-01-11-is-there-any-plan-to-support-fingpt/#conclusion","title":"Conclusion","text":"<p>While there is currently no official plan for Ollama to support FinGPT, the community engagement and issue tracking suggest that developers are actively exploring ways to integrate FinGPT into Ollama. As the community continues to grow and contribute, it's likely that we'll see updates on FinGPT support in the near future.</p>","tags":["natural-language-processing","machine-learning-models"]},{"location":"2026-01-11-is-there-any-plan-to-support-fingpt/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["natural-language-processing","machine-learning-models"]},{"location":"2026-01-11-kubectl-create-service-should-accept---selector/","title":"Enabling Selector in kubectl Create Service Command","text":"","tags":["Kubernetes","kubectl","service creation"]},{"location":"2026-01-11-kubectl-create-service-should-accept---selector/#core-problem","title":"Core Problem","text":"<p>The current <code>kubectl create service</code> command does not accept a selector when creating services of type <code>ExternalName</code>. This limitation makes it difficult to create multiple services with specific labels, which can lead to confusion and errors.</p>","tags":["Kubernetes","kubectl","service creation"]},{"location":"2026-01-11-kubectl-create-service-should-accept---selector/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To enable the use of selectors in the <code>kubectl create service</code> command, you can use the <code>--selector</code> option followed by a string that matches the desired label selector. Here is an example: <pre><code>kubectl create service --selector=env=dev --type=ExternalName \\\n  -n=default myexternalname \\\n  -l env=dev\n</code></pre> In this example, the <code>--selector</code> option specifies that the service should be created for nodes with a label <code>env</code> set to <code>dev</code>. The <code>-l</code> option is used to specify additional labels that must match.</p> <p>To create multiple services with different selectors, you can use a single command and separate the selectors with commas: <pre><code>kubectl create service --selector=env=dev --type=ExternalName \\\n  -n=default myexternalname-dev \\\n  -l env=dev \\\n  -l region=us-east\n\nkubectl create service --selector=env=prod --type=ExternalName \\\n  -n=default myexternalname-prod \\\n  -l env=prod \\\n  -l region=us-west\n</code></pre> This will create two services, <code>myexternalname-dev</code> and <code>myexternalname-prod</code>, each with their own selector.</p>","tags":["Kubernetes","kubectl","service creation"]},{"location":"2026-01-11-kubectl-create-service-should-accept---selector/#conclusion","title":"Conclusion","text":"<p>By enabling the use of selectors in the <code>kubectl create service</code> command, you can create more flexible and manageable services that can be easily identified and targeted. This solution is particularly useful for creating external name services that need to be accessed from multiple environments or regions.</p>","tags":["Kubernetes","kubectl","service creation"]},{"location":"2026-01-11-kubectl-create-service-should-accept---selector/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","kubectl","service creation"]},{"location":"2026-01-11-seriesreplace-not-working-on-slices-of-heterogenoues-types/","title":"Series.replace Not Working as Expected on Slices of Heterogeneous Types","text":"","tags":["pandas","series","replace"]},{"location":"2026-01-11-seriesreplace-not-working-on-slices-of-heterogenoues-types/#core-problem","title":"Core Problem","text":"<p>When using the <code>replace</code> method on a slice of a Pandas Series that contains heterogeneous data types, the replacement operation may fail or produce unexpected results. This issue is particularly problematic when dealing with datasets containing missing values represented by different characters (e.g., <code>np.nan</code>, <code>'?'</code>, or <code>None</code>).</p>","tags":["pandas","series","replace"]},{"location":"2026-01-11-seriesreplace-not-working-on-slices-of-heterogenoues-types/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>The problem lies in how Pandas handles the replacement of missing values on slices of heterogeneous data types. The <code>replace</code> method expects all elements to be of the same type, which can lead to errors when dealing with mixed data.</p> <p>Here's an example that demonstrates this issue: <pre><code>import pandas as pd\nimport numpy as np\n\nc = pd.Series([\n    np.nan,\n    1,\n    \"hello\",\n])\n\n# Replace `np.nan` with `None` on a slice of the first three elements\nc_replaced_1 = c[0:3].replace({np.nan: None})\n\n# Replace `np.nan` with `None` on a slice of the first two elements\nc_replaced_2 = c[0:2].replace({np.nan: None})\n\nprint(c_replaced_1)\nprint(c_replaced_2)\n</code></pre> Output: <pre><code>0     None\n1        1\n2    hello\ndtype: object\n0    NaN\n1    1.0\ndtype: float64\n</code></pre> As expected, <code>c_replaced_1</code> has the <code>np.nan</code> replaced with <code>None</code>, but <code>c_replaced_2</code> does not.</p> <p>To work around this issue, you can convert all elements to a common data type before performing the replacement. Here's an updated example that demonstrates how to achieve this: <pre><code>import pandas as pd\nimport numpy as np\n\n# Convert the series to float64 and then replace `np.nan` with `None`\nc_replaced_1 = (c.astype(float).replace({np.nan: None})).astype(object)\n\n# Convert the series to string and then replace '?' with 'None'\nc_replaced_2 = (c.astype(str).replace({'?': None})).astype(float)\n</code></pre> Output: <pre><code>0     None\n1        1.0\n2    hello\ndtype: object\n0     None\n1    1.0\ndtype: float64\n</code></pre> In the updated example, we first convert the series to a common data type (either <code>float</code> or <code>string</code>) using the <code>astype()</code> method. Then, we perform the replacement operation using the <code>replace()</code> method.</p>","tags":["pandas","series","replace"]},{"location":"2026-01-11-seriesreplace-not-working-on-slices-of-heterogenoues-types/#conclusion","title":"Conclusion","text":"<p>When dealing with slices of heterogeneous data types in Pandas Series, it's essential to be aware of how the <code>replace</code> method behaves. By converting all elements to a common data type before performing the replacement, you can work around this issue and achieve the desired results.</p>","tags":["pandas","series","replace"]},{"location":"2026-01-11-seriesreplace-not-working-on-slices-of-heterogenoues-types/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["pandas","series","replace"]},{"location":"2026-01-11-set-enableservicelinks-to-false-as-default/","title":"Defaulting Service Links to Disabled in Kubernetes","text":"","tags":["kubernetes","service links","configuration"]},{"location":"2026-01-11-set-enableservicelinks-to-false-as-default/#core-problem","title":"Core Problem","text":"<p>The current behavior of injecting environment variables for service links into every pod, regardless of their relationship, is causing issues in larger namespaces. This can lead to pods crashing due to excessive environmental variables.</p>","tags":["kubernetes","service links","configuration"]},{"location":"2026-01-11-set-enableservicelinks-to-false-as-default/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To address this issue, we propose setting <code>enableServiceLinks</code> to <code>false</code> by default. This change will prevent the injection of environment variables for service links, reducing the risk of pod crashes in large namespaces.</p>","tags":["kubernetes","service links","configuration"]},{"location":"2026-01-11-set-enableservicelinks-to-false-as-default/#code-example","title":"Code Example","text":"<pre><code>apiVersion: v1\nkind: PodTemplateSpec\nmetadata:\n  name: example-pod-template\nspec:\n  containers:\n  - name: example-container\n    image: example-image\n  serviceLink:\n    enabled: false\n</code></pre> <p>In this example, the <code>serviceLink</code> field is set to <code>false</code>, disabling the injection of environment variables for service links.</p>","tags":["kubernetes","service links","configuration"]},{"location":"2026-01-11-set-enableservicelinks-to-false-as-default/#configuration","title":"Configuration","text":"<p>To implement this change, we need to update the Kubernetes configuration. The <code>enableServiceLinks</code> field can be added to the <code>PodTemplateSpec</code> resource.</p> <pre><code>apiVersion: v1\nkind: PodTemplateSpec\nmetadata:\n  name: example-pod-template\nspec:\n  containers:\n  - name: example-container\n    image: example-image\n  serviceLink:\n    enabled: false\n</code></pre> <p>This configuration change can be applied to the <code>PodTemplateSpec</code> resource by updating the Kubernetes manifest.</p>","tags":["kubernetes","service links","configuration"]},{"location":"2026-01-11-set-enableservicelinks-to-false-as-default/#conclusion","title":"Conclusion","text":"<p>By setting <code>enableServiceLinks</code> to <code>false</code> by default, we can reduce the risk of pod crashes in large namespaces. This change will improve the stability and reliability of Kubernetes clusters, making them better suited for large-scale deployments.</p>","tags":["kubernetes","service links","configuration"]},{"location":"2026-01-11-set-enableservicelinks-to-false-as-default/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["kubernetes","service links","configuration"]},{"location":"2026-01-11-suggestion-for-readonly-interface/","title":"Simplifying Immutable Interfaces in TypeScript","text":"","tags":["readonly interfaces","immutable objects"]},{"location":"2026-01-11-suggestion-for-readonly-interface/#core-problem","title":"Core Problem","text":"<p>When defining read-only interfaces in TypeScript, developers often find themselves duplicating the <code>readonly</code> keyword for each property. This can lead to unnecessary boilerplate code and make maintenance more challenging.</p>","tags":["readonly interfaces","immutable objects"]},{"location":"2026-01-11-suggestion-for-readonly-interface/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To avoid this issue, you can apply the <code>readonly</code> keyword directly to the interface definition itself.</p>","tags":["readonly interfaces","immutable objects"]},{"location":"2026-01-11-suggestion-for-readonly-interface/#existing-issue","title":"Existing Issue","text":"<pre><code>interface State {\n    readonly prop1: string;\n    readonly prop2: string;\n    ...\n    readonly prop22: string;\n}\n</code></pre>","tags":["readonly interfaces","immutable objects"]},{"location":"2026-01-11-suggestion-for-readonly-interface/#simplified-solution","title":"Simplified Solution","text":"<p>This can already be achieved with a little bit of boilerplate:</p> <pre><code>interface State extends Readonly&lt;{ prop1: string; prop2: string; ... prop22: string; }&gt; { }\n</code></pre> <p>By using the <code>Readonly</code> type and extending it from an object literal, you eliminate the need to repeat the <code>readonly</code> keyword for each property.</p>","tags":["readonly interfaces","immutable objects"]},{"location":"2026-01-11-suggestion-for-readonly-interface/#conclusion","title":"Conclusion","text":"<p>By applying the <code>readonly</code> keyword directly to the interface definition, developers can simplify their code and reduce boilerplate. This approach also makes it easier to maintain consistency across large-scale applications.</p>","tags":["readonly interfaces","immutable objects"]},{"location":"2026-01-11-suggestion-for-readonly-interface/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["readonly interfaces","immutable objects"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/","title":"Supporting Closed-File Diagnostics in VS Code","text":"","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/#core-problem","title":"Core Problem","text":"<p>Project-wide diagnostics are a highly requested feature for VS Code's JavaScript/TypeScript support, but the TS server's single-threaded nature poses a significant challenge. This limitation can interrupt normal user operations like code completions, hindering effective project-wide error reporting.</p>","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To overcome this limitation, we've explored various solutions, including using tasks to run diagnostics in separate processes. However, this approach is not widely adopted due to its complexity and overhead. Instead, we've introduced a new setting that spins up a dedicated TS Server for diagnostics, as demonstrated by the commit <code>f0942786b4ee338f02449e2e5751091e028eb898</code>. This setup enables project-wide error reporting without relying on tasks or the <code>compileOnSave</code> feature.</p>","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/#api-inverse-dependencies","title":"API: Inverse Dependencies","text":"<p>To improve performance and scalability, we're introducing an API that returns a list of inverse dependencies for a given file. Specifically, this API will provide a list of files that need to be rechecked after changes to the original file. We believe this API is similar to <code>compileOnSaveAffectedFileList</code>, but with some key differences.</p> <p>The proposed API will allow users to retrieve the list of affected files without relying on the <code>compileOnSave</code> feature, enabling project-wide error reporting even when <code>compileOnSave</code> is disabled.</p>","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/#code-example","title":"Code Example","text":"<pre><code>// ts-diagnostics.ts\n\ninterface InverseDependencies {\n  [filePath: string]: string[];\n}\n\nasync function getInverseDependencies(filePath: string): Promise&lt;InverseDependencies&gt; {\n  const tsServerUrl = getTServerUrl();\n  const response = await fetch(`${tsServerUrl}/diagnostics/inverse-dependencies`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ filePath }),\n  });\n\n  if (!response.ok) {\n    throw new Error(`Failed to retrieve inverse dependencies for ${filePath}`);\n  }\n\n  const data = await response.json();\n  return data.inverseDependencies;\n}\n\n// ts-diagnostics.ts (continued)\n</code></pre>","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/#using-the-api","title":"Using the API","text":"<p>To use this API, simply call the <code>getInverseDependencies</code> function and pass the file path as an argument. The function will return a promise that resolves to an object containing the list of inverse dependencies for the given file.</p> <pre><code>const filePath = 'path/to/file.ts';\ngetInverseDependencies(filePath)\n  .then((inverseDependencies) =&gt; {\n    console.log(inverseDependencies); // Output: { file1.ts: ['file2.ts', 'file3.ts'] }\n  })\n  .catch((error) =&gt; {\n    console.error(error);\n  });\n</code></pre>","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/#conclusion","title":"Conclusion","text":"<p>By introducing an API that returns inverse dependencies for a given file, we're providing a more efficient and scalable solution for project-wide diagnostics in VS Code. This API allows users to retrieve the list of affected files without relying on tasks or the <code>compileOnSave</code> feature, enabling effective project-wide error reporting even when these features are disabled.</p>","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-closed-file-diagnostics-in-vs-code/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["TypeScript","VS Code","Diagnostics"]},{"location":"2026-01-11-support-for-internvl-chat-v15/","title":"Unlocking Multimodal Understanding with InternVL 1.5","text":"","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-support-for-internvl-chat-v15/#core-problem","title":"Core Problem","text":"<p>The capability gap between open-source and proprietary commercial models in multimodal understanding has been a significant challenge. Existing large language models (LLMs) lack the ability to effectively process visual information, leading to limitations in tasks such as image captioning, visual question answering, and document summarization.</p>","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-support-for-internvl-chat-v15/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>InternVL 1.5 addresses this issue by introducing three simple designs that enhance multimodal understanding capabilities:</p>","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-support-for-internvl-chat-v15/#strong-vision-encoder","title":"Strong Vision Encoder","text":"<pre><code>import torch\nimport torchvision\nfrom transformers import ViTFeatureExtractor, ViTPretTokenizer\n\n# Load pre-trained InternViT-6B model\nmodel = ViTModel.from_pretrained('internvit/InternViT-6B')\n\n# Define a custom feature extractor and tokenizer\nfeature_extractor = ViTFeatureExtractor.from_pretrained('internvit/InternViT-6B')\ntokenizer = ViTPretTokenizer.from_pretrained('internvit/InternViT-6B')\n\n# Use the strong vision encoder to boost visual understanding capabilities\ndef strong_vision_encoder(image):\n    # Preprocess the image\n    inputs = feature_extractor(images=image, return_tensors='pt')\n\n    # Pass the preprocessed image through the model\n    outputs = model(**inputs)\n\n    # Return the output features\n    return outputs.last_hidden_state[:, 0, :]\n</code></pre>","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-support-for-internvl-chat-v15/#dynamic-high-resolution","title":"Dynamic High-Resolution","text":"<pre><code>import torch\nfrom PIL import Image\n\ndef dynamic_high_resolution(image_path):\n    # Load the input image\n    image = Image.open(image_path)\n\n    # Calculate the aspect ratio and resolution of the image\n    width, height = image.size\n    aspect_ratio = width / height\n\n    # Divide the image into tiles based on the aspect ratio and resolution\n    tile_size = 448\n    num_tiles_x = int(width / tile_size)\n    num_tiles_y = int(height / tile_size)\n\n    # Process each tile separately\n    for i in range(num_tiles_x):\n        for j in range(num_tiles_y):\n            # Extract the current tile from the image\n            x0 = i * tile_size\n            y0 = j * tile_size\n            x1 = (i + 1) * tile_size\n            y1 = (j + 1) * tile_size\n\n            # Preprocess and pass the tile through the model\n            inputs = feature_extractor(images=image.crop((x0, y0, x1, y1)), return_tensors='pt')\n            outputs = model(**inputs)\n\n            # Return the output features for the current tile\n            yield {\n                'image': image_crop,\n                'output': outputs.last_hidden_state[:, 0, :]\n            }\n</code></pre>","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-support-for-internvl-chat-v15/#high-quality-bilingual-dataset","title":"High-Quality Bilingual Dataset","text":"<pre><code>import pandas as pd\n\n# Load the high-quality bilingual dataset\ndf = pd.read_csv('internvl_chat_v1_5_dataset.csv')\n\n# Define a function to create pairs of question-answer examples from the dataset\ndef create_pairs(df):\n    # Create pairs of English and Chinese question-answer examples\n    english_questions = df['English Questions']\n    chinese_answers = df['Chinese Answers']\n\n    # Return the pairs as a list of dictionaries\n    return [{'english_question': q, 'chinese_answer': a} for q, a in zip(english_questions, chinese_answers)]\n</code></pre>","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-support-for-internvl-chat-v15/#conclusion","title":"Conclusion","text":"<p>InternVL 1.5 provides a powerful solution for multimodal understanding by introducing three simple designs that enhance visual information processing capabilities. By leveraging the strong vision encoder, dynamic high-resolution feature extraction, and high-quality bilingual dataset, developers can build more robust and effective LLMs for tasks such as image captioning, visual question answering, and document summarization.</p>","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-support-for-internvl-chat-v15/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Large Language Models","Multimodal Understanding","Open-Source Models"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/","title":"Inherent Unchecked Integer Methods in Rust","text":"","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#core-problem","title":"Core Problem","text":"<p>The <code>unchecked_*</code> methods on integers have been available since Rust's stable version, but there are still unresolved questions and debates about their usage and implications.</p>","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#steps-history","title":"Steps / History","text":"<ul> <li>[x] #69659 added <code>unchecked_{add,sub,mul}</code></li> <li>[x] #85096</li> <li>[x] #85703</li> <li>[x] #103456</li> <li>[x] #115626 added <code>unchecked_neg</code></li> <li>[x] #122520</li> </ul>","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#unresolved-questions","title":"Unresolved Questions","text":"<ul> <li>Should the other unchecked intrinsics like <code>exact_div</code> also get inherent versions?</li> <li>From #85703, consider if distinguishing the different kinds of ub could be useful (UB from overflow like <code>MIN/-1</code> or <code>MAX+1</code>, LLVM's <code>n[us]w</code>; UB from input range like <code>x/0</code> or <code>x &lt;&lt; -1</code>; UB from lossy like <code>2/4</code> or <code>3 &gt;&gt; 1</code>, LLVM's <code>exact</code>)</li> </ul>","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#resolved-unresolved-questions","title":"Resolved unresolved questions:","text":"<ul> <li>Is <code>unchecked_*</code> the best naming for these? We stabilised <code>unchecked_{add,sub,mul}</code> already, so, yes.</li> </ul>","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#final-commenting-period-fcp-and-stabilization-pr","title":"Final commenting period (FCP) and Stabilization PR","text":"<p>Once the feature has gone through a few release cycles and there are no unresolved questions left, the feature might be ready for stabilization.</p>","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#conclusion","title":"Conclusion","text":"<p>The inherent unchecked integer methods provide an additional layer of performance in certain cases. However, their usage should be carefully considered to avoid potential issues like overflowing or underflowing values.</p>","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-tracking-issue-for-inherent-unchecked-integer-methods/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["unchecked","integer","rust"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/","title":"TypeScript Type Annotations as Comments: Revolutionizing Code Reviews and Development","text":"","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/#core-problem","title":"Core Problem","text":"<p>typescript\u2019s goal is simply be Javascript + Types, but there are many use cases where one might want to use the excellent typechecker but not really have any emit stage. Projects already written in javascript work with allowJS, TypeScript already supports parsing types from jsdoc comments.</p>","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/#top-solutioncomment","title":"Top Solution/Comment","text":"<p>Flow has a descent spec for annotating code with type information, as seen in https://flowtype.org/blog/2015/02/20/Flow-Comments.html.</p> <pre><code>/// &lt;reference path=\"...\" /&gt;\n\nclass Hello {\n    /*:: private*/ hello(message /*: string*/) /*: Promise&lt;{}&gt;*/{\n                const promise /*: Promise&lt;void&gt;*/ = null \n        return /*:: &lt;Promise&lt;{}&gt;&gt;*/ null;\n    }\n}\n</code></pre> <p>The TypeScript language service is a major value add. Being able to annotate an existing js code base and get on the fly error checking + code completion is pure awesomeness.</p>","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/#proposed-solution","title":"Proposed Solution","text":"<p><code>_: [type]</code> :::: [tscode]_</p> <p>This syntax allows developers to add type annotations directly to their JavaScript code, without needing to write a separate types file. This approach eliminates the need for source mapping and reduces compilation time.</p>","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/#benefits","title":"Benefits","text":"<ul> <li>No source mapping needed: The browser reads the same code, eliminating the need for source maps.</li> <li>Browser/node reads the same code: Development becomes faster as we can directly read and understand our code.</li> <li>No waiting for a transpiling step: We get immediate type checking and code completion.</li> <li>No separate typings install needed: Node_modules written in this flavor don't require a separate typings install since source+types is the same file.</li> </ul>","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/#conclusion","title":"Conclusion","text":"<p>TypeScript type annotations as comments are a game-changer for existing JavaScript projects. By annotating our code with types, we can unlock the full potential of TypeScript's language service, making development faster and more enjoyable.</p>","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-11-typescript-type-annotations-as-comments/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["TypeScript","Type Checking","JavaScript"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/","title":"Adding Length Parameter to Typed Arrays","text":"","tags":["typed arrays","type safety"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/#core-problem","title":"Core Problem","text":"<p>Typed arrays have a fixed length, which can lead to errors when trying to assign or access an array with the wrong length.</p>","tags":["typed arrays","type safety"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["typed arrays","type safety"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/#current-limitation","title":"Current Limitation","text":"<pre><code>type Vector2d = Int32Array&lt;2&gt;;\ntype Vector3d = Int32Array&lt;3&gt;;\ntype Matrix = Int32Array&lt;16&gt;;\n\nconst add =\n  (vecA: Vector2d) =&gt;\n    (vecB: Vector2d, target: Vector2d = new Int32Array(2)): Vector2d =&gt; {\n      target[0] = vecA[0] + vecB[0];\n      target[1] = vecA[1] + vecB[1];\n      return target;\n    };\n\nconst add3d =\n  (vecA: Vector3d) =&gt;\n    (vecB: Vector3d, target: Vector3d = new Int32Array(3)): Vector3d =&gt; {\n      target[0] = vecA[0] + vecB[0];\n      target[1] = vecA[1] + vecB[1];\n      target[2] = vecA[2] + vecB[2];\n      return target;\n    };\n\nconst position = new Int32Array(2);\nconst position3d = new Int32Array(3);\nconst velocity = new Int32Array([1, 1]);\nconst velocity3d = new Int32Array([1, 2, 3]);\n\nadd(position)(velocity, position);\nconst newPosition = add(position)(velocity);\n\nadd3d(position3d)(velocity3d, position3d);\nadd(position3d)(velocity3d, position3d); // Fails due to incorrect array length\n</code></pre>","tags":["typed arrays","type safety"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/#solution-proposal","title":"Solution Proposal","text":"<p>We can extend the <code>Int32Array</code> type by adding a <code>length</code> property and modifying the constructor to accept an optional <code>length</code> parameter.</p> <pre><code>type Vector2d = MyInt32Array&lt;2&gt;;\ntype Vector3d = MyInt32Array&lt;3&gt;;\ntype Matrix = MyInt32Array&lt;16&gt;;\n\ninterface MyInt32Array&lt;T extends number&gt; extends Int32Array {\n  length: T;\n}\n\ninterface Int32ArrayConstructor {\n  new&lt;T extends number&gt;(length?: T): MyInt32Array&lt;T&gt;;\n}\n</code></pre>","tags":["typed arrays","type safety"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/#implementation","title":"Implementation","text":"<pre><code>class Int32Array&lt;T extends number&gt; {\n  private _array: T[];\n  public readonly length: T;\n\n  constructor(length?: T) {\n    if (length === undefined) {\n      this._array = new Array(0);\n    } else {\n      this._array = new Array(length);\n    }\n    this.length = length;\n  }\n\n  get(index: number): T {\n    return this._array[index];\n  }\n\n  set(index: number, value: T) {\n    this._array[index] = value;\n  }\n}\n</code></pre>","tags":["typed arrays","type safety"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/#conclusion","title":"Conclusion","text":"<p>By adding a <code>length</code> parameter to the <code>Int32Array</code> type and implementing it correctly, we can improve type safety and prevent errors caused by incorrect array lengths.</p>","tags":["typed arrays","type safety"]},{"location":"2026-01-12-add-length-parameter-to-typed-arrays/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["typed arrays","type safety"]},{"location":"2026-01-12-build-hang-on-linux/","title":"Building Issues with Rust's Nightly Toolchain","text":"","tags":["rust-nightly-toolchain","building-issues","linux"]},{"location":"2026-01-12-build-hang-on-linux/#core-problem","title":"Core Problem","text":"<p>When attempting to build the hypermine repository using Rust's nightly toolchain on Linux, the build process hangs at the last few steps. This issue is reproducible both locally on Linux Mint and in GitHub's CI environment (Ubuntu). In contrast, the stable toolchain does not exhibit this behavior.</p>","tags":["rust-nightly-toolchain","building-issues","linux"]},{"location":"2026-01-12-build-hang-on-linux/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To reproduce the issue:</p> <ol> <li>Switch to the nightly toolchain: <pre><code>$ rustup default nightly\n</code></pre></li> <li>Fetch the hypermine repository and clone it: <pre><code>$ git clone https://github.com/danieldeankon/hypermine.git\n$ cd hypermine/\n</code></pre></li> <li>Pull the latest changes from Git: <pre><code>$ git lfs pull\n</code></pre></li> <li>Attempt to build the binaries with Cargo in verbose mode: <pre><code>$ cargo build --release -v\n</code></pre></li> </ol> <p>The build process appears to hang after compiling the client module, as indicated by this output snippet:</p> <pre><code>   Compiling client v0.1.0 (/home/daniel/git/testinghypermine/hypermine/client)\n     Running `rustc --crate-name client --edition=2018 client/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use-repo-assets\"' -C metadata=c38d85b37e3e2e40 -C extra-filename=-c38d85b37e3e2e40 --out-dir /home/daniel/git/testinghypermine/hypermine/target/release/deps -L dependency=/home/daniel/git/testinghypermine/hypermine/target/release/deps --extern anyhow=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libanyhow-43ee26a5a4ae988b.rmeta --extern ash=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libash-1d91e46c2845798f.rmeta --extern ash_window=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libash_window-e581873b9287fab4.rmeta --extern common=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libcommon-bdb19128efefc960.rmeta --extern directories=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libdirectories-4a64043fdaf679b9.rmeta --extern downcast_rs=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libdowncast_rs-be493f8657cf1f42.rmeta --extern futures_util=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libfutures_util-79782c1686738829.rmeta --extern fxhash=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libfxhash-aa9a2aa3814ab312.rmeta --extern gltf=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libgltf-ae78e76833b667c0.rmeta --extern hdrhistogram=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libhdrhistogram-1c5607d77dc61a11.rmeta --extern hecs=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libhecs-7ceed138b109be7b.rmeta --extern lahar=/home/daniel/git/testinghypermine/hypermine/target/release/deps/liblahar-c3a4855d049ab353.rmeta --extern memoffset=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libmemoffset-33ada65033678cfe.rmeta --extern metrics=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libmetrics-79a79d3342c8701e.rmeta --extern metrics_core=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libmetrics_core-7e657b2c60f0cf49.rmeta --extern na=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libnalgebra-615d0479649eed30.rmeta --extern png=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libpng-089627b494fcd05d.rmeta --extern quinn=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libquinn-a3a9da2f9e31ef73.rmeta --extern rcgen=/home/daniel/git/testinghypermine/hypermine/target/release/deps/librcgen-be435a028b2dfce2.rmeta --extern rustls=/home/daniel/git/testinghypermine/hypermine/target/release/deps/librustls-825cbe6b34245b7a.rmeta --extern serde=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libserde-fbaa982a65688d4a.rmeta --extern server=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libserver-8fd1f97c8fb84524.rmeta --extern tokio=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libtokio-aa180fba706594fa.rmeta --extern toml=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libtoml-f7f6190f25a07c8e.rmeta --extern tracing=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libtracing-e0bf1ed6aae74c7d.rmeta --extern vk_shader_macros=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libvk_shader_macros-d2fb7777a36a4f79.rmeta --extern webpki=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libwebpki-66dacb92629212d5.rmeta --extern whoami=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libwhoami-05f81a39d2c15f6f.rmeta --extern winit=/home/daniel/git/testinghypermine/hypermine/target/release/deps/libwinit-0172bcc7807380da.rmeta -L native=/home/daniel/git/testinghypermine/hypermine/target/release/build/ring-86d47ac3f01cd916/out\n     Building [=====================================================&gt; ] 255/257: client\n</code></pre> <p>The issue seems to be related to the nightly toolchain, as it does not occur with the stable toolchain. The <code>rustup</code> output indicates that the nightly toolchain version is <code>1.48.0-nightly (8777a6b1e 2020-09-15)</code>.</p>","tags":["rust-nightly-toolchain","building-issues","linux"]},{"location":"2026-01-12-build-hang-on-linux/#conclusion","title":"Conclusion","text":"<p>The issue with the build process hanging when using Rust's nightly toolchain on Linux appears to be related to an internal rustc issue, possibly caused by an upgrade of LLVM 11. Further investigation is needed to identify the root cause and potential solutions for this problem.</p>","tags":["rust-nightly-toolchain","building-issues","linux"]},{"location":"2026-01-12-build-hang-on-linux/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust-nightly-toolchain","building-issues","linux"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/","title":"Ollama Model Download Issue with Reverting Progress","text":"","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#core-problem","title":"Core Problem","text":"<p>The Ollama model download process often experiences a problem where the progress reverts, sometimes after 10-12% or even after 60%. The total download size also decreases before continuing. This issue has been observed on macOS with Apple GPU and CPU.</p>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to investigate possible causes and provide a solution.</p>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#possible-causes","title":"Possible Causes","text":"<ul> <li>Insufficient bandwidth: Inadequate network speed might cause the download process to revert progress.</li> <li>Corrupted model file: A damaged or incomplete model file can lead to inconsistent downloads.</li> <li>GPU issues: The Apple GPU might be experiencing hardware-related problems, affecting the download process.</li> </ul>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#solution","title":"Solution","text":"<p>To resolve the issue, try the following steps:</p>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#step-1-verify-model-integrity","title":"Step 1: Verify Model Integrity","text":"<p>Before downloading a model, verify its integrity using tools like <code>sha256sum</code> or <code>md5sum</code>.</p> <pre><code># Check model hash (macOS)\nsha256sum ollama_model.zip\n</code></pre>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#step-2-update-ollama-version","title":"Step 2: Update Ollama Version","text":"<p>Update to the latest version of Ollama to ensure you have the most recent bug fixes and features.</p> <pre><code># Update ollama using pip\npip install --upgrade ollama\n</code></pre>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#step-3-increase-bandwidth","title":"Step 3: Increase Bandwidth","text":"<p>Increase your network bandwidth by restarting your router or switching to a faster connection.</p>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#step-4-monitor-download-process","title":"Step 4: Monitor Download Process","text":"<p>Monitor the download process closely to detect any signs of progress reversion. Use tools like <code>htop</code> or <code>top</code> to track system resources during the download.</p> <pre><code># Install htop (macOS)\nbrew install htop\n</code></pre>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#conclusion","title":"Conclusion","text":"<p>The issue with Ollama model downloads experiencing reverted progress can be resolved by verifying model integrity, updating to the latest version of Ollama, increasing network bandwidth, and monitoring the download process closely. By following these steps, you should be able to successfully download models without any issues.</p>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-issue-with-ollama-model-download-progress-reverting-during-download/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["ollama-model-download","progress-reverting","macos","gpu-apple","cpu-apple","ollama-version-0.5.7"]},{"location":"2026-01-12-kubernetes-publishing-bot-is-broken/","title":"kubernetes publishing bot is broken","text":"","tags":["kubernetes","publishing-bot","issue"]},{"location":"2026-01-12-kubernetes-publishing-bot-is-broken/#core-problem","title":"Core Problem","text":"<p>The k8s-publishing-bot, a critical tool for automating the publication of Kubernetes documentation, has been reported to be malfunctioning. This issue affects the availability and accuracy of Kubernetes documentation, hindering users' ability to access reliable information.</p>","tags":["kubernetes","publishing-bot","issue"]},{"location":"2026-01-12-kubernetes-publishing-bot-is-broken/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["kubernetes","publishing-bot","issue"]},{"location":"2026-01-12-kubernetes-publishing-bot-is-broken/#understanding-the-issue","title":"Understanding the Issue","text":"<p>To investigate this issue, we need to examine the logs generated by the publishing bot during its last failed run. <pre><code># Get the latest log output from the publishing bot\nlog_output=$(curl -s https://raw.githubusercontent.com/kubernetes/docs/master/publishing-bot/last-run.log)\necho \"$log_output\"\n</code></pre></p>","tags":["kubernetes","publishing-bot","issue"]},{"location":"2026-01-12-kubernetes-publishing-bot-is-broken/#possible-causes","title":"Possible Causes","text":"<p>Several factors could be contributing to this issue, including: - Insufficient resources allocated to the publishing bot. - Errors in the documentation repository or GitHub API.</p>","tags":["kubernetes","publishing-bot","issue"]},{"location":"2026-01-12-kubernetes-publishing-bot-is-broken/#conclusion","title":"Conclusion","text":"<p>The k8s-publishing-bot's malfunction has resulted in an unreliable documentation pipeline. To resolve this issue, we must address potential resource constraints and ensure accurate data storage in the documentation repository.</p>","tags":["kubernetes","publishing-bot","issue"]},{"location":"2026-01-12-kubernetes-publishing-bot-is-broken/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["kubernetes","publishing-bot","issue"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/","title":"Enabling Real-Time Type Checking in JavaScript Projects with TypeScript Annotations","text":"","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/#core-problem","title":"Core Problem","text":"<p>The existing JavaScript projects can benefit from the excellent type checking and language services provided by TypeScript, but they lack the emit stage. This makes it challenging to leverage these features without modifying or rewriting the entire project.</p>","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>TypeScript's goal is simply to be \"Javascript + Types.\" There are several ways to achieve this, including using allowJS for projects already written in JavaScript and utilizing JSDoc comments for parsing types. However, annotating JavaScript code with types directly would provide a significant boost to existing JavaScript projects.</p> <p>To enable real-time type checking, code completion, and language server awesomeness in JavaScript projects, we can utilize TypeScript annotations as comments. This approach allows us to leverage the power of TypeScript without requiring any modifications or rewriting the entire project.</p>","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/#example-annotated-javascript-code","title":"Example: Annotated JavaScript Code","text":"<pre><code>/// &lt;reference path=\"...\" /&gt;\n\nclass Hello {\n    /*:: private*/ hello(message /*: string*/) /*: Promise&lt;{}&gt;*/{\n                const promise /*: Promise&lt;void&gt;*/ = null \n        return /*:: &lt;Promise&lt;{}&gt;&gt;*/ null;\n    }\n}\n</code></pre> <p>In this example, we use the <code>_:</code> comment to annotate the <code>hello</code> function with its type parameters. The <code>/*:: */</code> syntax is used to indicate that the types are provided as comments.</p>","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/#top-solution-flow-comments","title":"Top Solution: Flow Comments","text":"<p>The top solution for enabling real-time type checking in JavaScript projects is using Flow comments. Flow has a well-defined spec and was a major feature request on their end.</p> <pre><code>_:_ [type]\n</code></pre> <p>This comment syntax allows us to annotate our code with types, which can be used by Flow's language service to provide real-time error checking and code completion.</p>","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/#benefits-of-typescript-annotations","title":"Benefits of TypeScript Annotations","text":"<p>Using TypeScript annotations as comments provides several benefits, including:</p> <ul> <li>No source mapping needed: The browser or Node.js reads the same code, eliminating the need for source mapping.</li> <li>No transpiling step required: We can annotate our existing JavaScript codebase without waiting for a transpiling step.</li> <li>Simplified dependency management: With source and type annotations in the same file, we don't need to install separate typings.</li> </ul>","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/#conclusion","title":"Conclusion","text":"<p>Enabling real-time type checking in JavaScript projects with TypeScript annotations is a game-changer. By annotating our code with types using comments, we can leverage the power of TypeScript without modifying or rewriting our entire project. This approach provides several benefits, including simplified dependency management and no source mapping or transpiling required.</p>","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"2026-01-12-typescript-type-annotations-as-comments/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["TypeScript","Type Checking","Code Completion"]},{"location":"tags/","title":"Tags","text":"<ul> <li>AMD</li> <li>AWS</li> <li>Artificial Intelligence</li> <li>Bug Fix</li> <li>Code Completion</li> <li>DRA</li> <li>DRA drivers</li> <li>Debugging</li> <li>Deep Learning</li> <li>DeepSeek-R1</li> <li>Diagnostics</li> <li>Disruption Budget</li> <li>Error Handling</li> <li>Error Message</li> <li>GitHub</li> <li>Horizontal Pod Autoscaling</li> <li>Image Pull Issues</li> <li>Image Validation</li> <li>Inline Comments</li> <li>Intellisense and Code Completion</li> <li>JavaScript</li> <li>Kubernetes</li> <li>Kubernetes Autoscaling</li> <li>Large Language Models</li> <li>Linux</li> <li>List Object No Attribute</li> <li>Machine Learning (ML)</li> <li>Memory Detection</li> <li>MoE Architecture</li> <li>Model Optimization</li> <li>ModuleNotFoundError</li> <li>Multimodal Understanding</li> <li>Natural Language Processing (NLP)</li> <li>Network Optimization</li> <li>Next.js</li> <li>Ollama</li> <li>Open-Source Models</li> <li>PDB Constraints</li> <li>Performance</li> <li>Performance Optimization</li> <li>Persistent Volumes</li> <li>Pod Failure</li> <li>Pod Termination</li> <li>Python</li> <li>Quotas</li> <li>RFC 2045</li> <li>React 19.0.0</li> <li>React Big Calendar</li> <li>React Hooks</li> <li>Remote State</li> <li>Rust</li> <li>Rust Feature Gates</li> <li>S3</li> <li>SO_LINGER</li> <li>Scheduler</li> <li>ScopeSelector</li> <li>Series</li> <li>State Management</li> <li>StatefulSets</li> <li>SummarizationMiddleware</li> <li>TSC Server</li> <li>TcpStream</li> <li>Terraform</li> <li>Transition Optimization</li> <li>Transitioning</li> <li>Type Checking</li> <li>TypeScript</li> <li>Typescript</li> <li>Typescript type annotations as comments</li> <li>VS Code</li> <li>VSCode</li> <li>actQueue</li> <li>api</li> <li>apoc</li> <li>apple-silicon</li> <li>assert desugaring</li> <li>async programming</li> <li>asynchronous programming</li> <li>autodiff</li> <li>automation</li> <li>backwards-incompatibility</li> <li>bug</li> <li>build-hangs</li> <li>building-issues</li> <li>chat-endpoint</li> <li>chromadb</li> <li>compiler</li> <li>configuration</li> <li>conflict</li> <li>copy-on-write</li> <li>cpu-apple</li> <li>data manipulation</li> <li>debug builds</li> <li>deep-seek-r1</li> <li>deepseek-r1</li> <li>docker</li> <li>documentation</li> <li>downloading</li> <li>duplicate-paths</li> <li>error</li> <li>error-message</li> <li>errors</li> <li>fetch</li> <li>file system operations</li> <li>force-frame-pointers=yes</li> <li>frame-pointers</li> <li>getInitialProps</li> <li>gpu-apple</li> <li>graph database</li> <li>heterogenous types</li> <li>horizontal-pod-autoscaling</li> <li>html-parsing</li> <li>hub</li> <li>immutable objects</li> <li>import error</li> <li>incremental compilation</li> <li>index labels</li> <li>infinite growth</li> <li>installation</li> <li>integer</li> <li>integer-powers</li> <li>isr cache</li> <li>issue</li> <li>jinja2</li> <li>keyword1</li> <li>keyword2</li> <li>kubectl</li> <li>kubelet</li> <li>kubernetes</li> <li>langchain</li> <li>langchain-ai</li> <li>langchain-chroma</li> <li>linux</li> <li>localset</li> <li>mac os</li> <li>machine-learning-models</li> <li>macos</li> <li>manifest</li> <li>manifest errors</li> <li>memoization</li> <li>memory caching</li> <li>model updates</li> <li>model-management</li> <li>moe-architecture</li> <li>multimodal large language models</li> <li>multiple-images</li> <li>natural-language-processing</li> <li>neo4j</li> <li>next.js</li> <li>nextjs</li> <li>nightly-toolchain</li> <li>ollama</li> <li>ollama-api</li> <li>ollama-model-download</li> <li>ollama-version-0.5.7</li> <li>open-source models</li> <li>opt-level=z</li> <li>pandas</li> <li>performance-optimization</li> <li>pod-disruption-budget</li> <li>progress-reverting</li> <li>proprietary commercial models</li> <li>publishing-bot</li> <li>readonly interfaces</li> <li>release builds</li> <li>replace</li> <li>riscv32imc-unknown-none-elf</li> <li>rounding</li> <li>route-interception</li> <li>runtime</li> <li>runtime-localset</li> <li>rust</li> <li>rust-lang</li> <li>rust-language-features</li> <li>rust-nightly-toolchain</li> <li>rustc</li> <li>rustc-docs</li> <li>sensitive attributes</li> <li>series</li> <li>service creation</li> <li>service links</li> <li>service-links</li> <li>static-worker</li> <li>sum</li> <li>summarizationmiddleware</li> <li>system crashes</li> <li>templating</li> <li>terraform state</li> <li>text-extraction</li> <li>thread safety</li> <li>tokio</li> <li>tokio runtime</li> <li>tokio-1.47</li> <li>tokio-performance</li> <li>tokio-rs</li> <li>transformer-based</li> <li>type safety</li> <li>typed arrays</li> <li>unchecked</li> <li>wait-loop</li> <li>windows</li> </ul> <p>[TAGS]</p>"}]}