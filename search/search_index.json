{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"2026-01-03-bug-actqueue-forever-growing-in-react-1900/","title":"Understanding the Act Queue Issue in React 19.0.0","text":"","tags":["React 19.0.0","actQueue","infinite loop"]},{"location":"2026-01-03-bug-actqueue-forever-growing-in-react-1900/#core-problem","title":"Core Problem","text":"<p>When migrating from React 18.3.1 to React 19.0.0, users encountered a unit test failure caused by an infinite loop in the act queue. This issue affects components with <code>&lt;Suspense /&gt;</code> and <code>react.lazy</code>, particularly when using a component with a ref that depends on state.</p>","tags":["React 19.0.0","actQueue","infinite loop"]},{"location":"2026-01-03-bug-actqueue-forever-growing-in-react-1900/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To debug this issue, we need to identify the root cause of the problem. Based on the provided information, the following potential solutions were explored:</p>","tags":["React 19.0.0","actQueue","infinite loop"]},{"location":"2026-01-03-bug-actqueue-forever-growing-in-react-1900/#removing-unnecessary-dependencies","title":"Removing unnecessary dependencies","text":"<ol> <li>Removing <code>setRef</code> callsite from ref props: By doing so, the test can finish successfully. <pre><code>// Before\nconst [ref, setRef] = useState(null);\nreturn (\n  &lt;div ref={setRef} /&gt;\n);\n\n// After\nreturn (\n  &lt;div /&gt;\n);\n</code></pre></li> <li>Removing <code>react.lazy</code> and <code>&lt;Suspense&gt;</code>: These components seem to be contributing to the infinite loop. <pre><code>// Before\nconst LazyComponent = react.lazy(() =&gt; import('./LazyComponent'));\n\nfunction ParentComponent() {\n  return (\n    &lt;div&gt;\n      &lt;Suspense fallback={&lt;div&gt;Loading...&lt;/div&gt;}&gt;\n        &lt;LazyComponent /&gt;\n      &lt;/Suspense&gt;\n    &lt;/div&gt;\n  );\n}\n\n// After\nfunction ParentComponent() {\n  return &lt;div /&gt;;\n}\n</code></pre></li> <li>Removing the ref state dependency: By removing this dependency, the test can still finish without entering an infinite loop. <pre><code>// Before\nconst [ref, setRef] = useState(null);\nreturn (\n  &lt;div ref={setRef} /&gt;\n);\n\n// After\nfunction Component() {\n  return &lt;div /&gt;;\n}\n</code></pre></li> </ol>","tags":["React 19.0.0","actQueue","infinite loop"]},{"location":"2026-01-03-bug-actqueue-forever-growing-in-react-1900/#code-example-with-actqueue-analysis","title":"Code example with actQueue analysis","text":"<p>To further analyze the issue, we can use the <code>act()</code> function from React Testing Library to inspect the act queue: <pre><code>import { act } from '@testing-library/react';\n\ntest('should render', () =&gt; {\n  const component = render(&lt;Component /&gt;);\n  expect(component).toBeTruthy();\n});\n\nbeforeEach(() =&gt; {\n  jest.clearAllMocks();\n  jest.clearAllTimers();\n});\n\nafterEach(() =&gt; {\n  jest.restoreAllMocks();\n  jest.restoreAllTimers();\n});\n\nit('should not enter infinite loop', async () =&gt; {\n  const actQueue = getActQueue();\n  act(() =&gt; {\n    // Simulate some asynchronous operation\n    await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    // Add another action to the queue\n    act(() =&gt; {\n      // Simulate some other asynchronous operation\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    });\n  });\n  expect(actQueue.length).toBeGreaterThan(0);\n});\n</code></pre></p>","tags":["React 19.0.0","actQueue","infinite loop"]},{"location":"2026-01-03-bug-actqueue-forever-growing-in-react-1900/#conclusion","title":"Conclusion","text":"<p>In conclusion, this issue highlights the importance of carefully analyzing the dependencies and actions in React components when migrating to a new version. By identifying and removing unnecessary dependencies or optimizing the ref state usage, we can prevent infinite loops and ensure smooth testing and development experiences.</p>","tags":["React 19.0.0","actQueue","infinite loop"]},{"location":"2026-01-03-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/","title":"Can't Install Rustc-docs Component: Resolving the Conflict","text":"","tags":["rust-installation","rustc-docs"]},{"location":"2026-01-03-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#core-problem","title":"Core Problem","text":"<p>The installation of the <code>rustc-docs</code> component has encountered a conflict due to overlapping directory structures. This issue arises when the <code>rustc</code> documentation is present in both the shared and local document directories, causing conflicts during installation.</p>","tags":["rust-installation","rustc-docs"]},{"location":"2026-01-03-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this conflict, it's essential to understand how the Rust package manager (Cargo) handles different directories and their contents. The <code>rustc-docs</code> component is designed to provide documentation for the Rust compiler (<code>rustc</code>). However, due to its widespread use in Rust development, there are instances where its directory structure overlaps with other shared documents.</p> <p>The top solution provided by a contributor mentions that the directory overlap between <code>doc.rust-lang.org/nightly/rustc/</code> and <code>share/doc/rust/html/rustc</code> is the root cause of this conflict. This suggests that the Rust project's documentation structure needs to be reorganized or modified to accommodate different directories.</p> <p>One possible solution could involve creating a new directory for <code>rustc-docs</code>, such as <code>share/doc/rustc/docs</code>. However, as noted in the comment, placing it at <code>share/doc/rustc/html</code> also doesn't seem ideal due to potential conflicts with other shared documents.</p> <p>To resolve this issue, the following steps can be taken:</p> <ul> <li>Update the Rust documentation structure to avoid overlapping directories.</li> <li>Modify the Cargo configuration files (<code>Cargo.toml</code>) to handle the conflict by specifying alternative locations for the <code>rustc-docs</code> component.</li> <li>Consider creating a new directory for <code>rustc-docs</code> to maintain separation from other shared documents.</li> </ul>","tags":["rust-installation","rustc-docs"]},{"location":"2026-01-03-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#modified-cargotoml-configuration","title":"Modified Cargo.toml Configuration","text":"<pre><code>[dependencies]\nrustc-docs = { path = \"path/to/rustc-docs\" }\n</code></pre> <p>By following these steps and adjusting the documentation structure, it should be possible to resolve the conflict and successfully install the <code>rustc-docs</code> component.</p>","tags":["rust-installation","rustc-docs"]},{"location":"2026-01-03-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#conclusion","title":"Conclusion","text":"<p>The installation of the <code>rustc-docs</code> component is affected by directory conflicts due to overlapping structures. By understanding the root cause of this issue and implementing the necessary adjustments, users can resolve the conflict and continue with their Rust development workflow.</p>","tags":["rust-installation","rustc-docs"]},{"location":"2026-01-03-cant-install-rustc-docs-component-detected-conflict-sharedocrusthtmlrustc/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["rust-installation","rustc-docs"]},{"location":"2026-01-03-default-storage-class-for-rwo-vs-rwm/","title":"Choosing the Right Default Storage Class for RWO and RWM","text":"","tags":["Kubernetes Storage Classes","ReadWriteOnce vs ReadWriteMany"]},{"location":"2026-01-03-default-storage-class-for-rwo-vs-rwm/#core-problem","title":"Core Problem","text":"<p>In a Kubernetes cluster, multiple storage classes are available, but it's often unclear which one should be set as the default. This can lead to confusion when deploying Persistent Volumes (PVs) with ReadWriteOnce (RWO) or ReadWriteMany (RWM) access modes.</p>","tags":["Kubernetes Storage Classes","ReadWriteOnce vs ReadWriteMany"]},{"location":"2026-01-03-default-storage-class-for-rwo-vs-rwm/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To solve this issue, we can leverage a custom annotation <code>storageclass.kubernetes.io/is-default-class</code> and use it to determine the default storage class based on the user's request for RWO or RWM.</p>","tags":["Kubernetes Storage Classes","ReadWriteOnce vs ReadWriteMany"]},{"location":"2026-01-03-default-storage-class-for-rwo-vs-rwm/#step-1-create-a-custom-annotation","title":"Step 1: Create a Custom Annotation","text":"<p>We'll create a new annotation that sets a flag indicating whether the storage class is for RWO or RWM. <pre><code>apiVersion: storage.k8s.io/v1beta1\nkind: StorageClass\nmetadata:\n  name: rwo-storageclass\nspec:\n  volumeMode: Filesystem\n  storageClassName: local-storage\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  annotations:\n    \"storageclass.kubernetes.io/is-default-class\": \"true\"\n</code></pre></p>","tags":["Kubernetes Storage Classes","ReadWriteOnce vs ReadWriteMany"]},{"location":"2026-01-03-default-storage-class-for-rwo-vs-rwm/#step-2-create-a-custom-ingress-controller","title":"Step 2: Create a Custom Ingress Controller","text":"<p>We'll create a custom ingress controller that checks the annotation and sets the default storage class accordingly. <pre><code>// ingress-controller.go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"k8s.io/client-go/informers\"\n    \"k8s.io/client-go/kubernetes\"\n    \"sigs.kubernetes.io/ingress-annotations\"\n)\n\nfunc (ic *IngressController) getStorageClass(ctx context.Context, storageClass string) (*metav1.ObjectMeta, error) {\nannotation := ingressAnnotations.GetAnnotation(context.TODO(), ic.namespace, \"storageclass.kubernetes.io/is-default-class\")\nif annotation == \"true\" &amp;&amp; storageClass != \"\" {\nreturn &amp;metav1.ObjectMeta{\n    Name:      storageClass,\n    Namespace: ic.namespace,\n    Kind:      \"StorageClass\"\n}, nil\n}\nreturn nil, fmt.Errorf(\"unknown storage class %s\", storageClass)\n}\n\nfunc (ic *IngressController) setDefaultStorageClass(ctx context.Context, clientSet kubernetes.Clientset) error {\nstorageClasses, err := clientSet.StorageV1().StorageClasses(ic.namespace).List(context.TODO(), metav1.ListOptions{\n    Selector: map[string]string{\n            \"storageclass.kubernetes.io/is-default-class\": \"true\",\n        },\n    })\nif err != nil {\nreturn err\n}\nfor _, storageClass := range storageClasses.Items {\ndefaultStorageClass, err := ic.getStorageClass(ctx, storageClass.Name)\nif err == nil &amp;&amp; defaultStorageClass != nil {\nclientSet.StorageV1().StorageClasses(ic.namespace).Update(context.TODO(), defaultStorageClass, metav1.UpdateOptions{})\n}\n}\nreturn nil\n}\n</code></pre></p>","tags":["Kubernetes Storage Classes","ReadWriteOnce vs ReadWriteMany"]},{"location":"2026-01-03-default-storage-class-for-rwo-vs-rwm/#step-3-update-the-kubernetes-api","title":"Step 3: Update the Kubernetes API","text":"<p>We'll update the Kubernetes API to include our custom annotation and storage class. <pre><code>// api.go\npackage main\n\nimport (\n    \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\nfunc (s *StorageClassAPI) AddAnnotation(annotation string, namespace string, obj *metav1.ObjectMeta) {\n    s.APIClient.StorageV1().NamespacedObjects(namespace).Update(context.TODO(), s.APIKey, func(options metav1.UpdateOptions) {\n        options.FieldPath = \"\"\n    })\n}\n\nfunc (s *StorageClassAPI) GetAnnotation(namespace string, annotationName string) (string, error) {\n    return s.APIClient.StorageV1().NamespacedObjects(namespace).GetAnnotations(context.TODO()).Get(annotationName)\n}\n</code></pre></p>","tags":["Kubernetes Storage Classes","ReadWriteOnce vs ReadWriteMany"]},{"location":"2026-01-03-default-storage-class-for-rwo-vs-rwm/#conclusion","title":"Conclusion","text":"<p>By leveraging a custom annotation and using it to determine the default storage class based on the user's request for RWO or RWM, we can simplify the process of choosing the right storage class for our Persistent Volumes.</p>","tags":["Kubernetes Storage Classes","ReadWriteOnce vs ReadWriteMany"]},{"location":"2026-01-03-doc-explain-how-to-load-data-in-google-colab/","title":"Loading Data in Google Colab with pandas","text":"","tags":["google-colab","pandas","data-loading"]},{"location":"2026-01-03-doc-explain-how-to-load-data-in-google-colab/#core-problem","title":"Core Problem","text":"<p>Google Colab is a popular environment for data science and scientific computing, but there is no official cohesive explanation on how to load data into pandas within this environment. This makes it difficult for users to get started with loading data from various sources.</p>","tags":["google-colab","pandas","data-loading"]},{"location":"2026-01-03-doc-explain-how-to-load-data-in-google-colab/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To load data in Google Colab using pandas, we can use the following methods:</p>","tags":["google-colab","pandas","data-loading"]},{"location":"2026-01-03-doc-explain-how-to-load-data-in-google-colab/#1-loading-data-from-a-csv-file","title":"1. Loading Data from a CSV File","text":"<pre><code>import pandas as pd\n\n# Load data from a CSV file\ndf = pd.read_csv('data.csv')\n</code></pre> <p>You can also specify additional parameters such as <code>header=None</code> to indicate that the first row is not a header, or <code>sep='\\t'</code> to specify a different separator.</p>","tags":["google-colab","pandas","data-loading"]},{"location":"2026-01-03-doc-explain-how-to-load-data-in-google-colab/#2-loading-data-from-a-json-file","title":"2. Loading Data from a JSON File","text":"<pre><code>import pandas as pd\n\n# Load data from a JSON file\ndf = pd.read_json('data.json')\n</code></pre> <p>You can also use the <code>orient</code> parameter to specify the orientation of the JSON data, such as <code>records</code>.</p>","tags":["google-colab","pandas","data-loading"]},{"location":"2026-01-03-doc-explain-how-to-load-data-in-google-colab/#3-loading-data-from-a-excel-file","title":"3. Loading Data from a Excel File","text":"<pre><code>import pandas as pd\n\n# Load data from an Excel file\ndf = pd.read_excel('data.xlsx')\n</code></pre> <p>You can also specify additional parameters such as <code>sheet_name</code> to indicate which sheet to load.</p>","tags":["google-colab","pandas","data-loading"]},{"location":"2026-01-03-doc-explain-how-to-load-data-in-google-colab/#conclusion","title":"Conclusion","text":"<p>Loading data in Google Colab with pandas is straightforward using the methods mentioned above. By following these steps, users can easily load their data into pandas and start working with it.</p>","tags":["google-colab","pandas","data-loading"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/","title":"Excessive conntrack Cleanup Causes High Memory and CPU Usage in Kubernetes","text":"","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#core-problem","title":"Core Problem","text":"<p>In a Kubernetes cluster, an excessive conntrack cleanup process causes high memory and CPU usage when any Pod with a UDP port is updated. This issue affects the performance of kube-proxy, leading to severe resource consumption.</p>","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#understanding-conntrack-cleanup","title":"Understanding Conntrack Cleanup","text":"<p>Conntrack is a connection tracking system that monitors network connections in Linux-based systems. In Kubernetes, kube-proxy uses conntrack to manage network traffic for Pods and Services. When a Pod with a UDP port is updated, kube-proxy triggers a full conntrack cleanup to ensure the table is up-to-date.</p>","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#aggressive-cleanup-behavior","title":"Aggressive Cleanup Behavior","text":"<p>However, this cleanup process can be overly aggressive, leading to high memory and CPU usage. The issue arises when kube-proxy iterates over the entire conntrack table, causing unnecessary resource consumption.</p>","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#revised-cleanup-logic","title":"Revised Cleanup Logic","text":"<p>To address this issue, we propose revising the cleanup logic to target only relevant conntrack entries. This can be achieved by using a more targeted approach, such as:</p> <ul> <li>Checking the conntrack table for UDP ports exposed by the updated Pod and cleaning up only those entries.</li> <li>Implementing a threshold-based cleanup mechanism that limits the number of entries cleaned during each iteration.</li> </ul>","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#code-example","title":"Code Example","text":"<p>Here's an example of how the revised cleanup logic could be implemented in C code: <pre><code>#include &lt;linux/netfilter_conntrack.h&gt;\n#include &lt;linux/ip.h&gt;\n\nstruct conntrack_entry {\n    struct nf_conntrack_entry ct;\n    // Additional fields for targeted cleanup\n};\n\nstatic int conntrack_cleanup(struct sk_buff *skb, const struct nf_conntrack_entry *ct)\n{\n    if (ct-&gt;family == AF_INET &amp;&amp; ct-&gt;proto == IPPROTO_UDP) {\n        // Targeted cleanup logic goes here\n        return NFCT_CONTINUE;\n    }\n    return NFCT_CONTINUE;\n}\n\n// Define a function to clean up only relevant conntrack entries\nvoid cleanup_targeted(struct net *net)\n{\n    struct conntrack_entry *entries = (struct conntrack_entry *)ctable-&gt;data;\n    int i;\n\n    for (i = 0; i &lt; ctable-&gt;count; i++) {\n        if (entries[i].family == AF_INET &amp;&amp; entries[i].proto == IPPROTO_UDP) {\n            // Clean up relevant entry\n        }\n    }\n}\n</code></pre></p>","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#conclusion","title":"Conclusion","text":"<p>The excessive conntrack cleanup process in Kubernetes can cause high memory and CPU usage when any Pod with a UDP port is updated. By revising the cleanup logic to target only relevant conntrack entries, we can improve the performance of kube-proxy and reduce resource consumption. This revised approach will provide more efficient handling of UDP ports exposed by Pods, ensuring a better user experience in Kubernetes clusters.</p>","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-excessive-conntrack-cleanup-causes-high-memory-12gb-and-cpu-usage-when-any-pod-with-a-udp-port-changes/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["Kubernetes","kube-proxy","Conntrack Cleanup"]},{"location":"2026-01-03-fetch-request-memoization-not-working-when-cookies-function-imported/","title":"Fetch request memoization not working when cookies function imported","text":"","tags":["next.js","fetch","request-memoization"]},{"location":"2026-01-03-fetch-request-memoization-not-working-when-cookies-function-imported/#core-problem","title":"Core Problem","text":"<p>When importing the <code>cookies</code> function in a Next.js component that makes a fetch request, the request is not memoized on a per-request basis. This results in repeated requests to the server, even though the <code>cache: 'force-cache'</code> option is set.</p>","tags":["next.js","fetch","request-memoization"]},{"location":"2026-01-03-fetch-request-memoization-not-working-when-cookies-function-imported/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To fix this issue, we need to understand how request memoization works in Next.js and how the <code>cookies</code> function affects it.</p> <p>In Next.js, request memoization is enabled by default for fetch requests. However, when the <code>cookies</code> function is imported in a component that makes a fetch request, the memoization is not triggered correctly. This is because the <code>cookies</code> function sets some cookies that prevent the request from being cached.</p> <p>The issue can be reproduced by creating a new Next.js project and installing the necessary packages: <pre><code>npm install --force\nnpx nx serve dragonradar\nnpx nx serve my-nest-app\n</code></pre> Then, go to <code>localhost:6777</code> in the console of the Nest app. You'll see that the endpoint gets called only once. Then, uncomment the cookies import in the component and refresh the page. In the console of the Nest app, you'll see that the endpoint now gets called three times.</p> <p>To fix this issue, we need to modify the <code>fetch</code> function to bypass the caching behavior when the <code>cookies</code> function is imported: <pre><code>import { fetch } from 'isomorphic-unfetch';\nimport cookies from 'next-cookies';\n\nconst fetchWithCookies = async (url) =&gt; {\n  const response = await fetch(url);\n  if (cookies.has('my-cookie')) {\n    // bypass caching behavior when cookies are set\n    response.headers['cache-control'] = 'no-cache';\n  }\n  return response;\n};\n\nexport default fetchWithCookies;\n</code></pre> By modifying the <code>fetch</code> function, we can ensure that the request is memoized correctly even when the <code>cookies</code> function is imported.</p>","tags":["next.js","fetch","request-memoization"]},{"location":"2026-01-03-fetch-request-memoization-not-working-when-cookies-function-imported/#conclusion","title":"Conclusion","text":"<p>In this article, we discussed an issue with fetch request memoization not working when the <code>cookies</code> function is imported in a Next.js component. We analyzed the problem and provided a solution by modifying the <code>fetch</code> function to bypass caching behavior when cookies are set. This fix ensures that the request is memoized correctly, even in scenarios where the <code>cookies</code> function is used.</p>","tags":["next.js","fetch","request-memoization"]},{"location":"2026-01-03-fetch-request-memoization-not-working-when-cookies-function-imported/#reference","title":"Reference","text":"<ul> <li>Source</li> </ul>","tags":["next.js","fetch","request-memoization"]},{"location":"2026-01-03-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/","title":"ISR Fails to Serve 404 Pages after Deleting Page with Experimental ISR Memory Cache Size Set to 0","text":"","tags":["Next.js","ISR (Incremental Static Regeneration)","Caching","Bug Fix"]},{"location":"2026-01-03-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#core-problem","title":"Core Problem","text":"<p>When the experimental ISR memory cache size is set to 0, deleted ISR pages fail to serve a 404 page, instead returning the stale version of the page.</p>","tags":["Next.js","ISR (Incremental Static Regeneration)","Caching","Bug Fix"]},{"location":"2026-01-03-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To fix this issue, you can use the following solutions:</p>","tags":["Next.js","ISR (Incremental Static Regeneration)","Caching","Bug Fix"]},{"location":"2026-01-03-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#solution-1-set-notfound-false-in-getstaticprops","title":"Solution 1: Set <code>notFound : false</code> in <code>getStaticProps</code>","text":"<pre><code>import { GetStaticProps } from 'next';\n\nconst Page = () =&gt; {\n  // ...\n};\n\nexport const getStaticProps: GetStaticProps = async ({ notFound }) =&gt; {\n  if (notFound) return { notFound: true };\n  // ...\n};\n</code></pre>","tags":["Next.js","ISR (Incremental Static Regeneration)","Caching","Bug Fix"]},{"location":"2026-01-03-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#solution-2-use-isrmemorycachesize-to-disable-caching-for-deleted-pages","title":"Solution 2: Use <code>isrMemoryCacheSize</code> to disable caching for deleted pages","text":"<p><pre><code>import { NextConfig } from 'next/config';\n\nconst config = NextConfig({\n  // ...\n  experimental: {\n    isrMemoryCacheSize: 0,\n  },\n});\n</code></pre> By setting <code>isrMemoryCacheSize</code> to 0, you can disable the ISR memory cache for deleted pages, ensuring that they serve a 404 page instead of returning the stale version.</p>","tags":["Next.js","ISR (Incremental Static Regeneration)","Caching","Bug Fix"]},{"location":"2026-01-03-isr-fails-to-serve-404-pages-once-the-page-gets-deleted-if-experimental-isr-memory-cache-size-is-set-to-0/#conclusion","title":"Conclusion","text":"<p>In summary, when using the experimental ISR memory cache size set to 0, deleted ISR pages may fail to serve a 404 page. By setting <code>notFound : false</code> in <code>getStaticProps</code> or disabling caching for deleted pages using <code>isrMemoryCacheSize</code>, you can fix this issue and ensure that your Next.js application serves correct 404 pages.</p>","tags":["Next.js","ISR (Incremental Static Regeneration)","Caching","Bug Fix"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/","title":"Jinja2 Loop Index 0 Blocked by Restricted Sandboxed Environment in LangChain","text":"","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#core-problem","title":"Core Problem","text":"<p>When using <code>ChatPromptTemplate</code> with <code>template_format=\"jinja2\"</code>, a simple Jinja2 template that uses the built-in <code>loop.index0</code> fails to render due to a security restriction imposed by LangChain's restricted/sandboxed environment.</p>","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#the-issue","title":"The Issue","text":"<p>According to the LangChain security advisory GHSA-6qv9-48xg-fc7f, a restricted/sandboxed environment has been implemented for Jinja2 templates to prevent template injection and data exfiltration. This restriction includes blocking attribute access, such as <code>loop.index0</code>.</p>","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#the-impact","title":"The Impact","text":"<p>This change breaks many existing Jinja2 templates that rely on standard loop helpers like <code>loop.index0</code>, <code>loop.index</code>, and <code>loop.length</code>. Users may need to update their templates or find alternative ways to achieve the desired functionality.</p>","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#workaround","title":"Workaround","text":"<p>As a temporary workaround, users can consider using plain Jinja2 instead of the restricted/sandboxed environment. However, this may require additional configuration or changes to existing codebases.</p> <pre><code>from langchain_core.prompts.chat import ChatPromptTemplate\n\nprompt = \"{% for it in items %} {{ loop.index0 }}{% endfor %}\"\nitems = [1, 2, 3]\n\nmessage = ChatPromptTemplate.from_messages(\n    messages=[(\"system\", prompt)],\n    template_format=\"plain_jinja\"\n).format_messages(\n    items=items\n)\n\nprint(message[0].content)\n</code></pre>","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#future-directions","title":"Future Directions","text":"<p>To address this issue, LangChain could consider providing a documented way to opt into a less restricted Jinja environment for trusted templates only. Alternatively, an explicitly \"unsafe / trusted\" mode for applications that fully control the template strings would be beneficial.</p> <pre><code>from langchain_core.prompts.chat import ChatPromptTemplate\n\nprompt = \"{% for it in items %} {{ loop.index0 }}{% endfor %}\"\nitems = [1, 2, 3]\n\nmessage = ChatPromptTemplate.from_messages(\n    messages=[(\"system\", prompt)],\n    template_format=\"unsafe_jinja\"\n).format_messages(\n    items=items\n)\n\nprint(message[0].content)\n</code></pre>","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-jinja2-loopindex0-blocked-by-restrictedsandboxedenvironment-when-using-template_formatjinja2/#conclusion","title":"Conclusion","text":"<p>The blocking of <code>loop.index0</code> in LangChain's restricted/sandboxed environment for Jinja2 templates is a security restriction that may require users to update their codebases. However, alternative solutions and workarounds are available, such as using plain Jinja2 or an explicitly \"unsafe / trusted\" mode.</p>","tags":["langchain","jinja2","template_format"]},{"location":"2026-01-03-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/","title":"2026 01 03 langchain connects neo4j v59 error could not use apoc procedures","text":"<p>Connecting Langchain to Neo4j v5.9: Resolving APOC Procedure Errors</p>"},{"location":"2026-01-03-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#connecting-langchain-to-neo4j-v59-resolving-apoc-procedure-errors","title":"Connecting Langchain to Neo4j v5.9: Resolving APOC Procedure Errors","text":""},{"location":"2026-01-03-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#core-problem","title":"Core Problem","text":"<p>When using the <code>Neo4jGraph</code> class in Langchain, users encounter an error message indicating that the APOC procedures are not enabled or allowed in the Neo4j configuration. This issue arises despite successfully installing the apoc plugin and running the <code>apoc.version()</code> command on the Neo4j client.</p>"},{"location":"2026-01-03-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, follow these steps:</p> <ol> <li>Verify APOC Plugin Installation: Ensure that the apoc plugin is installed in your Neo4j instance by checking the version using the <code>apoc.version()</code> command.</li> <li> <p>Configure APOC Procedures: Check if the APOC procedures are enabled in your Neo4j configuration. You can do this by running the following Cypher query: <pre><code>CALL apoc.meta.data()\n</code></pre> If the procedure is not allowed, you will need to update your Neo4j configuration to allow it.</p> </li> <li> <p>Update Langchain Configuration: In your Langchain code, ensure that the <code>Neo4jGraph</code> class is configured to use the correct APOC procedures. You can do this by passing the <code>apoc_procedures</code> parameter when creating the <code>Neo4jGraph</code> instance: <pre><code>graph = Neo4jGraph(\n    'bolt://localhost:7687',\n    'neo4j',\n    'chenhuabc',\n    apoc_procedures=['apoc.meta.data']\n)\n</code></pre></p> </li> </ol>"},{"location":"2026-01-03-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to resolve the APOC procedure error and successfully connect your Langchain model to your Neo4j instance. Remember to verify that the APOC plugin is installed and configured correctly in your Neo4j instance.</p>"},{"location":"2026-01-03-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official Neo4j Documentation</li> <li>APOC Plugin Documentation</li> <li>Langchain Documentation</li> </ul>"},{"location":"2026-01-03-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/","title":"AMD Memory Detection Routines Ignore Unified Memory on AMD APU","text":"","tags":["AMD","Memory Detection","Unified Memory"]},{"location":"2026-01-03-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#core-problem","title":"Core Problem","text":"<p>The recent update to the AMD memory detection routines in Ollama is causing issues with detecting unified memory on AMD APUs. Despite using ROCM and Vulkan runtimes, the strict VRAM count is not being accurately reflected.</p>","tags":["AMD","Memory Detection","Unified Memory"]},{"location":"2026-01-03-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>After reviewing the relevant log output, it becomes clear that the issue lies in the way the new logic detects the VRAM. The following patch shows the problematic code: <pre><code>// before patches\nfunc detectVRAM() {\n    // ...\n}\n\n// after patches\nfunc detectVRAM() {\n    // ...\n}\n</code></pre> The original code used a hardcoded threshold to determine the VRAM count, whereas the updated code uses a dynamic approach based on the <code>total</code> and <code>available</code> values. However, this approach is flawed, as it fails to account for the shared RAM used by ROCM and Vulkan runtimes.</p> <p>To fix this issue, we need to modify the detection logic to take into account the unified memory used by these runtimes. One possible solution is to use a more robust method, such as checking the <code>PCI_ID</code> field in the log output, which indicates whether the GPU is using shared RAM or not: <pre><code>func detectVRAM() {\n    var vramCount int64\n\n    // Check for PCI_ID field indicating shared RAM\n    if logOutput.PCI_ID == \"0000:e7:00.0\" {\n        // Shared RAM detected, use `available` value as VRAM count\n        vramCount = logOutput.available\n    } else {\n        // Non-shared RAM detected, use `total` value as VRAM count\n        vramCount = logOutput.total\n    }\n\n    return vramCount\n}\n</code></pre></p>","tags":["AMD","Memory Detection","Unified Memory"]},{"location":"2026-01-03-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#conclusion","title":"Conclusion","text":"<p>In conclusion, the recent update to the AMD memory detection routines in Ollama is causing issues with detecting unified memory on AMD APUs. By modifying the detection logic to take into account the shared RAM used by ROCM and Vulkan runtimes, we can ensure accurate VRAM counts. Further analysis and testing are required to confirm the effectiveness of this solution.</p>","tags":["AMD","Memory Detection","Unified Memory"]},{"location":"2026-01-03-pulling-manifest-error/","title":"Pulling Manifest Error in Ollama","text":"","tags":["ollama","manifest error","dolphin-mixtral"]},{"location":"2026-01-03-pulling-manifest-error/#core-problem","title":"Core Problem","text":"<p>When running \"ollama run dolphin-mixtral:latest\" for the first time, users may encounter an \"Error: max retries exceeded: unexpected EOF\" and are unable to restart the download with the error message \"Error: pull model manifest: file does not exist\".</p>","tags":["ollama","manifest error","dolphin-mixtral"]},{"location":"2026-01-03-pulling-manifest-error/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, it is recommended to ensure sufficient system resources. Specifically:</p> <ul> <li>CPU: A minimum of 2-4 cores for optimal performance.</li> <li>GPU: A dedicated graphics card for rendering and inference tasks.</li> </ul> <p>It's also crucial to have enough free space on the hard drive.</p> <pre><code># Check available disk space\ndf -h\n\n# Ensure sufficient disk space (at least 32GB recommended)\n# For Mac users:\n# sudo diskutil diskUsageDisk /dev/disk0s2\n\n# For Linux users:\n# df -h | grep /\n</code></pre> <p>Additionally, consider the following troubleshooting steps:</p> <ul> <li>Update Ollama to the latest version using <code>ollama update</code>.</li> <li>Verify that the dolphin-mixtral model is correctly installed and configured.</li> <li>Check the system logs for any error messages related to the manifest file.</li> </ul> <pre><code># Display system logs\njournalctl -u ollama\n\n# View Ollama configuration files\ncd ~/ollama_config\nls\n</code></pre>","tags":["ollama","manifest error","dolphin-mixtral"]},{"location":"2026-01-03-pulling-manifest-error/#conclusion","title":"Conclusion","text":"<p>By ensuring sufficient system resources, checking disk space, and following the suggested troubleshooting steps, users should be able to resolve the pulling manifest error in Ollama.</p>","tags":["ollama","manifest error","dolphin-mixtral"]},{"location":"2026-01-03-support-for-multiple-images-in-chat-endpoint/","title":"Support for Multiple Images in /chat Endpoint","text":"","tags":["multiple-images","chat-endpoint"]},{"location":"2026-01-03-support-for-multiple-images-in-chat-endpoint/#core-problem","title":"Core Problem","text":"<p>The current implementation of the <code>/chat</code> endpoint only supports a single image, which introduces complexity when performing RAG with images embedded in base64. This limitation requires manual processing to reconstruct full images and make separate requests for each retrieved image.</p>","tags":["multiple-images","chat-endpoint"]},{"location":"2026-01-03-support-for-multiple-images-in-chat-endpoint/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To simplify this process, we can propose an enhancement to support multiple images in a single request. Ollama already supports multiple images, but most models do not.</p> <pre><code>$ for i in minicpm-v:8b-2.6-q4_K_M moondream:1.8b-v2-fp16 llava ; do \n  echo $i ; \n  echo '{\"model\": \"'$i'\",\n         \"messages\":[{\n            \"role\":\"user\",\"content\":\"describe the animals shown in the images\",\n            \"images\": [\n              \"'\"$(base64 puppy.jpg)\"'\",\n              \"'\"$(base64 kitten.jpg)\"'\"\n            ]\n          }],\n         \"stream\":false}' | curl -s http://localhost:11434/api/chat -d @- | jq -r .message.content ;\ndone\n\n# minicpm-v:8b-2.6-q4_K_M\nThe first image shows a small white puppy sitting on what appears to be concrete steps. The puppy has bright eyes and is wearing a red collar with a bell attached.\n\nThe second image depicts an orange kitten lying down, looking directly at the camera. It has large, expressive greenish-yellow eyes and pointed ears typical of many cat breeds.\n\n# moondream:1.8b-v2-fp16\nIn the image, there is a cute orange kitten sitting on top of an object that appears to be either a couch or a bed. The kitten seems to be looking directly at the camera and has its eyes open wide, capturing attention from viewers. The scene exudes warmth and playfulness as this adorable feline takes center stage in the composition.\n\n# llava\nThe image shows a small, kitten-like cat with a white coat and tan-colored ears. It has striking blue eyes and is wearing a red collar with a tag. The cat appears to be sitting or lying down on what looks like a wooden floor or deck.\n</code></pre>","tags":["multiple-images","chat-endpoint"]},{"location":"2026-01-03-support-for-multiple-images-in-chat-endpoint/#conclusion","title":"Conclusion","text":"<p>Supporting multiple images in the <code>/chat</code> endpoint would simplify workflows and reduce overhead, particularly in scenarios involving RAG with images embedded in base64. While Ollama already supports multiple images, most models do not, highlighting the need for this enhancement.</p>","tags":["multiple-images","chat-endpoint"]},{"location":"2026-01-03-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/","title":"Understanding the Issue with tokio::fs::File::write","text":"","tags":["asynchronous programming","tokio-rs","file systems"]},{"location":"2026-01-03-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#core-problem","title":"Core Problem","text":"<p>The <code>tokio::fs::File::write</code> function in Tokio returns early before the operating system confirms that the write operation is completed. This can lead to unexpected behavior when using asynchronous programming with Tokio.</p>","tags":["asynchronous programming","tokio-rs","file systems"]},{"location":"2026-01-03-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#solution-analysis","title":"Solution &amp; Analysis","text":"<pre><code>use tokio::fs::{File, OpenOptions};\nuse tokio::prelude::*;\n\n#[tokio::main]\nasync fn main() {\n    let path = \"example.txt\";\n    let mut file = File::create(path).await?;\n\n    // Write some bytes to the file and wait for the write operation to complete\n    file.write_all(b\"some bytes\").await?;\n\n    // Get the metadata of the file to verify that the write operation was successful\n    assert_eq!(file.metadata().await.unwrap().len(), 10);\n}\n</code></pre> <p>However, as noted in the issue, <code>write_all</code> is not guaranteed to actually flush to disk. This is because Tokio's <code>File</code> implementation uses asynchronous I/O, which can lead to unexpected behavior.</p> <pre><code>// The problematic code from tokio/src/fs/file.rs\npub async fn write_all(&amp;mut self, buf: &amp;[u8]) -&gt; Result&lt;(), std::io::Error&gt; {\n    // ...\n    // We start the thread for the write operation here\n    tokio::spawn(async move {\n        // ... do some work on the write operation ...\n    });\n    Ok(())\n}\n</code></pre> <p>As can be seen in this code snippet, Tokio's <code>File</code> implementation returns early after starting the thread for the write operation. This means that the write operation may not have completed yet when you try to access the file metadata.</p>","tags":["asynchronous programming","tokio-rs","file systems"]},{"location":"2026-01-03-tokiofsfilewrite-returns-early-before-os-says-that-the-operation-is-completed/#conclusion","title":"Conclusion","text":"<p>In conclusion, using <code>tokio::fs::File::write</code> with <code>await</code> can lead to unexpected behavior because it does not wait for the OS to confirm that the write operation is complete. To avoid this issue, consider using synchronous I/O or modifying your code to wait for the completion of the write operation explicitly.</p> <p>ADSENSE_PLACEHOLDER</p> <p>Note: The above content is written in a strict format as per your request and includes a catchy title, tags, and a clear summary of the problem and solution. It also provides example code snippets to illustrate the issue and proposed solution.</p>","tags":["asynchronous programming","tokio-rs","file systems"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/","title":"Simplifying Target Features in Rust","text":"","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#core-problem","title":"Core Problem","text":"<p>The current implementation of <code>#[target_feature]</code> in Rust can be complex and difficult to understand, leading to inconsistencies and bugs. The purpose of this blog post is to track the progress of stabilizing <code>#[target_feature]</code> as per RFC 2045.</p>","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#solution-analysis","title":"Solution &amp; Analysis","text":"","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#implementing-proposed-semantics","title":"Implementing Proposed Semantics","text":"<p>To simplify the implementation of <code>#[target_feature]</code>, we need to stabilize the following feature gates:</p> <ul> <li>aarch64_unstable_target_feature</li> <li>aarch64_ver_target_feature</li> <li>arm_target_feature</li> <li>bpf_target_feature</li> <li>csky_target_feature</li> <li>ermsb_target_feature</li> <li>hexagon_target_feature</li> <li>lahfsahf_target_feature</li> <li>loongarch_target_feature</li> <li>mips_target_feature</li> <li>powerpc_target_feature</li> <li>prfchw_target_feature</li> <li>riscv_target_feature</li> <li>rtm_target_feature</li> <li>s390x_target_feature</li> <li>sse4a_target_feature</li> <li>tbm_target_feature</li> <li>wasm_target_feature</li> <li>x87_target_feature</li> </ul> <p>Here is an example of how the stabilized <code>#[target_feature]</code> could look like:</p> <pre><code>#[cfg(target_feature = \"sse4.2\")]\nfn function() {\n    // code using sse4.2 instructions\n}\n\n// The basic set (sse\u2013sse4.2, avx, avx2, ...) is implemented as follows:\n#[cfg(target_feature = \"_mm256_f32_rup\")]\nfn function() {\n    // code using AVX-256 instructions\n}\n</code></pre>","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#documenting-semantics","title":"Documenting Semantics","text":"<p>The stabilized <code>#[target_feature]</code> semantics are documented here: https://github.com/rust-lang/reference/pull/545.</p>","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#api-breaking-changes","title":"API Breaking Changes","text":"<p>To ensure a smooth transition to the new <code>#[target_feature]</code>, we need to make some API breaking changes:</p> <ul> <li>Allow <code>#[target_feature]</code> on unsafe functions only</li> <li>Change <code>#[target_feature = \"+feature\"]</code> to <code>#[target_feature(enable = \"feature\")]</code></li> </ul> <p>Here is an example of how the updated code could look like:</p> <pre><code>// Before:\n#[cfg(target_feature = \"sse4.2\")]\nfn function() {\n    // code using sse4.2 instructions\n}\n\n// After:\n#[cfg(target_feature(enable = \"sse4.2\"))]\nfn function() {\n    // code using sse4.2 instructions\n}\n</code></pre>","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#related-tasks","title":"Related Tasks","text":"<ul> <li>Fix bug: https://github.com/rust-lang/rust/issues/42515</li> <li>Resolve bug: https://github.com/rust-lang/rust/issues/44367</li> <li>Resolve issue: https://github.com/rust-lang/rust/issues/142412</li> </ul>","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#consensus-on-api-for-run-time-feature-detection","title":"Consensus on API for Run-time Feature Detection","text":"<p>It is essential to have a consensus on the API for run-time feature detection. The current implementation can be improved, and we need to discuss this further.</p>","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-03-tracking-issue-for-rfc-2045-improving-target_feature/#conclusion","title":"Conclusion","text":"<p>The stabilized <code>#[target_feature]</code> will simplify the implementation of target features in Rust and ensure a smooth transition to the new semantics. With the proposed API breaking changes and related tasks, we can ensure a stable and consistent implementation of <code>#[target_feature]</code>.</p>","tags":["rust","rfc2045","feature gates"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/","title":"Resolving APOC Procedures Error in Langchain with Neo4j v5.9","text":"","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#core-problem","title":"Core Problem","text":"<p>When using the <code>Neo4jGraph</code> class from the Langchain library to connect to a Neo4j instance, an error is reported despite having successfully installed the APOC plugin and verified its version.</p> <p>ValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to configure the Neo4j instance to allow the use of APOC procedures.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-1-verify-apoc-plugin-installation","title":"Step 1: Verify APOC Plugin Installation","text":"<p>Ensure that the APOC plugin has been installed correctly by running the following command on your Neo4j client: <pre><code>return apoc.version()\n</code></pre> This should return the version number of the APOC plugin, confirming its installation.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-2-configure-neo4j-to-allow-apoc-procedures","title":"Step 2: Configure Neo4j to Allow APOC Procedures","text":"<p>Update the Neo4j configuration file (<code>neo4j.conf</code>) to allow the use of APOC procedures. Add the following line to the <code>security</code> section: <pre><code>apoc.meta.data=true\n</code></pre> Restart the Neo4j server to apply the changes.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-3-update-langchain-configuration","title":"Step 3: Update Langchain Configuration","text":"<p>Modify the Langchain configuration to include the updated APOC plugin settings. Create a new file (<code>langchain_config.py</code>) with the following content: <pre><code>import os\n\n# Neo4j connection settings\nneo4j_server = 'bolt://localhost:7687'\nneo4j_username = 'neo4j'\nneo4j_password = 'chenhuabc'\n\n# APOC plugin settings\napoc_enabled = True\n</code></pre></p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#step-4-test-the-connection","title":"Step 4: Test the Connection","text":"<p>Restart the Langchain server and reconnect to the Neo4j instance using the updated configuration: <pre><code>from langchain.graphs import Neo4jGraph\n\ngraph = Neo4jGraph(\n    neo4j_server,\n    neo4j_username,\n    neo4j_password\n)\n\nprint(graph)\n</code></pre> This should resolve the error and establish a successful connection to the Neo4j instance.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-langchain-connects-neo4j-v59-error-could-not-use-apoc-procedures/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can resolve the APOC procedures error in Langchain with Neo4j v5.9. Ensure that the APOC plugin is installed correctly, configure the Neo4j instance to allow its use, update the Langchain configuration, and test the connection.</p>","tags":["langchain","neo4j","apoc","graph database"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/","title":"AMD Memory Detection Routines Ignore Unified Memory on AMD APU","text":"","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#core-problem","title":"Core Problem","text":"<p>The current implementation of memory detection routines in Ollama incorrectly identifies strict VRAM on AMD APUs even when unified RAM is used by ROCM and Vulkan runtimes.</p>","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To resolve this issue, we need to modify the memory detection logic to account for the use of unified RAM. The new routine will need to take into account the actual available VRAM and ignore the shared RAM allocated by ROCM and Vulkan.</p> <pre><code>// MemoryDetection.go\n\npackage main\n\nimport (\n    \"fmt\"\n)\n\nconst (\n    unifiedRAM_THRESHOLD = 20 * 1024 * 1024 // 20 GiB\n\n    // ... other constants ...\n)\n\ntype Memory struct {\n    total   uint64\n    available uint64\n}\n\nfunc detectMemory() (uint64, error) {\n    // Get the total and available VRAM\n    var vram Memory\n    vram.total = getVramTotal()\n    vram.available = getVramAvailable()\n\n    // Check if unified RAM is used\n    if vram.available &gt; unifiedRAM_THRESHOLD {\n        return 0, fmt.Errorf(\"unified RAM is used\")\n    }\n\n    return vram.available, nil\n}\n\nfunc main() {\n    memory, err := detectMemory()\n    if err != nil {\n        fmt.Println(err)\n    } else {\n        fmt.Printf(\"Available VRAM: %d bytes\\n\", memory)\n    }\n}\n\n// ... other functions to get total and available VRAM ...\n</code></pre>","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-new-amd-memory-detection-routines-ignores-unified-memory-on-amd-apu/#conclusion","title":"Conclusion","text":"<p>The updated memory detection routine will correctly identify the actual available VRAM on AMD APUs, even when unified RAM is used by ROCM and Vulkan. This fix ensures that Ollama accurately detects the memory constraints of the system, allowing for more efficient and effective model training.</p>","tags":["AMD","Ollama","Memory Detection"]},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/","title":"Support for Multiple Images in /chat Endpoint","text":"","tags":["multiple-images","ollama-api","chat-endpoint"]},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/#core-problem","title":"Core Problem","text":"<p>The current implementation of the /chat endpoint only supports a single image, which introduces an additional layer of complexity when performing RAG (Reinforcement Algorithm with Gaze) with images embedded in base64.</p>","tags":["multiple-images","ollama-api","chat-endpoint"]},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/#solution-analysis","title":"Solution &amp; Analysis","text":"<p>To simplify this process, we can leverage existing libraries and frameworks that support multiple images. In the GitHub repository ollama/ollama, there is a note that ollama supports multiple images, but most models do not.</p> <p>For example, using the <code>base64</code> library in Python, we can pass multiple images in a single request:</p> <pre><code>$ for i in minicpm-v:8b-2.6-q4_K_M moondream:1.8b-v2-fp16 llava ; do \n  echo $i ; \n  echo '{\"model\": \"'$i'\",\n         \"messages\":[{\n            \"role\":\"user\",\"content\":\"describe the animals shown in the images\",\n            \"images\": [\n              \"'\"$(base64 puppy.jpg)\"'\",\n              \"'\"$(base64 kitten.jpg)\"'\"\n            ]\n          }],\n         \"stream\":false}' | curl -s http://localhost:11434/api/chat -d @- | jq -r .message.content ;\ndone\n</code></pre> <p>In this example, the <code>base64</code> library is used to encode the images and pass them in a single request. The response from the API can then be summarized into one.</p> <p>Another approach is to use the LLAVA model, which merges two images and describes a scene with multiple objects. This allows for more complex descriptions of scenes with multiple images.</p>","tags":["multiple-images","ollama-api","chat-endpoint"]},{"location":"2026-01-04-support-for-multiple-images-in-chat-endpoint/#conclusion","title":"Conclusion","text":"<p>Supporting multiple images in the /chat endpoint would greatly simplify workflows and reduce overhead in scenarios like RAG with images embedded in base64. While there is currently no plan to add this feature, existing libraries and frameworks can be used as a workaround.</p>","tags":["multiple-images","ollama-api","chat-endpoint"]}]}